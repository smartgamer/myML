{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimFin Tutorial 10 - Neural Networks\n",
    "\n",
    "[Original repository on GitHub](https://github.com/simfin/simfin-tutorials)\n",
    "\n",
    "This tutorial was originally written by [Hvass Labs](https://github.com/Hvass-Labs)\n",
    "\n",
    "----\n",
    "\n",
    "\"The attackers can be stopped by removing the head or destroying the brain.\" &ndash; [Shaun of The Dead](https://www.youtube.com/watch?v=ONEZfj6h1xc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous tutorial we used a Machine Learning (ML) model to try and predict future stock-returns for 1-3 year investment periods given various signals. This worked quite well on the training-set, but unfortunately it failed to generalize to the test-set.\n",
    "\n",
    "In this tutorial we will use a so-called Neural Network, which has become very fashionable in recent years, because they have been able to solve extremely hard problems, that were previously impossible to solve.\n",
    "\n",
    "Unfortunately, Neural Networks have become so hyped, that people sometimes misunderstand what they really are: A Neural Network is just a function approximator that maps some input x (e.g. signals) to some output y (e.g. stock-returns). It consists of a number of layers of both linear and non-linear mappings, whose parameters will be learned during the training process, so it learns to map from x to y in the training-set, and the learned mapping hopefully generalizes to new data.\n",
    "\n",
    "In theory, a Neural Network is capable of approximating any function to any desired degree of precision - but what is truly important, is not how well it performs on the training-set, but how well it performs on new data that it has not seen during the training.\n",
    "\n",
    "This tutorial shows how to use financial data from SimFin with the TensorFlow library for creating Neural Networks. It is assumed you are already familiar with both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow and Keras imports.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Scikit-learn imports.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Import the main functionality from the SimFin Python API.\n",
    "import simfin as sf\n",
    "\n",
    "# Import names used for easy access to SimFin's data-columns.\n",
    "from simfin.names import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8.3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version of the SimFin Python API.\n",
    "sf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow version.\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimFin Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimFin data-directory.\n",
    "sf.set_data_dir('~/simfin_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimFin load API key or use free data.\n",
    "sf.load_api_key(path='~/simfin_api_key.txt', default_key='free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn set plotting style.\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Hub\n",
    "\n",
    "We use a `StockHub` object to easily load and process financial data with these settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested in the US stock-market.\n",
    "market = 'us'\n",
    "\n",
    "# Add this date-offset to the fundamental data such as\n",
    "# Income Statements etc., because the REPORT_DATE is not\n",
    "# when it was actually made available to the public,\n",
    "# which can be 1, 2 or even 3 months after the Report Date.\n",
    "offset = pd.DateOffset(days=60)\n",
    "\n",
    "# Refresh the fundamental datasets (Income Statements etc.)\n",
    "# every 30 days.\n",
    "refresh_days = 30\n",
    "\n",
    "# Refresh the dataset with shareprices every 10 days.\n",
    "refresh_days_shareprices = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 6 µs, total: 22 µs\n",
      "Wall time: 28.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hub = sf.StockHub(market=market, offset=offset,\n",
    "                  refresh_days=refresh_days,\n",
    "                  refresh_days_shareprices=refresh_days_shareprices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signals\n",
    "\n",
    "First we calculate financial signals for the stocks, such as the Current Ratio, Debt Ratio, Net Profit Margin, Return on Assets, etc. These are calculated using data from the financial reports: Income Statements, Balance Sheets and Cash-Flow Statements, which are automatically downloaded and loaded by the data-hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"us-income-ttm\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-balance-ttm\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-cashflow-ttm\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-shareprices-daily\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Cache-file 'fin_signals-2a38bb7d.pickle' on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "CPU times: user 6.7 s, sys: 1.17 s, total: 7.87 s\n",
      "Wall time: 7.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_fin_signals = hub.fin_signals(variant='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate growth signals for the stocks, such as Earnings Growth, FCF Growth, Sales Growth, etc. These are also calculated using data from the financial reports: Income Statements, Balance Sheets and Cash-Flow Statements, which are automatically downloaded and loaded by the data-hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset \"us-income-quarterly\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-balance-quarterly\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Dataset \"us-cashflow-quarterly\" on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "Cache-file 'growth_signals-2a38bb7d.pickle' on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "CPU times: user 729 ms, sys: 266 ms, total: 994 ms\n",
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_growth_signals = hub.growth_signals(variant='daily')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Assets Growth</th>\n",
       "      <th>Assets Growth QOQ</th>\n",
       "      <th>Assets Growth YOY</th>\n",
       "      <th>Earnings Growth</th>\n",
       "      <th>Earnings Growth QOQ</th>\n",
       "      <th>Earnings Growth YOY</th>\n",
       "      <th>FCF Growth</th>\n",
       "      <th>FCF Growth QOQ</th>\n",
       "      <th>FCF Growth YOY</th>\n",
       "      <th>Sales Growth</th>\n",
       "      <th>Sales Growth QOQ</th>\n",
       "      <th>Sales Growth YOY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-09</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">ZYXI</th>\n",
       "      <th>2020-09-14</th>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-15</th>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7113810 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Assets Growth  Assets Growth QOQ  Assets Growth YOY  \\\n",
       "Ticker Date                                                              \n",
       "A      2007-01-03            NaN                NaN                NaN   \n",
       "       2007-01-04            NaN                NaN                NaN   \n",
       "       2007-01-05            NaN                NaN                NaN   \n",
       "       2007-01-08            NaN                NaN                NaN   \n",
       "       2007-01-09            NaN                NaN                NaN   \n",
       "...                          ...                ...                ...   \n",
       "ZYXI   2020-09-14       0.683181           0.106466           0.683181   \n",
       "       2020-09-15       0.683181           0.106466           0.683181   \n",
       "       2020-09-16       0.683181           0.106466           0.683181   \n",
       "       2020-09-17       0.683181           0.106466           0.683181   \n",
       "       2020-09-18       0.683181           0.106466           0.683181   \n",
       "\n",
       "                   Earnings Growth  Earnings Growth QOQ  Earnings Growth YOY  \\\n",
       "Ticker Date                                                                    \n",
       "A      2007-01-03              NaN                  NaN                  NaN   \n",
       "       2007-01-04              NaN                  NaN                  NaN   \n",
       "       2007-01-05              NaN                  NaN                  NaN   \n",
       "       2007-01-08              NaN                  NaN                  NaN   \n",
       "       2007-01-09              NaN                  NaN                  NaN   \n",
       "...                            ...                  ...                  ...   \n",
       "ZYXI   2020-09-14         0.124203             0.027239             0.395467   \n",
       "       2020-09-15         0.124203             0.027239             0.395467   \n",
       "       2020-09-16         0.124203             0.027239             0.395467   \n",
       "       2020-09-17         0.124203             0.027239             0.395467   \n",
       "       2020-09-18         0.124203             0.027239             0.395467   \n",
       "\n",
       "                   FCF Growth  FCF Growth QOQ  FCF Growth YOY  Sales Growth  \\\n",
       "Ticker Date                                                                   \n",
       "A      2007-01-03         NaN             NaN             NaN           NaN   \n",
       "       2007-01-04         NaN             NaN             NaN           NaN   \n",
       "       2007-01-05         NaN             NaN             NaN           NaN   \n",
       "       2007-01-08         NaN             NaN             NaN           NaN   \n",
       "       2007-01-09         NaN             NaN             NaN           NaN   \n",
       "...                       ...             ...             ...           ...   \n",
       "ZYXI   2020-09-14   -0.212002        5.847761        2.823333      0.636049   \n",
       "       2020-09-15   -0.212002        5.847761        2.823333      0.636049   \n",
       "       2020-09-16   -0.212002        5.847761        2.823333      0.636049   \n",
       "       2020-09-17   -0.212002        5.847761        2.823333      0.636049   \n",
       "       2020-09-18   -0.212002        5.847761        2.823333      0.636049   \n",
       "\n",
       "                   Sales Growth QOQ  Sales Growth YOY  \n",
       "Ticker Date                                            \n",
       "A      2007-01-03               NaN               NaN  \n",
       "       2007-01-04               NaN               NaN  \n",
       "       2007-01-05               NaN               NaN  \n",
       "       2007-01-08               NaN               NaN  \n",
       "       2007-01-09               NaN               NaN  \n",
       "...                             ...               ...  \n",
       "ZYXI   2020-09-14          0.264972          0.870739  \n",
       "       2020-09-15          0.264972          0.870739  \n",
       "       2020-09-16          0.264972          0.870739  \n",
       "       2020-09-17          0.264972          0.870739  \n",
       "       2020-09-18          0.264972          0.870739  \n",
       "\n",
       "[7113810 rows x 12 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_growth_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate valuation signals for the stocks, such as P/E, P/Sales, etc. These are calculated from the share-prices and data from the financial reports. Because the data-hub has already loaded the required datasets in the function-calls above, the data is merely reused here, and the data-hub can proceed directly to computing the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache-file 'val_signals-739b68a6.pickle' on disk (8 days old).\n",
      "- Loading from disk ... Done!\n",
      "CPU times: user 31.2 ms, sys: 230 ms, total: 261 ms\n",
      "Wall time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_val_signals = hub.val_signals(variant='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now combine all the signals into a single DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.88 s, sys: 555 ms, total: 3.43 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine the DataFrames.\n",
    "dfs = [df_fin_signals, df_growth_signals, df_val_signals]\n",
    "df_signals = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then show the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>(Dividends + Share Buyback) / FCF</th>\n",
       "      <th>Asset Turnover</th>\n",
       "      <th>CapEx / (Depr + Amor)</th>\n",
       "      <th>Current Ratio</th>\n",
       "      <th>Debt Ratio</th>\n",
       "      <th>Dividends / FCF</th>\n",
       "      <th>Gross Profit Margin</th>\n",
       "      <th>Interest Coverage</th>\n",
       "      <th>Inventory Turnover</th>\n",
       "      <th>Log Revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Earnings Yield</th>\n",
       "      <th>FCF Yield</th>\n",
       "      <th>Market-Cap</th>\n",
       "      <th>P/Cash</th>\n",
       "      <th>P/E</th>\n",
       "      <th>P/FCF</th>\n",
       "      <th>P/NCAV</th>\n",
       "      <th>P/NetNet</th>\n",
       "      <th>P/Sales</th>\n",
       "      <th>Price to Book Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">A</th>\n",
       "      <th>2010-09-29</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036491</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>1.142744e+10</td>\n",
       "      <td>4.931996</td>\n",
       "      <td>27.403921</td>\n",
       "      <td>25.450857</td>\n",
       "      <td>-20.589973</td>\n",
       "      <td>-3.763357</td>\n",
       "      <td>2.269600</td>\n",
       "      <td>4.066703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-09-30</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035627</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>1.170453e+10</td>\n",
       "      <td>5.051587</td>\n",
       "      <td>28.068411</td>\n",
       "      <td>26.067990</td>\n",
       "      <td>-21.089239</td>\n",
       "      <td>-3.854611</td>\n",
       "      <td>2.324633</td>\n",
       "      <td>4.165312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-10-01</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035247</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>1.183080e+10</td>\n",
       "      <td>5.106084</td>\n",
       "      <td>28.371217</td>\n",
       "      <td>26.349215</td>\n",
       "      <td>-21.316752</td>\n",
       "      <td>-3.896195</td>\n",
       "      <td>2.349712</td>\n",
       "      <td>4.210248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-10-04</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>1.143094e+10</td>\n",
       "      <td>4.933510</td>\n",
       "      <td>27.412332</td>\n",
       "      <td>25.458669</td>\n",
       "      <td>-20.596293</td>\n",
       "      <td>-3.764513</td>\n",
       "      <td>2.270296</td>\n",
       "      <td>4.067951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-10-05</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035983</td>\n",
       "      <td>0.038744</td>\n",
       "      <td>1.158878e+10</td>\n",
       "      <td>5.001631</td>\n",
       "      <td>27.790839</td>\n",
       "      <td>25.810200</td>\n",
       "      <td>-20.880685</td>\n",
       "      <td>-3.816493</td>\n",
       "      <td>2.301644</td>\n",
       "      <td>4.124121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   (Dividends + Share Buyback) / FCF  Asset Turnover  \\\n",
       "Ticker Date                                                            \n",
       "A      2010-09-29                           0.171492        0.553297   \n",
       "       2010-09-30                           0.171492        0.553297   \n",
       "       2010-10-01                           0.171492        0.553297   \n",
       "       2010-10-04                           0.171492        0.553297   \n",
       "       2010-10-05                           0.171492        0.553297   \n",
       "\n",
       "                   CapEx / (Depr + Amor)  Current Ratio  Debt Ratio  \\\n",
       "Ticker Date                                                           \n",
       "A      2010-09-29               0.622857       1.966061    0.404176   \n",
       "       2010-09-30               0.622857       1.966061    0.404176   \n",
       "       2010-10-01               0.622857       1.966061    0.404176   \n",
       "       2010-10-04               0.622857       1.966061    0.404176   \n",
       "       2010-10-05               0.622857       1.966061    0.404176   \n",
       "\n",
       "                   Dividends / FCF  Gross Profit Margin  Interest Coverage  \\\n",
       "Ticker Date                                                                  \n",
       "A      2010-09-29             -0.0             0.542205           5.636364   \n",
       "       2010-09-30             -0.0             0.542205           5.636364   \n",
       "       2010-10-01             -0.0             0.542205           5.636364   \n",
       "       2010-10-04             -0.0             0.542205           5.636364   \n",
       "       2010-10-05             -0.0             0.542205           5.636364   \n",
       "\n",
       "                   Inventory Turnover  Log Revenue  ...  Earnings Yield  \\\n",
       "Ticker Date                                         ...                   \n",
       "A      2010-09-29            7.318314     9.701999  ...        0.036491   \n",
       "       2010-09-30            7.318314     9.701999  ...        0.035627   \n",
       "       2010-10-01            7.318314     9.701999  ...        0.035247   \n",
       "       2010-10-04            7.318314     9.701999  ...        0.036480   \n",
       "       2010-10-05            7.318314     9.701999  ...        0.035983   \n",
       "\n",
       "                   FCF Yield    Market-Cap    P/Cash        P/E      P/FCF  \\\n",
       "Ticker Date                                                                  \n",
       "A      2010-09-29   0.039291  1.142744e+10  4.931996  27.403921  25.450857   \n",
       "       2010-09-30   0.038361  1.170453e+10  5.051587  28.068411  26.067990   \n",
       "       2010-10-01   0.037952  1.183080e+10  5.106084  28.371217  26.349215   \n",
       "       2010-10-04   0.039279  1.143094e+10  4.933510  27.412332  25.458669   \n",
       "       2010-10-05   0.038744  1.158878e+10  5.001631  27.790839  25.810200   \n",
       "\n",
       "                      P/NCAV  P/NetNet   P/Sales  Price to Book Value  \n",
       "Ticker Date                                                            \n",
       "A      2010-09-29 -20.589973 -3.763357  2.269600             4.066703  \n",
       "       2010-09-30 -21.089239 -3.854611  2.324633             4.165312  \n",
       "       2010-10-01 -21.316752 -3.896195  2.349712             4.210248  \n",
       "       2010-10-04 -20.596293 -3.764513  2.270296             4.067951  \n",
       "       2010-10-05 -20.880685 -3.816493  2.301644             4.124121  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_signals.dropna(how='all').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Some of the signals have a lot of missing data which cannot be handled by TensorFlow. Let us first see the fraction of each signal-column that is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R&D / Gross Profit                   0.649276\n",
       "Return on Research Capital           0.649276\n",
       "R&D / Revenue                        0.614202\n",
       "Dividend Yield                       0.438901\n",
       "Inventory Turnover                   0.290166\n",
       "Debt Ratio                           0.262063\n",
       "Net Acquisitions / Total Assets      0.257896\n",
       "Sales Growth                         0.147292\n",
       "Sales Growth YOY                     0.144781\n",
       "FCF Growth                           0.135264\n",
       "Earnings Growth                      0.135264\n",
       "Assets Growth                        0.135264\n",
       "FCF Growth YOY                       0.133461\n",
       "Earnings Growth YOY                  0.133446\n",
       "Assets Growth YOY                    0.133446\n",
       "Interest Coverage                    0.097272\n",
       "Gross Profit Margin                  0.094301\n",
       "Sales Growth QOQ                     0.053825\n",
       "Assets Growth QOQ                    0.040359\n",
       "Earnings Growth QOQ                  0.040359\n",
       "FCF Growth QOQ                       0.040359\n",
       "CapEx / (Depr + Amor)                0.038557\n",
       "P/Sales                              0.028200\n",
       "Log Revenue                          0.020019\n",
       "Net Profit Margin                    0.019926\n",
       "Asset Turnover                       0.019926\n",
       "P/Cash                               0.019259\n",
       "P/E                                  0.018029\n",
       "FCF Yield                            0.018029\n",
       "P/FCF                                0.018029\n",
       "P/NCAV                               0.018029\n",
       "P/NetNet                             0.018029\n",
       "Market-Cap                           0.018029\n",
       "Price to Book Value                  0.018029\n",
       "Earnings Yield                       0.018029\n",
       "Quick Ratio                          0.010978\n",
       "Share Buyback / FCF                  0.009747\n",
       "Return on Equity                     0.009747\n",
       "Return on Assets                     0.009747\n",
       "Dividends / FCF                      0.009747\n",
       "Current Ratio                        0.009747\n",
       "(Dividends + Share Buyback) / FCF    0.009747\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all rows with only NaN values.\n",
    "df = df_signals.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "# For each column, show the fraction of the rows that are NaN.\n",
    "(df.isnull().sum() / len(df)).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(Dividends + Share Buyback) / FCF</th>\n",
       "      <th>Asset Turnover</th>\n",
       "      <th>CapEx / (Depr + Amor)</th>\n",
       "      <th>Current Ratio</th>\n",
       "      <th>Debt Ratio</th>\n",
       "      <th>Dividends / FCF</th>\n",
       "      <th>Gross Profit Margin</th>\n",
       "      <th>Interest Coverage</th>\n",
       "      <th>Inventory Turnover</th>\n",
       "      <th>Log Revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Earnings Yield</th>\n",
       "      <th>FCF Yield</th>\n",
       "      <th>Market-Cap</th>\n",
       "      <th>P/Cash</th>\n",
       "      <th>P/E</th>\n",
       "      <th>P/FCF</th>\n",
       "      <th>P/NCAV</th>\n",
       "      <th>P/NetNet</th>\n",
       "      <th>P/Sales</th>\n",
       "      <th>Price to Book Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036491</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>1.142744e+10</td>\n",
       "      <td>4.931996</td>\n",
       "      <td>27.403921</td>\n",
       "      <td>25.450857</td>\n",
       "      <td>-20.589973</td>\n",
       "      <td>-3.763357</td>\n",
       "      <td>2.269600</td>\n",
       "      <td>4.066703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035627</td>\n",
       "      <td>0.038361</td>\n",
       "      <td>1.170453e+10</td>\n",
       "      <td>5.051587</td>\n",
       "      <td>28.068411</td>\n",
       "      <td>26.067990</td>\n",
       "      <td>-21.089239</td>\n",
       "      <td>-3.854611</td>\n",
       "      <td>2.324633</td>\n",
       "      <td>4.165312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035247</td>\n",
       "      <td>0.037952</td>\n",
       "      <td>1.183080e+10</td>\n",
       "      <td>5.106084</td>\n",
       "      <td>28.371217</td>\n",
       "      <td>26.349215</td>\n",
       "      <td>-21.316752</td>\n",
       "      <td>-3.896195</td>\n",
       "      <td>2.349712</td>\n",
       "      <td>4.210248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>0.039279</td>\n",
       "      <td>1.143094e+10</td>\n",
       "      <td>4.933510</td>\n",
       "      <td>27.412332</td>\n",
       "      <td>25.458669</td>\n",
       "      <td>-20.596293</td>\n",
       "      <td>-3.764513</td>\n",
       "      <td>2.270296</td>\n",
       "      <td>4.067951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.171492</td>\n",
       "      <td>0.553297</td>\n",
       "      <td>0.622857</td>\n",
       "      <td>1.966061</td>\n",
       "      <td>0.404176</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.542205</td>\n",
       "      <td>5.636364</td>\n",
       "      <td>7.318314</td>\n",
       "      <td>9.701999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035983</td>\n",
       "      <td>0.038744</td>\n",
       "      <td>1.158878e+10</td>\n",
       "      <td>5.001631</td>\n",
       "      <td>27.790839</td>\n",
       "      <td>25.810200</td>\n",
       "      <td>-20.880685</td>\n",
       "      <td>-3.816493</td>\n",
       "      <td>2.301644</td>\n",
       "      <td>4.124121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265802</th>\n",
       "      <td>-0.055400</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>12.846824</td>\n",
       "      <td>7.781540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019939</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>5.483703e+08</td>\n",
       "      <td>32.417254</td>\n",
       "      <td>50.152759</td>\n",
       "      <td>85.575885</td>\n",
       "      <td>27.376081</td>\n",
       "      <td>36.765128</td>\n",
       "      <td>9.068468</td>\n",
       "      <td>20.342407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265803</th>\n",
       "      <td>-0.055400</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>12.846824</td>\n",
       "      <td>7.781540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020001</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>5.466598e+08</td>\n",
       "      <td>32.316140</td>\n",
       "      <td>49.996325</td>\n",
       "      <td>85.308961</td>\n",
       "      <td>27.290690</td>\n",
       "      <td>36.650452</td>\n",
       "      <td>9.040182</td>\n",
       "      <td>20.278956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265804</th>\n",
       "      <td>-0.055400</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>12.846824</td>\n",
       "      <td>7.781540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>5.476861e+08</td>\n",
       "      <td>32.376808</td>\n",
       "      <td>50.090186</td>\n",
       "      <td>85.469115</td>\n",
       "      <td>27.341925</td>\n",
       "      <td>36.719258</td>\n",
       "      <td>9.057154</td>\n",
       "      <td>20.317027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265805</th>\n",
       "      <td>-0.055400</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>12.846824</td>\n",
       "      <td>7.781540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>5.890790e+08</td>\n",
       "      <td>34.823775</td>\n",
       "      <td>53.875890</td>\n",
       "      <td>91.928680</td>\n",
       "      <td>29.408366</td>\n",
       "      <td>39.494417</td>\n",
       "      <td>9.741673</td>\n",
       "      <td>21.852542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265806</th>\n",
       "      <td>-0.055400</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>12.846824</td>\n",
       "      <td>7.781540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>5.883948e+08</td>\n",
       "      <td>34.783329</td>\n",
       "      <td>53.813316</td>\n",
       "      <td>91.821910</td>\n",
       "      <td>29.374210</td>\n",
       "      <td>39.448547</td>\n",
       "      <td>9.730359</td>\n",
       "      <td>21.827162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4265807 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         (Dividends + Share Buyback) / FCF  Asset Turnover  \\\n",
       "0                                 0.171492        0.553297   \n",
       "1                                 0.171492        0.553297   \n",
       "2                                 0.171492        0.553297   \n",
       "3                                 0.171492        0.553297   \n",
       "4                                 0.171492        0.553297   \n",
       "...                                    ...             ...   \n",
       "4265802                          -0.055400        1.645039   \n",
       "4265803                          -0.055400        1.645039   \n",
       "4265804                          -0.055400        1.645039   \n",
       "4265805                          -0.055400        1.645039   \n",
       "4265806                          -0.055400        1.645039   \n",
       "\n",
       "         CapEx / (Depr + Amor)  Current Ratio  Debt Ratio  Dividends / FCF  \\\n",
       "0                     0.622857       1.966061    0.404176        -0.000000   \n",
       "1                     0.622857       1.966061    0.404176        -0.000000   \n",
       "2                     0.622857       1.966061    0.404176        -0.000000   \n",
       "3                     0.622857       1.966061    0.404176        -0.000000   \n",
       "4                     0.622857       1.966061    0.404176        -0.000000   \n",
       "...                        ...            ...         ...              ...   \n",
       "4265802                    NaN       4.955648         NaN         0.000468   \n",
       "4265803                    NaN       4.955648         NaN         0.000468   \n",
       "4265804                    NaN       4.955648         NaN         0.000468   \n",
       "4265805                    NaN       4.955648         NaN         0.000468   \n",
       "4265806                    NaN       4.955648         NaN         0.000468   \n",
       "\n",
       "         Gross Profit Margin  Interest Coverage  Inventory Turnover  \\\n",
       "0                   0.542205           5.636364            7.318314   \n",
       "1                   0.542205           5.636364            7.318314   \n",
       "2                   0.542205           5.636364            7.318314   \n",
       "3                   0.542205           5.636364            7.318314   \n",
       "4                   0.542205           5.636364            7.318314   \n",
       "...                      ...                ...                 ...   \n",
       "4265802             0.792558         912.071429           12.846824   \n",
       "4265803             0.792558         912.071429           12.846824   \n",
       "4265804             0.792558         912.071429           12.846824   \n",
       "4265805             0.792558         912.071429           12.846824   \n",
       "4265806             0.792558         912.071429           12.846824   \n",
       "\n",
       "         Log Revenue  ...  Earnings Yield  FCF Yield    Market-Cap     P/Cash  \\\n",
       "0           9.701999  ...        0.036491   0.039291  1.142744e+10   4.931996   \n",
       "1           9.701999  ...        0.035627   0.038361  1.170453e+10   5.051587   \n",
       "2           9.701999  ...        0.035247   0.037952  1.183080e+10   5.106084   \n",
       "3           9.701999  ...        0.036480   0.039279  1.143094e+10   4.933510   \n",
       "4           9.701999  ...        0.035983   0.038744  1.158878e+10   5.001631   \n",
       "...              ...  ...             ...        ...           ...        ...   \n",
       "4265802     7.781540  ...        0.019939   0.011686  5.483703e+08  32.417254   \n",
       "4265803     7.781540  ...        0.020001   0.011722  5.466598e+08  32.316140   \n",
       "4265804     7.781540  ...        0.019964   0.011700  5.476861e+08  32.376808   \n",
       "4265805     7.781540  ...        0.018561   0.010878  5.890790e+08  34.823775   \n",
       "4265806     7.781540  ...        0.018583   0.010891  5.883948e+08  34.783329   \n",
       "\n",
       "               P/E      P/FCF     P/NCAV   P/NetNet   P/Sales  \\\n",
       "0        27.403921  25.450857 -20.589973  -3.763357  2.269600   \n",
       "1        28.068411  26.067990 -21.089239  -3.854611  2.324633   \n",
       "2        28.371217  26.349215 -21.316752  -3.896195  2.349712   \n",
       "3        27.412332  25.458669 -20.596293  -3.764513  2.270296   \n",
       "4        27.790839  25.810200 -20.880685  -3.816493  2.301644   \n",
       "...            ...        ...        ...        ...       ...   \n",
       "4265802  50.152759  85.575885  27.376081  36.765128  9.068468   \n",
       "4265803  49.996325  85.308961  27.290690  36.650452  9.040182   \n",
       "4265804  50.090186  85.469115  27.341925  36.719258  9.057154   \n",
       "4265805  53.875890  91.928680  29.408366  39.494417  9.741673   \n",
       "4265806  53.813316  91.821910  29.374210  39.448547  9.730359   \n",
       "\n",
       "         Price to Book Value  \n",
       "0                   4.066703  \n",
       "1                   4.165312  \n",
       "2                   4.210248  \n",
       "3                   4.067951  \n",
       "4                   4.124121  \n",
       "...                      ...  \n",
       "4265802            20.342407  \n",
       "4265803            20.278956  \n",
       "4265804            20.317027  \n",
       "4265805            21.852542  \n",
       "4265806            21.827162  \n",
       "\n",
       "[4265807 rows x 42 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remove all signals that have more than 25% missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Debt Ratio', 'Dividend Yield', 'Inventory Turnover',\n",
       "       'Net Acquisitions / Total Assets', 'R&D / Gross Profit',\n",
       "       'R&D / Revenue', 'Return on Research Capital'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of the columns before removing any.\n",
    "columns_before = df_signals.columns\n",
    "\n",
    "# Threshold for the number of rows that must be NaN for each column.\n",
    "thresh = 0.75 * len(df_signals.dropna(how='all'))\n",
    "\n",
    "# Remove all columns which don't have sufficient data.\n",
    "df_signals = df_signals.dropna(axis='columns', thresh=thresh)\n",
    "\n",
    "# List of the columns after the removal.\n",
    "columns_after = df_signals.columns\n",
    "\n",
    "# Show the columns that were removed.\n",
    "columns_before.difference(columns_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>(Dividends + Share Buyback) / FCF</th>\n",
       "      <th>Asset Turnover</th>\n",
       "      <th>CapEx / (Depr + Amor)</th>\n",
       "      <th>Current Ratio</th>\n",
       "      <th>Dividends / FCF</th>\n",
       "      <th>Gross Profit Margin</th>\n",
       "      <th>Interest Coverage</th>\n",
       "      <th>Log Revenue</th>\n",
       "      <th>Net Profit Margin</th>\n",
       "      <th>Quick Ratio</th>\n",
       "      <th>Return on Assets</th>\n",
       "      <th>Return on Equity</th>\n",
       "      <th>Share Buyback / FCF</th>\n",
       "      <th>Assets Growth</th>\n",
       "      <th>Assets Growth QOQ</th>\n",
       "      <th>Assets Growth YOY</th>\n",
       "      <th>Earnings Growth</th>\n",
       "      <th>Earnings Growth QOQ</th>\n",
       "      <th>Earnings Growth YOY</th>\n",
       "      <th>FCF Growth</th>\n",
       "      <th>FCF Growth QOQ</th>\n",
       "      <th>FCF Growth YOY</th>\n",
       "      <th>Sales Growth</th>\n",
       "      <th>Sales Growth QOQ</th>\n",
       "      <th>Sales Growth YOY</th>\n",
       "      <th>Earnings Yield</th>\n",
       "      <th>FCF Yield</th>\n",
       "      <th>Market-Cap</th>\n",
       "      <th>P/Cash</th>\n",
       "      <th>P/E</th>\n",
       "      <th>P/FCF</th>\n",
       "      <th>P/NCAV</th>\n",
       "      <th>P/NetNet</th>\n",
       "      <th>P/Sales</th>\n",
       "      <th>Price to Book Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">A</th>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ZYXI</th>\n",
       "      <th>2020-09-16</th>\n",
       "      <td>-0.0554</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>7.78154</td>\n",
       "      <td>0.180817</td>\n",
       "      <td>4.016611</td>\n",
       "      <td>0.297451</td>\n",
       "      <td>0.405609</td>\n",
       "      <td>-0.055868</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "      <td>0.019964</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>547686090.0</td>\n",
       "      <td>32.376808</td>\n",
       "      <td>50.090186</td>\n",
       "      <td>85.469115</td>\n",
       "      <td>27.341925</td>\n",
       "      <td>36.719258</td>\n",
       "      <td>9.057154</td>\n",
       "      <td>20.317027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-17</th>\n",
       "      <td>-0.0554</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>7.78154</td>\n",
       "      <td>0.180817</td>\n",
       "      <td>4.016611</td>\n",
       "      <td>0.297451</td>\n",
       "      <td>0.405609</td>\n",
       "      <td>-0.055868</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>589078980.0</td>\n",
       "      <td>34.823775</td>\n",
       "      <td>53.875890</td>\n",
       "      <td>91.928680</td>\n",
       "      <td>29.408366</td>\n",
       "      <td>39.494417</td>\n",
       "      <td>9.741673</td>\n",
       "      <td>21.852542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-18</th>\n",
       "      <td>-0.0554</td>\n",
       "      <td>1.645039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.955648</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.792558</td>\n",
       "      <td>912.071429</td>\n",
       "      <td>7.78154</td>\n",
       "      <td>0.180817</td>\n",
       "      <td>4.016611</td>\n",
       "      <td>0.297451</td>\n",
       "      <td>0.405609</td>\n",
       "      <td>-0.055868</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.106466</td>\n",
       "      <td>0.683181</td>\n",
       "      <td>0.124203</td>\n",
       "      <td>0.027239</td>\n",
       "      <td>0.395467</td>\n",
       "      <td>-0.212002</td>\n",
       "      <td>5.847761</td>\n",
       "      <td>2.823333</td>\n",
       "      <td>0.636049</td>\n",
       "      <td>0.264972</td>\n",
       "      <td>0.870739</td>\n",
       "      <td>0.018583</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>588394800.0</td>\n",
       "      <td>34.783329</td>\n",
       "      <td>53.813316</td>\n",
       "      <td>91.821910</td>\n",
       "      <td>29.374210</td>\n",
       "      <td>39.448547</td>\n",
       "      <td>9.730359</td>\n",
       "      <td>21.827162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7113810 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   (Dividends + Share Buyback) / FCF  Asset Turnover  \\\n",
       "Ticker Date                                                            \n",
       "A      2007-01-03                                NaN             NaN   \n",
       "       2007-01-04                                NaN             NaN   \n",
       "       2007-01-05                                NaN             NaN   \n",
       "...                                              ...             ...   \n",
       "ZYXI   2020-09-16                            -0.0554        1.645039   \n",
       "       2020-09-17                            -0.0554        1.645039   \n",
       "       2020-09-18                            -0.0554        1.645039   \n",
       "\n",
       "                   CapEx / (Depr + Amor)  Current Ratio  Dividends / FCF  \\\n",
       "Ticker Date                                                                \n",
       "A      2007-01-03                    NaN            NaN              NaN   \n",
       "       2007-01-04                    NaN            NaN              NaN   \n",
       "       2007-01-05                    NaN            NaN              NaN   \n",
       "...                                  ...            ...              ...   \n",
       "ZYXI   2020-09-16                    NaN       4.955648         0.000468   \n",
       "       2020-09-17                    NaN       4.955648         0.000468   \n",
       "       2020-09-18                    NaN       4.955648         0.000468   \n",
       "\n",
       "                   Gross Profit Margin  Interest Coverage  Log Revenue  \\\n",
       "Ticker Date                                                              \n",
       "A      2007-01-03                  NaN                NaN          NaN   \n",
       "       2007-01-04                  NaN                NaN          NaN   \n",
       "       2007-01-05                  NaN                NaN          NaN   \n",
       "...                                ...                ...          ...   \n",
       "ZYXI   2020-09-16             0.792558         912.071429      7.78154   \n",
       "       2020-09-17             0.792558         912.071429      7.78154   \n",
       "       2020-09-18             0.792558         912.071429      7.78154   \n",
       "\n",
       "                   Net Profit Margin  Quick Ratio  Return on Assets  \\\n",
       "Ticker Date                                                           \n",
       "A      2007-01-03                NaN          NaN               NaN   \n",
       "       2007-01-04                NaN          NaN               NaN   \n",
       "       2007-01-05                NaN          NaN               NaN   \n",
       "...                              ...          ...               ...   \n",
       "ZYXI   2020-09-16           0.180817     4.016611          0.297451   \n",
       "       2020-09-17           0.180817     4.016611          0.297451   \n",
       "       2020-09-18           0.180817     4.016611          0.297451   \n",
       "\n",
       "                   Return on Equity  Share Buyback / FCF  Assets Growth  \\\n",
       "Ticker Date                                                               \n",
       "A      2007-01-03               NaN                  NaN            NaN   \n",
       "       2007-01-04               NaN                  NaN            NaN   \n",
       "       2007-01-05               NaN                  NaN            NaN   \n",
       "...                             ...                  ...            ...   \n",
       "ZYXI   2020-09-16          0.405609            -0.055868       0.683181   \n",
       "       2020-09-17          0.405609            -0.055868       0.683181   \n",
       "       2020-09-18          0.405609            -0.055868       0.683181   \n",
       "\n",
       "                   Assets Growth QOQ  Assets Growth YOY  Earnings Growth  \\\n",
       "Ticker Date                                                                \n",
       "A      2007-01-03                NaN                NaN              NaN   \n",
       "       2007-01-04                NaN                NaN              NaN   \n",
       "       2007-01-05                NaN                NaN              NaN   \n",
       "...                              ...                ...              ...   \n",
       "ZYXI   2020-09-16           0.106466           0.683181         0.124203   \n",
       "       2020-09-17           0.106466           0.683181         0.124203   \n",
       "       2020-09-18           0.106466           0.683181         0.124203   \n",
       "\n",
       "                   Earnings Growth QOQ  Earnings Growth YOY  FCF Growth  \\\n",
       "Ticker Date                                                               \n",
       "A      2007-01-03                  NaN                  NaN         NaN   \n",
       "       2007-01-04                  NaN                  NaN         NaN   \n",
       "       2007-01-05                  NaN                  NaN         NaN   \n",
       "...                                ...                  ...         ...   \n",
       "ZYXI   2020-09-16             0.027239             0.395467   -0.212002   \n",
       "       2020-09-17             0.027239             0.395467   -0.212002   \n",
       "       2020-09-18             0.027239             0.395467   -0.212002   \n",
       "\n",
       "                   FCF Growth QOQ  FCF Growth YOY  Sales Growth  \\\n",
       "Ticker Date                                                       \n",
       "A      2007-01-03             NaN             NaN           NaN   \n",
       "       2007-01-04             NaN             NaN           NaN   \n",
       "       2007-01-05             NaN             NaN           NaN   \n",
       "...                           ...             ...           ...   \n",
       "ZYXI   2020-09-16        5.847761        2.823333      0.636049   \n",
       "       2020-09-17        5.847761        2.823333      0.636049   \n",
       "       2020-09-18        5.847761        2.823333      0.636049   \n",
       "\n",
       "                   Sales Growth QOQ  Sales Growth YOY  Earnings Yield  \\\n",
       "Ticker Date                                                             \n",
       "A      2007-01-03               NaN               NaN             NaN   \n",
       "       2007-01-04               NaN               NaN             NaN   \n",
       "       2007-01-05               NaN               NaN             NaN   \n",
       "...                             ...               ...             ...   \n",
       "ZYXI   2020-09-16          0.264972          0.870739        0.019964   \n",
       "       2020-09-17          0.264972          0.870739        0.018561   \n",
       "       2020-09-18          0.264972          0.870739        0.018583   \n",
       "\n",
       "                   FCF Yield   Market-Cap     P/Cash        P/E      P/FCF  \\\n",
       "Ticker Date                                                                  \n",
       "A      2007-01-03        NaN          NaN        NaN        NaN        NaN   \n",
       "       2007-01-04        NaN          NaN        NaN        NaN        NaN   \n",
       "       2007-01-05        NaN          NaN        NaN        NaN        NaN   \n",
       "...                      ...          ...        ...        ...        ...   \n",
       "ZYXI   2020-09-16   0.011700  547686090.0  32.376808  50.090186  85.469115   \n",
       "       2020-09-17   0.010878  589078980.0  34.823775  53.875890  91.928680   \n",
       "       2020-09-18   0.010891  588394800.0  34.783329  53.813316  91.821910   \n",
       "\n",
       "                      P/NCAV   P/NetNet   P/Sales  Price to Book Value  \n",
       "Ticker Date                                                             \n",
       "A      2007-01-03        NaN        NaN       NaN                  NaN  \n",
       "       2007-01-04        NaN        NaN       NaN                  NaN  \n",
       "       2007-01-05        NaN        NaN       NaN                  NaN  \n",
       "...                      ...        ...       ...                  ...  \n",
       "ZYXI   2020-09-16  27.341925  36.719258  9.057154            20.317027  \n",
       "       2020-09-17  29.408366  39.494417  9.741673            21.852542  \n",
       "       2020-09-18  29.374210  39.448547  9.730359            21.827162  \n",
       "\n",
       "[7113810 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print all columns\n",
    "with pd.option_context('display.max_rows', 6, 'display.max_columns', None): \n",
    "    display(df_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 Year Returns\n",
    "\n",
    "We want to try and predict the average 1-3 year returns. We will actually consider the mean log-returns, because that is easier to calculate, but the result is nearly the same as the non-log mean returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the new column for the returns.\n",
    "TOTAL_RETURN_1_3Y = 'Total Return 1-3 Years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the mean log-returns for all 1-3 year periods.\n",
    "df_returns_1_3y = \\\n",
    "    hub.mean_log_returns(name=TOTAL_RETURN_1_3Y,\n",
    "                         future=True, annualized=True,\n",
    "                         min_years=1, max_years=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Year Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache-file 'mean_log_change-0cd85b4d.pickle' on disk (0 days old).\n",
      "- Loading from disk ... Done!\n",
      "CPU times: user 87 ms, sys: 156 ms, total: 243 ms\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TOTAL_RETURN_1_1Y = 'Total Return 1-1 Years'\n",
    "# Calculate the mean log-returns for only 1 year periods.\n",
    "df_returns_1_1y = \\\n",
    "    hub.mean_log_returns(name=TOTAL_RETURN_1_1Y,\n",
    "                         future=True, annualized=True,\n",
    "                         min_years=0.5, max_years=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ticker  Date      \n",
       "A       2007-01-03    0.103831\n",
       "        2007-01-04    0.097915\n",
       "        2007-01-05    0.109227\n",
       "        2007-01-08    0.112659\n",
       "        2007-01-09    0.109247\n",
       "                        ...   \n",
       "ZYXI    2020-09-14         NaN\n",
       "        2020-09-15         NaN\n",
       "        2020-09-16         NaN\n",
       "        2020-09-17         NaN\n",
       "        2020-09-18         NaN\n",
       "Name: Total Return 1-1 Years, Length: 7113810, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_returns_1_1y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## price next year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(   'A', '2007-01-03'),\n",
       "            (   'A', '2007-01-04'),\n",
       "            (   'A', '2007-01-05'),\n",
       "            (   'A', '2007-01-08'),\n",
       "            (   'A', '2007-01-09'),\n",
       "            (   'A', '2007-01-10'),\n",
       "            (   'A', '2007-01-11'),\n",
       "            (   'A', '2007-01-12'),\n",
       "            (   'A', '2007-01-16'),\n",
       "            (   'A', '2007-01-17'),\n",
       "            ...\n",
       "            ('ZYXI', '2020-09-04'),\n",
       "            ('ZYXI', '2020-09-08'),\n",
       "            ('ZYXI', '2020-09-09'),\n",
       "            ('ZYXI', '2020-09-10'),\n",
       "            ('ZYXI', '2020-09-11'),\n",
       "            ('ZYXI', '2020-09-14'),\n",
       "            ('ZYXI', '2020-09-15'),\n",
       "            ('ZYXI', '2020-09-16'),\n",
       "            ('ZYXI', '2020-09-17'),\n",
       "            ('ZYXI', '2020-09-18')],\n",
       "           names=['Ticker', 'Date'], length=7113810)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_signals.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/simfin-env/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/simfin-env/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/simfin-env/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15982/2657435259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_signals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%Y%m%d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_signals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_signals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Ticker\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/simfin-env/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/simfin-env/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "df_signals.index\n",
    "df_signals['Date'] = pd.to_datetime(df_signals['Date'],format='%Y%m%d')\n",
    "df_signals['Year'] = pd.DatetimeIndex(df_signals['Date']).year\n",
    "df_signals.groupby(by=[\"Ticker\",\"Year\"], dropna=False).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_1y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Signals & Returns\n",
    "\n",
    "We then combine the signals and returns into a single DataFrame to align the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfs = [df_signals, df_returns_1_3y]\n",
    "df_sig_rets = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfs = [df_signals, df_returns_1_1y]\n",
    "df_sig_rets = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all columns\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n",
    "    display(df_sig_rets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The data also contains outliers and still has missing values, so let us fix that now. A common method for removing outliers is so-called *Winsorization* of the data. It basically just limits or clips the data between e.g. the 5% and 95% quantiles of the data. We will Winsorize both the stock-returns and all the signals. Then we remove all rows with missing values, and finally we remove tickers that have less than 200 rows of data.\n",
    "\n",
    "There are more than 5 million data-rows before this cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sig_rets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signals.shape\n",
    "df_sig_rets['Total Return 1-3 Years']\n",
    "print(df_sig_rets.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signals.shape\n",
    "df_sig_rets['Total Return 1-1 Years']\n",
    "print(df_sig_rets.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Clip both the signals and returns at their 5% and 95% quantiles.\n",
    "# We do not set them to NaN because it would remove too much data.\n",
    "df_sig_rets = sf.winsorize(df_sig_rets)\n",
    "\n",
    "# Remove all rows with missing values (NaN)\n",
    "# because TensorFlow cannot handle that.\n",
    "df_sig_rets = df_sig_rets.dropna(how='any')\n",
    "\n",
    "# Remove all tickers which have less than 200 data-rows.\n",
    "df_sig_rets = df_sig_rets.groupby(TICKER) \\\n",
    "                .filter(lambda df: len(df)>200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data-cleaning has removed about 80% of the rows, which is a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig_rets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig_rets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all columns\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n",
    "    display(df_sig_rets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig_rets.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by one of the index\n",
    "x=df_sig_rets[df_sig_rets.index.get_level_values('Ticker').isin(['IBM'])].sample(10)\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n",
    "    display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n",
    "    display(df_sig_rets.loc['A'].sample(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that so many rows have been removed, is that TensorFlow cannot handle rows with missing values (NaN). Some missing values could be set to a default value, e.g. the Dividend Yield could be set to 0. Other columns could be omitted because they don't have any predictive power anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training / Test-Sets\n",
    "\n",
    "When training a Machine Learning model, we typically split the dataset into training- and test-sets. Only the training-set is used to train the model, after which the model is used on the test-set, to assess how the model performs on data it has not seen during training.\n",
    "\n",
    "If we randomly divide all the data-rows into training- and test-sets, we would most likely get data for every stock-ticker in both the training- and test-sets, which would probably make them highly correlated. To avoid this, we split the dataset according to stock-tickers, so a ticker belongs to either the training- or test-set, but not both. We use 80% of all the tickers in the training-set, and 20% in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all unique stock-tickers in the dataset.\n",
    "tickers = df_sig_rets.reset_index()[TICKER].unique()\n",
    "\n",
    "# Split the tickers into training- and test-sets.\n",
    "tickers_train, tickers_test = \\\n",
    "    train_test_split(tickers, train_size=0.8, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_train, tickers_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have lists of tickers for the training- and test-sets, we can select those rows from the DataFrame with signals and stock-returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_train = df_sig_rets.loc[tickers_train]\n",
    "df_test = df_sig_rets.loc[tickers_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then split these DataFrames into signals X and stock-returns y, that the Machine Learning model must try and find a mapping between. We use a common notation where the capital X indicates a 2-dimensional array or matrix, and the lower-case y indicates a 1-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames with signals for training- and test-sets.\n",
    "X_train = df_train.drop(columns=[TOTAL_RETURN_1_3Y])\n",
    "X_test = df_test.drop(columns=[TOTAL_RETURN_1_3Y])\n",
    "\n",
    "# DataFrames with returns for training- and test-sets.\n",
    "y_train = df_train[TOTAL_RETURN_1_3Y]\n",
    "y_test = df_test[TOTAL_RETURN_1_3Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames with signals for training- and test-sets.\n",
    "X_train = df_train.drop(columns=[TOTAL_RETURN_1_1Y])\n",
    "X_test = df_test.drop(columns=[TOTAL_RETURN_1_1Y])\n",
    "\n",
    "# DataFrames with returns for training- and test-sets.\n",
    "y_train = df_train[TOTAL_RETURN_1_1Y]\n",
    "y_test = df_test[TOTAL_RETURN_1_1Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check that the shapes of these arrays look reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also get the number of signals, which will be convenient later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_signals = X_train.shape[1]\n",
    "num_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Signals\n",
    "\n",
    "Neural Networks require that the input has \"moderate\" values e.g. between 0 and 1, or mean 0 and 1 standard deviation, which is the method we use here. It is not necessary to scale the stock-returns because they have already been Winsorized to remove outliers, and they are centered around 0 with a small standard deviation.\n",
    "\n",
    "If you want to input new data to the Neural Network, then you have to run the signals through the scaler first, because the Neural Network has been tuned to the scaled signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler and fit it to the signals in the training-data.\n",
    "signal_scaler = StandardScaler()\n",
    "signal_scaler.fit(X_train)\n",
    "\n",
    "def scale_signal(df):\n",
    "    \"\"\"Apply the signal-scaler to the given DataFrame.\"\"\"\n",
    "\n",
    "    # Apply the scaler. This returns a numpy array.\n",
    "    array = signal_scaler.transform(df)\n",
    "    \n",
    "    # Create a DataFrame with the correct column-names and index.\n",
    "    df_scaled = pd.DataFrame(data=array,\n",
    "                             columns=df.columns,\n",
    "                             index=df.index)\n",
    "    return df_scaled\n",
    "\n",
    "# Scale the signals in the training- and test-sets.\n",
    "X_train = scale_signal(X_train)\n",
    "X_test = scale_signal(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for small rounding errors, the signals in the training-set now have 0 mean and 1 standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same scaler on the test-data, also gives means that are close to 0 and standard deviations close to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: Regression\n",
    "\n",
    "Let us now build a Neural Network to learn the mapping from signals X to stock-returns y. We will use Keras and TensorFlow. If you are new to this, you can watch [this tutorial](https://www.youtube.com/watch?v=3yfRJKA1BiQ) for a basic explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This so-called \"activation function\" is applied after each layer.\n",
    "# It is important to use a non-linear function, otherwise\n",
    "# the Neural Network cannot learn non-linear mappings.\n",
    "activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Keras model.\n",
    "model_regr = Sequential()\n",
    "\n",
    "# Add an input layer for the signals.\n",
    "# Note how we set the dimensionality of the input here.\n",
    "model_regr.add(InputLayer(input_shape=(num_signals,)))\n",
    "\n",
    "# Add several dense aka. fully-connected layers.\n",
    "# You can experiment with different designs.\n",
    "model_regr.add(Dense(128, activation=activation))\n",
    "model_regr.add(Dense(64, activation=activation))\n",
    "model_regr.add(Dense(32, activation=activation))\n",
    "model_regr.add(Dense(16, activation=activation))\n",
    "model_regr.add(Dense(8, activation=activation))\n",
    "\n",
    "# Add a layer for the output of the Neural Network.\n",
    "# This is 1-dimensional to match the stock-return data.\n",
    "model_regr.add(Dense(1))\n",
    "\n",
    "# Compile the model but don't train it yet.\n",
    "model_regr.compile(loss='mse', metrics=['mae'],\n",
    "                   optimizer=RMSprop(0.001))\n",
    "\n",
    "# Show the model.\n",
    "model_regr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are common arguments for training both our regression and classification models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_args = \\\n",
    "{\n",
    "    # For efficiency, the model is trained on batches of data.\n",
    "    'batch_size': 4096,\n",
    "    \n",
    "    # Number of iterations aka. epochs over the training-set.\n",
    "    'epochs': 40,\n",
    "    \n",
    "    # Fraction of the training-set used for validation after\n",
    "    # each training-epoch, to assess how well the model performs\n",
    "    # on unseen data.\n",
    "    'validation_split': 0.2,\n",
    "\n",
    "    # Show status during training.\n",
    "    'verbose': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model. This takes a minute or two on an ordinary computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_regr = model_regr.fit(x=X_train.values,\n",
    "                              y=y_train.values, **fit_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the training-progress as the Mean Absolute Error (MAE) which is supposed to be minimized. After 40 epochs through the training-set, the model has learned to map from signals to stock-returns for the training-set, with an average absolute error of about 6%, which would be very good if it generalized to unseen data. Unfortunately, the performance on the validation-set gets worse during optimization and ends with a MAE about 24%, so the model is clearly not learning the underlying relation between signals and stock-returns, but it has instead learned peculiarities and noise in the training-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training history to a DataFrame.\n",
    "df = pd.DataFrame(history_regr.history)\n",
    "\n",
    "# Rename the relevant columns.\n",
    "names = {'mae': 'Training MAE', 'val_mae': 'Validation MAE'}\n",
    "df = df[['mae', 'val_mae']].rename(columns=names)\n",
    "\n",
    "# Plot the results.\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Results\n",
    "\n",
    "Let us now compute some performance measures for the Neural Network. First we use the fitted model to predict the stock-returns for the training- and test-sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_pred = model_regr.predict(X_train.values)\n",
    "y_test_pred = model_regr.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then calculate the $R^2$ value (aka. the coefficient of determination) between the actual stock-returns and those predicted by the model. A value of 1 means a perfect prediction, while a value of 0 means the model basically predicted the average stock-return, and a negative value means the model's prediction was worse than just using the average stock-return.\n",
    "\n",
    "The $R^2$ value is about 0.5 for the training-set, which is OK but could be much higher if we trained it more, and if we used a bigger Neural Network with more parameters to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y_train, y_pred=y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the $R^2$ value is very negative for the test-set, which is so bad, that it would be much better to just use the average stock-return. This shows the model has not learned the underlying relation between the signals X and stock-returns y. The model has merely learned noise or peculiarities in the training-data, which do not generalize to the unseen data in the test-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=y_test, y_pred=y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Examples\n",
    "\n",
    "We can also try and plot the model's predicted versus actual stock-returns for a stock in the training-set. As we can see, the model has learned this mapping reasonably well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-name for the models' predicted stock-returns.\n",
    "TOTAL_RETURN_PRED = 'Total Return Predicted'\n",
    "\n",
    "# Create a DataFrame with actual and predicted stock-returns.\n",
    "# This is for the training-set.\n",
    "df_y_train = pd.DataFrame(y_train)\n",
    "df_y_train[TOTAL_RETURN_PRED] = y_train_pred\n",
    "\n",
    "# Plot the actual and predicted stock-returns for the first stock.\n",
    "ticker = tickers_train[0]\n",
    "_ = df_y_train.loc[ticker].plot(title=ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try and plot the model's predicted versus actual stock-returns for the first stock in the test-set. As we can see, the model is mostly unable to predict the stock-returns, for this data that it has not seen during its training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual and predicted stock-returns.\n",
    "# This is for the test-set.\n",
    "df_y_test = pd.DataFrame(y_test)\n",
    "df_y_test[TOTAL_RETURN_PRED] = y_test_pred\n",
    "\n",
    "# Plot the actual and predicted stock-returns for the first stock.\n",
    "ticker = tickers_test[0]\n",
    "_ = df_y_test.loc[ticker].plot(title=ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: Classification\n",
    "\n",
    "Instead of mapping from signals X to a continuous variable y for the stock-return, we can try and map to discrete output-classes that indicate if the stock-return was a gain or loss. This is a so-called binary classification problem, and perhaps that is easier for the Neural Network.\n",
    "\n",
    "### Convert Output Data\n",
    "\n",
    "First we need to convert the stock-returns into 1 for gains and 0 for losses, and then split into training- and test-sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls,df_sig_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert stock-returns to binary classes where loss=0 and gain=1\n",
    "# In the previous tutorial we just used the sign directly, but it\n",
    "# is easier in TensorFlow / Keras to use class-values of 0 and 1.\n",
    "df_cls = (np.sign(df_sig_rets[TOTAL_RETURN_1_3Y]) + 1) * 0.5\n",
    "\n",
    "# Split the classes into training- and test-sets.\n",
    "y_train_cls = df_cls.loc[tickers_train]\n",
    "y_test_cls = df_cls.loc[tickers_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert stock-returns to binary classes where loss=0 and gain=1\n",
    "# In the previous tutorial we just used the sign directly, but it\n",
    "# is easier in TensorFlow / Keras to use class-values of 0 and 1.\n",
    "df_cls = (np.sign(df_sig_rets[TOTAL_RETURN_1_1Y]) + 1) * 0.5\n",
    "\n",
    "# Split the classes into training- and test-sets.\n",
    "y_train_cls = df_cls.loc[tickers_train]\n",
    "y_test_cls = df_cls.loc[tickers_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cls.shape,y_test_cls.shape,np.unique(df_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sig_rets[TOTAL_RETURN_1_1Y].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 classes\n",
    "conditions = [\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y] < (-0.4)),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=(-0.4)) & (df_sig_rets[TOTAL_RETURN_1_3Y]<(-0.2)),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=(-0.2)) & (df_sig_rets[TOTAL_RETURN_1_3Y]<(-0.1)),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=(-0.1)) & (df_sig_rets[TOTAL_RETURN_1_3Y]<0.1),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=0.1) & (df_sig_rets[TOTAL_RETURN_1_3Y]<0.2),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=0.2) & (df_sig_rets[TOTAL_RETURN_1_3Y]<0.4),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>= 0.4)]\n",
    "choices = [0,1,2,3,4,5,6]\n",
    "df_sig_rets['return3y'] = np.select(conditions, choices)\n",
    "df_cls = df_sig_rets['return3y']\n",
    "#print(df_sig_rets)\n",
    "np.unique(df_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 classes\n",
    "conditions = [\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y] < (-0.2)),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=(-0.2)) & (df_sig_rets[TOTAL_RETURN_1_3Y]<0.2),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_3Y]>=0.2)]\n",
    "choices = [0,1,2]\n",
    "df_sig_rets['return3y'] = np.select(conditions, choices)\n",
    "df_cls = df_sig_rets['return3y']\n",
    "#print(df_sig_rets)\n",
    "np.unique(df_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 classes\n",
    "conditions = [\n",
    "    (df_sig_rets[TOTAL_RETURN_1_1Y] < (-0.1)),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_1Y]>=(-0.1)) & (df_sig_rets[TOTAL_RETURN_1_1Y]<0.2),\n",
    "    (df_sig_rets[TOTAL_RETURN_1_1Y]>=0.2)]\n",
    "choices = [0,1,2]\n",
    "df_sig_rets['return1y'] = np.select(conditions, choices)\n",
    "df_cls = df_sig_rets['return1y']\n",
    "#print(df_sig_rets)\n",
    "np.unique(df_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(np.select(conditions, choices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the classes into training- and test-sets.\n",
    "y_train_cls = df_cls.loc[tickers_train]\n",
    "y_test_cls = df_cls.loc[tickers_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_sig_rets['return3y']), np.unique(y_train_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df_sig_rets['return1y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a binary classification problem (the result is either a gain or a loss), we could build a Neural Network to use `y_train_cls` directly. But instead we want to build a more general type of Neural Network which supports any number of output classes. For this we must convert the output to so-called one-hot encoded arrays, so a class of 0 (loss) becomes the array `[1,0]` and a class of 1 (gain) becomes the array `[0,1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_categorical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert classes to one-hot-encoded arrays.\n",
    "y_train_one_hot = to_categorical(y=y_train_cls.values,\n",
    "                                 num_classes=3)\n",
    "\n",
    "# Show the result.\n",
    "y_train_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training-set is also biased because it contains many more samples with gains compared to losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of the training-set that is classified as gains.\n",
    "gain_fraction = (y_train_cls == 1.0).sum() / len(y_train_cls)\n",
    "loss_fraction = 1.0 - gain_fraction\n",
    "\n",
    "# Show it.\n",
    "gain_fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train the Neural Network further below, we need to provide new class-weights to counter this imbalance between the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cls[y_train_cls==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_class_weight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the class-weights.\n",
    "class_weight = compute_class_weight('balanced',\n",
    "                                    y=y_train_cls,\n",
    "                                    classes=[0,1,2])\n",
    "\n",
    "# Show them.\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that when we multiply these weights by the class-fractions for losses and gains, they both end up with the same weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight * [loss_fraction, gain_fraction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The problem is that the sklearn API returns a numpy array but the keras requires a dictionary as an input for class_weight (see here). \n",
    "# https://stackoverflow.com/questions/61261907/on-colab-class-weight-is-causing-a-valueerror-the-truth-value-of-an-array-wit\n",
    "class_weight=dict(enumerate(class_weight.flatten(), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Neural Network: Classification\n",
    "\n",
    "Now that we have prepared the classification data, we can proceed to build the Neural Network. We will use the same overall design as for the regression model above, with minor changes for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.values,y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_args = \\\n",
    "{\n",
    "    # For efficiency, the model is trained on batches of data.\n",
    "    'batch_size': 4096,\n",
    "    \n",
    "    # Number of iterations aka. epochs over the training-set.\n",
    "    'epochs': 20,\n",
    "    \n",
    "    # Fraction of the training-set used for validation after\n",
    "    # each training-epoch, to assess how well the model performs\n",
    "    # on unseen data.\n",
    "    'validation_split': 0.2,\n",
    "\n",
    "    # Show status during training.\n",
    "    'verbose': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation function applied after each layer.\n",
    "# It is important to use a non-linear function, otherwise\n",
    "# the Neural Network cannot learn non-linear mappings.\n",
    "activation = 'relu'\n",
    "#activation = 'selu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model reaches 0.53-0.55 of validation accuracy at epoch=20\n",
    "\n",
    "# Create a new Keras model.\n",
    "model_clf = keras.Sequential()\n",
    "\n",
    "# Add an input layer for the signals.\n",
    "# Note how we set the dimensionality of the input here.\n",
    "model_clf.add(InputLayer(input_shape=(num_signals,)))\n",
    "\n",
    "#using LeakyReLU\n",
    "from keras.layers import LeakyReLU\n",
    "# here change your line to leave out an activation \n",
    "model_clf.add(Dense(256))\n",
    "# now add a ReLU layer explicitly:\n",
    "model_clf.add(LeakyReLU(alpha=0.05))\n",
    "#model_clf.add(Dropout(0.2))\n",
    "\n",
    "# add dropout helps\n",
    "from keras.layers import Dropout\n",
    "# Add several dense aka. fully-connected layers.\n",
    "# You can experiment with different designs.\n",
    "model_clf.add(Dense(128, activation=activation))\n",
    "model_clf.add(Dropout(0.3))\n",
    "model_clf.add(Dense(64, activation=activation))\n",
    "model_clf.add(Dropout(0.3))\n",
    "model_clf.add(Dense(32, activation=activation))\n",
    "model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(16, activation=activation))\n",
    "model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(8, activation=activation))\n",
    "#model_clf.add(Dropout(0.2))\n",
    "\n",
    "# Add a layer for the output of the Neural Network.\n",
    "# This has 2 outputs, one for each possible class, and it uses\n",
    "# the softmax activation function for classification.\n",
    "model_clf.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model but don't train it yet.\n",
    "# Note that it uses a different loss-function and metric\n",
    "# than the regression-model above.\n",
    "model_clf.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer=RMSprop(0.001))\n",
    "\n",
    "# Show the model.\n",
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Keras model.\n",
    "model_clf = keras.Sequential()\n",
    "\n",
    "# Add an input layer for the signals.\n",
    "# Note how we set the dimensionality of the input here.\n",
    "model_clf.add(InputLayer(input_shape=(num_signals,)))\n",
    "\n",
    "#using LeakyReLU\n",
    "from keras.layers import LeakyReLU\n",
    "# here change your line to leave out an activation \n",
    "model_clf.add(Dense(128))\n",
    "# # now add a ReLU layer explicitly:\n",
    "model_clf.add(LeakyReLU(alpha=0.05))\n",
    "# model_clf.add(Dropout(0.2))\n",
    "\n",
    "# add dropout helps\n",
    "from keras.layers import Dropout\n",
    "# Add several dense aka. fully-connected layers.\n",
    "# You can experiment with different designs.\n",
    "model_clf.add(Dense(128, activation=activation))\n",
    "model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(64, activation=activation))\n",
    "model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(32, activation=activation))\n",
    "# model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(16, activation=activation))\n",
    "# model_clf.add(Dropout(0.2))\n",
    "model_clf.add(Dense(8, activation=activation))\n",
    "#model_clf.add(Dropout(0.2))\n",
    "\n",
    "# Add a layer for the output of the Neural Network.\n",
    "# This has 2 outputs, one for each possible class, and it uses\n",
    "# the softmax activation function for classification.\n",
    "model_clf.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model but don't train it yet.\n",
    "# Note that it uses a different loss-function and metric\n",
    "# than the regression-model above.\n",
    "model_clf.compile(loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  optimizer=RMSprop(0.001))\n",
    "\n",
    "# Show the model.\n",
    "model_clf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model. This takes a minute or two on an ordinary computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history_clf = model_clf.fit(x=X_train.values, y=y_train_one_hot,\n",
    "                            class_weight=class_weight, **fit_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the training-progress as the classification accuracy which is supposed to be maximized. After 40 epochs through the training-set, the model has learned to map from signals to stock gain/loss for the training-set, with an accuracy of about 96% (this is not for the entire training-set, but only for one or more batches of data).\n",
    "\n",
    "Unfortunately, the performance on the validation-set gets worse during optimization and ends with an accuracy of about 60%, so the model is clearly not learning the underlying relation between signals and stock gain/loss, but it has instead learned peculiarities and noise in the training-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training history to a DataFrame.\n",
    "df = pd.DataFrame(history_clf.history)\n",
    "\n",
    "# Rename the relevant columns.\n",
    "names = {'accuracy': 'Training Accuracy',\n",
    "         'val_accuracy': 'Validation Accuracy'}\n",
    "df = df[['accuracy', 'val_accuracy']].rename(columns=names)\n",
    "\n",
    "# Plot the results.\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_clf.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Results\n",
    "\n",
    "We can then use the fitted model to predict the classes for the training- and test-sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "y_train_pred_cls = model_clf.predict(X_train.values)\n",
    "y_test_pred_cls = model_clf.predict(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network has 2 outputs which is a likelihood score for each of the two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_test_pred_cls, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert this to a single integer class-number, by taking the so-called argmax, which is the index of the largest element in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot-encoded arrays to integer class-numbers.\n",
    "y_train_pred_cls = y_train_pred_cls.argmax(axis=1)\n",
    "y_test_pred_cls = y_test_pred_cls.argmax(axis=1)\n",
    "\n",
    "# Show the result.\n",
    "y_test_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_cls),np.unique(y_test_pred_cls),np.unique(y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_test_pred_cls),np.unique(y_train_cls),np.unique(y_test_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the classification accuracy for the entire training-set, which is almost 90%, so the model has learned to map financial signals to gain/loss classifications for 1-3 year investment periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_train_cls, y_pred=y_train_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this performance does not generalize to the test-set, which the model has not seen during training. Here the classification accuracy is much lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test_cls, y_pred=y_test_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even worse than simply guessing that the stock-returns are always gains, which would give a classification accuracy of about 70%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of the test-set that is classified as gains.\n",
    "(y_test_cls == 2).sum() / len(y_test_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "We can also plot a so-called Confusion Matrix, which shows how well the classes were predicted. First we need a small helper-function for making the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot a classification confusion matrix.\n",
    "    \n",
    "    :param y_true: Array of true classes.\n",
    "    :param y_pred: Array of predicted classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class labels.\n",
    "    labels = [1.0, 0.0]\n",
    "    labels_text = ['Gain', 'Loss']\n",
    "\n",
    "    # Create confusion matrix.\n",
    "    mat = confusion_matrix(y_true=y_true, y_pred=y_pred,\n",
    "                           labels=labels)\n",
    "    \n",
    "    # Normalize so all matrix entries sum to 1.0\n",
    "    mat = mat / len(y_true)\n",
    "    \n",
    "    # Plot the matrix as a heatmap.\n",
    "    sns.heatmap(mat, robust=True,\n",
    "                xticklabels=labels_text,\n",
    "                yticklabels=labels_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have 5 classes\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot a classification confusion matrix.\n",
    "    \n",
    "    :param y_true: Array of true classes.\n",
    "    :param y_pred: Array of predicted classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class labels.\n",
    "    labels = [0,1,2,3,4,5,6]\n",
    "    labels_text = ['bigerloss','bigloss', 'Loss','nochange',\"gain\",\"biggain\",\"biggergain\"]\n",
    "\n",
    "    # Create confusion matrix.\n",
    "    mat = confusion_matrix(y_true=y_true, y_pred=y_pred,\n",
    "                           labels=labels)\n",
    "    \n",
    "    # Normalize so all matrix entries sum to 1.0\n",
    "    mat = mat / len(y_true)\n",
    "    \n",
    "    # Plot the matrix as a heatmap.\n",
    "    sns.heatmap(mat, robust=True,\n",
    "                xticklabels=labels_text,\n",
    "                yticklabels=labels_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3 classes\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot a classification confusion matrix.\n",
    "    \n",
    "    :param y_true: Array of true classes.\n",
    "    :param y_pred: Array of predicted classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class labels.\n",
    "    labels = [0,1,2]\n",
    "    labels_text = ['Loss','nochange',\"gain\"]\n",
    "\n",
    "    # Create confusion matrix.\n",
    "    mat = confusion_matrix(y_true=y_true, y_pred=y_pred,\n",
    "                           labels=labels)\n",
    "    \n",
    "    # Normalize so all matrix entries sum to 1.0\n",
    "    mat = mat / len(y_true)\n",
    "    \n",
    "    # Plot the matrix as a heatmap.\n",
    "    sns.heatmap(mat, robust=True,\n",
    "                xticklabels=labels_text,\n",
    "                yticklabels=labels_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training-set, we again see that the model has learned to accurately classify gains and losses. Note how the colours are different, because the training-set contains many more gains than losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cls,y_train_pred_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true=y_train_cls, y_pred=y_train_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix for the test-set, shows that many classes were correctly predicted as gains, and there were a lot of errors in the rest of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_true=y_test_cls, y_pred=y_test_pred_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Examples\n",
    "\n",
    "We can also try and plot the model's predicted versus actual gain/loss classification for a stock in the training-set. As we can see, the model has learned this mapping quite well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_index=np.where(tickers_train=='IBM')\n",
    "print(stock_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual and predicted classifications.\n",
    "# This is for the training-set.\n",
    "df_y_train_cls = pd.DataFrame(y_train_cls)\n",
    "df_y_train_cls[TOTAL_RETURN_PRED] = y_train_pred_cls\n",
    "\n",
    "# Plot the actual and predicted stock-returns for the first stock.\n",
    "ticker = tickers_train[0]\n",
    "_ = df_y_train_cls.loc[ticker].plot(title=ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_train_cls.loc[ticker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try and plot the model's predicted versus actual gain/loss classifications for the first stock in the test-set. As we can see, the model is quite bad at predicting this for data that it has not seen during its training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with actual and predicted classifications.\n",
    "# This is for the test-set.\n",
    "df_y_test_cls = pd.DataFrame(y_test_cls)\n",
    "df_y_test_cls[TOTAL_RETURN_PRED] = y_test_pred_cls\n",
    "\n",
    "# Plot the actual and predicted stock-returns for the first stock.\n",
    "ticker = tickers_test[0]\n",
    "_ = df_y_test_cls.loc[ticker].plot(title=ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_test_cls.loc[ticker]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We used a Neural Network to learn the relation between signals and future stock-returns. This worked reasonably well on the training-data, but it did not generalize to the test-data, where the Neural Network performed worse than just guessing. In fact, the result was even worse than the previous tutorial which used a Machine Learning (ML) model called a Random Forest.\n",
    "\n",
    "The problem might be the design of the Neural Network, that is, the number and size of the layers, their activation functions, etc. You can try and use so-called Hyper-Parameter Optimization such as the [Keras Tuner](https://keras-team.github.io/keras-tuner/), to try and find a better design for the Neural Network.\n",
    "\n",
    "But we have now seen two very different ML models having the same problem with this dataset, so perhaps **it means that these financial signals simply cannot be used to predict future stock-returns for 1-3 year investment periods.**\n",
    "\n",
    "\n",
    "### Machine Learning vs. Good Science\n",
    "\n",
    "In the previous tutorial, we had a brief discussion about Machine Learning versus Good Science. We made an important point that is worth repeating here.\n",
    "\n",
    "In general, these experiments show that Machine Learning cannot replace good science. *Even if the ML model had performed well on the test-data, it would not have provided an explanation for the underlying relation between signals and stock-returns, so we would not know whether it was truly something that would work in the future. To explain this we need proper scientific reasoning.*\n",
    "\n",
    "A good example of scientific reasoning is the discovery and formulation of the Law of Gravity. A simple experiment would be to drop various items from different heights and take lots of measurements of how fast the items fall to the ground.\n",
    "\n",
    "You can then make a noisy scatter-plot that shows the relation between item-size, item-weight, drop-height, and drop-speed. Statistical analysis would be able to give you a rough idea whether there is some relation in the data and if it is statistically significant, so the observed data is probably not just due to random chance. \n",
    "\n",
    "Machine Learning takes this one step further, in that it might be able to create a mathematical or computational model for the non-linear relation in the data, so you could use the ML model to predict the drop-speed if you input the other data. **But the ML model would not be able to explain *why* this relation exists in the data.** For this we need a proper theoretical framework such as Classical Mechanics in the case of the Law of Gravity.\n",
    "\n",
    "\n",
    "### Predicting Long-Term Stock Returns\n",
    "\n",
    "When it comes to predicting long-term stock-returns, there are actually only 3 factors that matter: <font color='red'> **The future dividends, sales-growth, and change in the P/Sales valuation ratio.** </font>(You could instead use earnings-growth or book-value growth, and the P/E or P/Book valuation ratios.) If we can predict the future dividend, sales-growth and P/Sales ratio, then we can predict the future stock-return.\n",
    "\n",
    "When making scatter-plots of historical P/Sales ratios versus long-term stock-returns, we often see a particular downwards-sloping curve, and it turns out there is a certain formula for this curve, which is derived from the mathematical definition of Annualized Return, see [here](https://github.com/Hvass-Labs/FinanceOps/blob/master/01C_Theory_of_Long-Term_Stock_Forecasting.ipynb). The formula takes as input the average dividend yield, sales-growth and P/Sales ratios that you predict for the future, and produces a curve that can be used to predict future stock-returns from these 3 factors and the P/Sales ratio today. The accuracy of the prediction depends on how well you have predicted these 3 factors.\n",
    "\n",
    "Machine Learning cannot uncover and explain such relations, whether it is the Law of Gravity, or the formula for long-term stock-returns. Human reasoning is still needed for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License (MIT)\n",
    "\n",
    "This is published under the\n",
    "[MIT License](https://github.com/simfin/simfin-tutorials/blob/master/LICENSE.txt)\n",
    "which allows very broad use for both academic and commercial purposes.\n",
    "\n",
    "You are very welcome to modify and use this source-code in your own project. Please keep a link to the [original repository](https://github.com/simfin/simfin-tutorials).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
