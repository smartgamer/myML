<!DOCTYPE html><html xmlns:cc="http://creativecommons.org/ns#"><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# medium-com: http://ogp.me/ns/fb/medium-com#"><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=contain"><title>How to do Semantic Segmentation using Deep learning</title><link rel="canonical" href="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef"><meta name="title" content="How to do Semantic Segmentation using Deep learning"><meta name="referrer" content="unsafe-url"><meta name="description" content="This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model. Nowadays, semantic segmentation is one of the key problems in the field…"><meta name="theme-color" content="#000000"><meta property="og:title" content="How to do Semantic Segmentation using Deep learning"><meta property="twitter:title" content="How to do Semantic Segmentation using Deep learning"><meta property="og:url" content="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef"><meta property="og:image" content="https://cdn-images-1.medium.com/max/1200/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg"><meta property="fb:app_id" content="542599432471018"><meta property="og:description" content="This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model."><meta name="twitter:description" content="This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model."><meta name="twitter:image:src" content="https://cdn-images-1.medium.com/max/1200/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg"><link rel="publisher" href="https://plus.google.com/103654360130207659246"><link rel="author" href="https://medium.com/@james_aka_yale"><meta property="author" content="James Le"><meta property="og:type" content="article"><meta name="twitter:card" content="summary_large_image"><meta property="article:publisher" content="https://www.facebook.com/medium"><meta property="article:author" content="https://medium.com/@james_aka_yale"><meta name="robots" content="index, follow"><meta property="article:published_time" content="2018-05-03T08:03:56.621Z"><meta name="twitter:creator" content="@james_aka_yale"><meta name="twitter:site" content="@Medium"><meta property="og:site_name" content="Medium"><meta name="twitter:label1" value="Reading time"><meta name="twitter:data1" value="9 min read"><meta name="twitter:app:name:iphone" content="Medium"><meta name="twitter:app:id:iphone" content="828256236"><meta name="twitter:app:url:iphone" content="medium://p/c673cc5862ef"><meta property="al:ios:app_name" content="Medium"><meta property="al:ios:app_store_id" content="828256236"><meta property="al:android:package" content="com.medium.reader"><meta property="al:android:app_name" content="Medium"><meta property="al:ios:url" content="medium://p/c673cc5862ef"><meta property="al:android:url" content="medium://p/c673cc5862ef"><meta property="al:web:url" content="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef"><link rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml" /><link rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/c673cc5862ef" /><script type="application/ld+json">{"@context":"http://schema.org","@type":"NewsArticle","image":{"@type":"ImageObject","width":1920,"height":791,"url":"https://cdn-images-1.medium.com/max/2000/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg"},"url":"https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef","dateCreated":"2018-05-03T08:03:56.621Z","datePublished":"2018-05-03T08:03:56.621Z","dateModified":"2018-07-09T10:13:24.186Z","headline":"How to do Semantic Segmentation using Deep learning","name":"How to do Semantic Segmentation using Deep learning","thumbnailUrl":"https://cdn-images-1.medium.com/max/2000/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg","keywords":["Tag:Machine Learning","Tag:Computer Vision","Tag:Deep Learning","Tag:Segmentation","Tag:Neural Networks","Topic:Data science","Publication:nanonets","LockedPostSource:0","Elevated:false","LayerCake:0"],"author":{"@type":"Person","name":"James Le","url":"https://medium.com/@james_aka_yale"},"creator":["James Le"],"publisher":{"@type":"Organization","name":"NanoNets","url":"https://medium.com/nanonets","logo":{"@type":"ImageObject","width":60,"height":60,"url":"https://cdn-images-1.medium.com/max/120/1*-a4bbXgtmdNW0Nj2RMzRGw.png"}},"mainEntityOfPage":"https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef"}</script><meta name="parsely-link" content="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef"><link rel="stylesheet" type="text/css" class="js-glyph-" id="glyph-8" href="https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css" /><link rel="stylesheet" href="https://cdn-static-1.medium.com/_/fp/css/main-branding-base.Jb-klQS5V5uWEz2HCHwK3w.css"><script>if (window.top !== window.self) window.top.location = window.self.location.href;var OB_startTime = new Date().getTime(); var OB_loadErrors = []; function _onerror(e) { OB_loadErrors.push(e) }; if (document.addEventListener) document.addEventListener("error", _onerror, true); else if (document.attachEvent) document.attachEvent("onerror", _onerror); function _asyncScript(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("script"); s.type = "text/javascript"; s.async = true; s.src = u; f.parentNode.insertBefore(s, f);}function _asyncStyles(u) {var d = document, f = d.getElementsByTagName("script")[0], s = d.createElement("link"); s.rel = "stylesheet"; s.href = u; f.parentNode.insertBefore(s, f); return s}(new Image()).src = "/_/stat?event=pixel.load&origin=" + encodeURIComponent(location.origin);</script><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date; ga("create", "UA-24232453-2", "auto", {"allowLinker": true, "legacyCookieDomain": window.location.hostname}); ga("send", "pageview");</script><script async src="https://www.google-analytics.com/analytics.js"></script><!--[if lt IE 9]><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js"></script><![endif]--><link rel="icon" href="https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico" class="js-favicon"><link rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/304/304/1*-a4bbXgtmdNW0Nj2RMzRGw.png"><link rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/240/240/1*-a4bbXgtmdNW0Nj2RMzRGw.png"><link rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/152/152/1*-a4bbXgtmdNW0Nj2RMzRGw.png"><link rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/120/120/1*-a4bbXgtmdNW0Nj2RMzRGw.png"><link rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"></head><body itemscope class=" postShowScreen browser-firefox is-withMagicUnderlinesv-glyph v-glyph--m2 is-noJs"><script>document.body.className = document.body.className.replace(/(^|\s)is-noJs(\s|$)/, "$1is-js$2")</script><div class="site-main" id="container"><div class="butterBar butterBar--error"></div><div class="surface"><div id="prerendered" class="screenContent"><canvas class="canvas-renderer"></canvas><div class="container u-maxWidth740 u-xs-margin0 notesPositionContainer js-notesPositionContainer"></div><div class="metabar u-clearfix js-metabar u-fixed u-backgroundTransparentWhiteDarkest u-xs-sizeFullViewportWidth"><div class="branch-journeys-top"></div><div class="js-metabarMiddle metabar-inner u-marginAuto u-maxWidth1000 u-flexCenter u-justifyContentSpaceBetween u-height65 u-xs-height56 u-paddingLeft20 u-paddingRight20"><div class="metabar-block u-flex1  u-flexCenter"><div class="js-metabarLogoLeft"><a href="https://medium.com/" data-log-event="home" class="siteNav-logo u-flexCenter u-paddingTop0"><span class="svgIcon svgIcon--logoMonogram svgIcon--45px is-flushLeft u-textColorDarker"><svg class="svgIcon-use" width="45" height="45" viewBox="0 0 45 45" ><path d="M5 40V5h35v35H5zm8.56-12.627c0 .555-.027.687-.318 1.03l-2.457 2.985v.396h6.974v-.396l-2.456-2.985c-.291-.343-.344-.502-.344-1.03V18.42l6.127 13.364h.714l5.256-13.364v10.644c0 .29 0 .342-.185.528l-1.848 1.796v.396h9.19v-.396l-1.822-1.796c-.184-.186-.21-.238-.21-.528V15.937c0-.291.026-.344.21-.528l1.823-1.797v-.396h-6.471l-4.622 11.542-5.203-11.542h-6.79v.396l2.14 2.64c.239.292.291.37.291.768v10.353z"/></svg></span><span class="u-textScreenReader">Homepage</span></a></div><div class="u-flexCenter u-height65 u-xs-height56"><span class="u-inlineBlock u-height28 u-xs-height24 u-verticalAlignTop u-marginRight20 u-marginLeft15 u-borderRightLighter"></span></div><div class="u-flexCenter u-height65 u-xs-height56 u-marginRight18"><a class="js-collectionLogoOrName u-uiTextBold u-fontSize18 u-lineHeightTightest u-xs-fontSize22" href="https://medium.com/nanonets?source=logo-lo_rejQ0qt6DyRP---81ecf7ff5ef4"><img height="36" width="36" class="u-paddingTop5" src="https://cdn-images-1.medium.com/letterbox/72/72/50/50/1*-a4bbXgtmdNW0Nj2RMzRGw.png?source=logoAvatar-lo_rejQ0qt6DyRP---81ecf7ff5ef4" alt="NanoNets" /></a></div><div class="u-flexCenter u-height65 u-xs-height56 u-xs-hide"><div class="buttonSet"><button class="button button--primary button--smallest u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton is-smallPill"  data-action="sign-up-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/nanonets" data-action-source="----81ecf7ff5ef4----------------------follow_header" data-collection-id="81ecf7ff5ef4"><span class="button-label  js-buttonLabel">Follow</span></button><a class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon"   href="https://twitter.com/nano_nets" title="Visit “NanoNets” on Twitter" aria-label="Visit “NanoNets” on Twitter" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--twitterFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25" ><path d="M21.725 5.338c-.744.47-1.605.804-2.513 1.006a3.978 3.978 0 0 0-2.942-1.293c-2.22 0-4.02 1.81-4.02 4.02 0 .32.034.63.07.94-3.31-.18-6.27-1.78-8.255-4.23a4.544 4.544 0 0 0-.574 2.01c.04 1.43.74 2.66 1.8 3.38-.63-.01-1.25-.19-1.79-.5v.08c0 1.93 1.38 3.56 3.23 3.95-.34.07-.7.12-1.07.14-.25-.02-.5-.04-.72-.07.49 1.58 1.97 2.74 3.74 2.8a8.49 8.49 0 0 1-5.02 1.72c-.3-.03-.62-.04-.93-.07A11.447 11.447 0 0 0 8.88 21c7.386 0 11.43-6.13 11.414-11.414.015-.21.01-.38 0-.578a7.604 7.604 0 0 0 2.01-2.08 7.27 7.27 0 0 1-2.297.645 3.856 3.856 0 0 0 1.72-2.23"/></svg></span></span></a><a class="button button--chromeless is-touchIconBlackPulse u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-paddingLeft0"   href="//facebook.com/nanonets" title="Visit “NanoNets” on Facebook" aria-label="Visit “NanoNets” on Facebook" rel="me" target="_blank"><span class="button-defaultState"><span class="svgIcon svgIcon--facebookFilled svgIcon--25px"><svg class="svgIcon-use" width="25" height="25" viewBox="0 0 25 25" ><path d="M21 12.646C21 7.65 16.97 3.6 12 3.6s-9 4.05-9 9.046a9.026 9.026 0 0 0 7.59 8.924v-6.376H8.395V12.64h2.193v-1.88c0-2.186 1.328-3.375 3.267-3.375.93 0 1.728.07 1.96.1V9.77H14.47c-1.055 0-1.26.503-1.26 1.242v1.63h2.517l-.33 2.554H13.21V21.6c4.398-.597 7.79-4.373 7.79-8.954"/></svg></span></span></a></div></div></div><div class="metabar-block u-flex0 u-flexCenter"><div class="u-flexCenter u-height65 u-xs-height56"><div class="buttonSet buttonSet--wide u-lineHeightInherit"><a class="button button--primary button--chromeless u-accentColor--buttonNormal is-inSiteNavBar u-xs-hide js-signInButton"   href="https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fnanonets%2Fhow-to-do-image-segmentation-using-deep-learning-c673cc5862ef&amp;source=--------------------------nav_reg&amp;operation=login" data-action="sign-in-prompt" data-redirect="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef" data-action-source="--------------------------nav_reg">Sign in</a><a class="button button--primary button--withChrome u-accentColor--buttonNormal is-inSiteNavBar js-signUpButton"   href="https://medium.com/m/signin?redirect=https%3A%2F%2Fmedium.com%2Fnanonets%2Fhow-to-do-image-segmentation-using-deep-learning-c673cc5862ef&amp;source=--------------------------nav_reg&amp;operation=register" data-action="sign-up-prompt" data-redirect="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef" data-action-source="--------------------------nav_reg">Get started</a></div></div></div></div></div><div class="metabar metabar--spacer js-metabarSpacer u-height65 u-xs-height56"></div><main role="main"><article class=" u-minHeight100vhOffset65 u-overflowHidden postArticle postArticle--full is-withAccentColors u-marginBottom40"  lang="en"><header class="container u-maxWidth740"><div class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaHeader u-paddingBottom10 row"><div class="col u-size12of12 js-postMetaLockup"><div class="uiScale uiScale-ui--regular uiScale-caption--regular postMetaLockup postMetaLockup--authorWithBio u-flexCenter js-postMetaLockup"><div class="u-flex0"><a class="link u-baseColor--link avatar"   href="https://medium.com/@james_aka_yale?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="52aa38cb8e25" data-action-type="hover" data-user-id="52aa38cb8e25" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/120/120/1*kbXSc2-EEtk9ekKq36woIQ.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of James Le"></a></div><div class="u-flex1 u-paddingLeft15 u-overflowHidden"><div class="u-lineHeightTightest"><a class="ds-link ds-link--styleSubtle ui-captionStrong u-inlineBlock link link--darken link--darker"   href="https://medium.com/@james_aka_yale?source=post_header_lockup" data-action="show-user-card" data-action-source="post_header_lockup" data-action-value="52aa38cb8e25" data-action-type="hover" data-user-id="52aa38cb8e25" dir="auto">James Le</a><span class="followState js-followState" data-user-id="52aa38cb8e25"><button class="button button--smallest u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton u-marginLeft10 u-xs-hide"  data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef" data-action-source="post_header_lockup"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--smallest u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton u-marginLeft10 u-xs-hide"  data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/52aa38cb8e25" data-action-source="post_header_lockup-52aa38cb8e25-------------------------follow_byline"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="ui-caption ui-xs-clamp2 postMetaInline">Blue Ocean Thinker (https://jameskle.com/)</div><div class="ui-caption postMetaInline js-testPostMetaInlineSupplemental"><time datetime="2018-05-03T08:03:56.621Z">May 3</time><span class="middotDivider u-fontSize12"></span><span class="readingTime" title="9 min read"></span></div></div></div></div></div></header><div class="postArticle-content js-postField js-notesSource js-trackedPost"  data-post-id="c673cc5862ef" data-source="post_page" data-collection-id="81ecf7ff5ef4" data-tracking-context="postPage"><section name="16f2" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h1 name="f0af" id="f0af" class="graf graf--h3 graf--leading graf--title">How to do Semantic Segmentation using Deep learning</h1><p name="a3c4" id="a3c4" class="graf graf--p graf-after--h3"><em class="markup--em markup--p-em">This article is a </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">comprehensive overview </em></strong><em class="markup--em markup--p-em">including a </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">step-by-step guide</em></strong><em class="markup--em markup--p-em"> to implement a deep learning </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">image segmentation model.</em></strong></p><h3 name="00d0" id="00d0" class="graf graf--h3 graf-after--p">#update: We just launched a new product: <a href="https://nanonets.com/object-detection-api/?utm_source=medium.com_Semantic_Segmentation&amp;utm_medium=blog&amp;utm_campaign=semantic_segmentation" data-href="https://nanonets.com/object-detection-api/?utm_source=medium.com_Semantic_Segmentation&amp;utm_medium=blog&amp;utm_campaign=semantic_segmentation" class="markup--anchor markup--h3-anchor" rel="nofollow noopener" target="_blank">Nanonets Object Detection APIs</a></h3></div><div class="section-inner sectionLayout--fullWidth"><figure name="71e4" id="71e4" class="graf graf--figure graf--layoutFillWidth graf-after--h3"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.199999999999996%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*rZ1vDrOBWqISFiNL5OMEbg.jpeg" data-width="2592" data-height="1068"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*rZ1vDrOBWqISFiNL5OMEbg.jpeg"></noscript></div></div><figcaption class="imageCaption">Deeplab Image Semantic Segmentation Network (Source: <a href="https://sthalles.github.io/deep_segmentation_network/" data-href="https://sthalles.github.io/deep_segmentation_network/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://sthalles.github.io/deep_segmentation_network/</a>)</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="e700" id="e700" class="graf graf--p graf-after--figure">Nowadays, <strong class="markup--strong markup--p-strong">semantic segmentation</strong> is one of the key problems in the field of computer vision. Looking at the big picture, semantic segmentation is one of the high-level task that paves the way towards complete scene understanding. The importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery. Some of those applications include self-driving vehicles, human-computer interaction, virtual reality etc. With the popularity of deep learning in recent years, many semantic segmentation problems are being tackled using deep architectures, most often Convolutional Neural Nets, which surpass other approaches by a large margin in terms of accuracy and efficiency.</p><h3 name="f9b0" id="f9b0" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">What is Semantic Segmentation?</strong></h3><p name="41fc" id="41fc" class="graf graf--p graf-after--h3">Semantic segmentation is a natural step in the progression from coarse to fine inference:</p><ul class="postList"><li name="bd03" id="bd03" class="graf graf--li graf-after--p">The origin could be located at <strong class="markup--strong markup--li-strong">classification</strong>, which consists of making a prediction for a whole input.</li><li name="88f3" id="88f3" class="graf graf--li graf-after--li">The next step is <strong class="markup--strong markup--li-strong">localization / detection</strong>, which provide not only the classes but also additional information regarding the spatial location of those classes.</li><li name="7277" id="7277" class="graf graf--li graf-after--li">Finally, <strong class="markup--strong markup--li-strong">semantic segmentation</strong> achieves fine-grained inference by making dense predictions inferring labels for every pixel, so that each pixel is labeled with the class of its enclosing object ore region.</li></div><div class="section-inner sectionLayout--outsetColumn"></ul><figure name="efba" id="efba" class="graf graf--figure graf--layoutOutsetCenter graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 433px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 43.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*MQCvfEbbA44fiZk5GoDvhA.png" data-width="1600" data-height="693" data-action="zoom" data-action-value="1*MQCvfEbbA44fiZk5GoDvhA.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*MQCvfEbbA44fiZk5GoDvhA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*MQCvfEbbA44fiZk5GoDvhA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*MQCvfEbbA44fiZk5GoDvhA.png"></noscript></div></div><figcaption class="imageCaption">An example of semantic segmentation (Source: <a href="https://blog.goodaudience.com/using-convolutional-neural-networks-for-image-segmentation-a-quick-intro-75bd68779225" data-href="https://blog.goodaudience.com/using-convolutional-neural-networks-for-image-segmentation-a-quick-intro-75bd68779225" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://blog.goodaudience.com/using-convolutional-neural-networks-for-image-segmentation-a-quick-intro-75bd68779225</a>)</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="023f" id="023f" class="graf graf--p graf-after--figure">It is also worthy to review some standard deep networks that have made significant contributions to the field of computer vision, as they are often used as the basis of semantic segmentation systems:</p><ul class="postList"><li name="0096" id="0096" class="graf graf--li graf-after--p"><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" data-href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--li-strong">AlexNet</strong></a>: Toronto’s pioneering deep CNN that won the 2012 ImageNet competition with a test accuracy of 84.6%. It consists of 5 convolutional layers, max-pooling ones, ReLUs as non-linearities, 3 fully-convolutional layers, and dropout.</li><li name="1756" id="1756" class="graf graf--li graf-after--li"><a href="https://arxiv.org/pdf/1409.1556.pdf" data-href="https://arxiv.org/pdf/1409.1556.pdf" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--li-strong">VGG-16</strong></a>: This Oxford’s model won the 2013 ImageNet competition with 92.7% accuracy. It uses a stack of convolution layers with small receptive fields in the first layers instead of few layers with big receptive fields.</li><li name="27b0" id="27b0" class="graf graf--li graf-after--li"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" data-href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--li-strong">GoogLeNet</strong></a>: This Google’s network won the 2014 ImageNet competition with accuracy of 93.3%. It is composed by 22 layers and a newly introduced building block called <em class="markup--em markup--li-em">inception</em> module. The module consists of a Network-in-Network layer, a pooling operation, a large-sized convolution layer, and small-sized convolution layer.</li><li name="c388" id="c388" class="graf graf--li graf-after--li"><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" data-href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" class="markup--anchor markup--li-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--li-strong">ResNet</strong></a>: This Microsoft’s model won the 2016 ImageNet competition with 96.4 % accuracy. It is well-known due to its depth (152 layers) and the introduction of residual blocks. The residual blocks address the problem of training a really deep architecture by introducing identity skip connections so that layers can copy their inputs to the next layer.</li></ul><figure name="b330" id="b330" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 289px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 41.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*7UXzOt97gQAmCCOL58hgAw.png" data-width="920" data-height="380" data-action="zoom" data-action-value="1*7UXzOt97gQAmCCOL58hgAw.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*7UXzOt97gQAmCCOL58hgAw.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*7UXzOt97gQAmCCOL58hgAw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*7UXzOt97gQAmCCOL58hgAw.png"></noscript></div></div><figcaption class="imageCaption">CNN Architectures (Source: <a href="https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/28ee688947cf9d31fc48f07a0497cd75200a9485" data-href="https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/28ee688947cf9d31fc48f07a0497cd75200a9485" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/28ee688947cf9d31fc48f07a0497cd75200a9485</a>)</figcaption></figure><h3 name="8dd7" id="8dd7" class="graf graf--h3 graf-after--figure"><strong class="markup--strong markup--h3-strong">What are the existing Semantic Segmentation approaches?</strong></h3><p name="1dbf" id="1dbf" class="graf graf--p graf-after--h3">A general semantic segmentation architecture can be broadly thought of as an <strong class="markup--strong markup--p-strong">encoder</strong> network followed by a <strong class="markup--strong markup--p-strong">decoder</strong> network:</p><ul class="postList"><li name="2a10" id="2a10" class="graf graf--li graf-after--p">The <strong class="markup--strong markup--li-strong">encoder</strong> is usually is a pre-trained classification network like VGG/ResNet followed by a decoder network.</li><li name="b8d7" id="b8d7" class="graf graf--li graf-after--li">The task of the <strong class="markup--strong markup--li-strong">decoder</strong> is to semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (higher resolution) to get a dense classification.</li></ul><p name="9609" id="9609" class="graf graf--p graf-after--li">Unlike classification where the end result of the very deep network is the only important thing, semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space. Different approaches employ different mechanisms as a part of the decoding mechanism. Let’s explore the 3 main approaches:</p><h4 name="3d31" id="3d31" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">1 — Region-Based Semantic Segmentation</strong></h4><p name="c022" id="c022" class="graf graf--p graf-after--h4">The region-based methods generally follow the “segmentation using recognition” pipeline, which first extracts free-form regions from an image and describes them, followed by region-based classification. At test time, the region-based predictions are transformed to pixel predictions, usually by labeling a pixel according to the highest scoring region that contains it.</p><figure name="a124" id="a124" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 295px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*ccdPdFdcSqkxRg-7902Uuw.jpeg" data-width="1280" data-height="720" data-action="zoom" data-action-value="1*ccdPdFdcSqkxRg-7902Uuw.jpeg"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*ccdPdFdcSqkxRg-7902Uuw.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*ccdPdFdcSqkxRg-7902Uuw.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*ccdPdFdcSqkxRg-7902Uuw.jpeg"></noscript></div></div><figcaption class="imageCaption">R-CNN Architecture</figcaption></figure><p name="98be" id="98be" class="graf graf--p graf-after--figure"><a href="https://arxiv.org/pdf/1311.2524.pdf" data-href="https://arxiv.org/pdf/1311.2524.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">R-CNN</strong></a> <strong class="markup--strong markup--p-strong">(Regions with CNN feature)</strong> is one representative work for the region-based methods. It performs the semantic segmentation based on the object detection results. To be specific, R-CNN first utilizes selective search to extract a large quantity of object proposals and then computes CNN features for each of them. Finally, it classifies each region using the class-specific linear SVMs. Compared with traditional CNN structures which are mainly intended for image classification, R-CNN can address more complicated tasks, such as object detection and image segmentation, and it even becomes one important basis for both fields. Moreover, R-CNN can be built on top of any CNN benchmark structures, such as AlexNet, VGG, GoogLeNet, and ResNet.</p><p name="1d74" id="1d74" class="graf graf--p graf-after--p">For the image segmentation task, R-CNN extracted 2 types of features for each region: full region feature and foreground feature, and found that it could lead to better performance when concatenating them together as the region feature. R-CNN achieved significant performance improvements due to using the highly discriminative CNN features. However, it also suffers from a couple of drawbacks for the segmentation task:</p><ul class="postList"><li name="e79b" id="e79b" class="graf graf--li graf-after--p">The feature is not compatible with the segmentation task.</li><li name="1723" id="1723" class="graf graf--li graf-after--li">The feature does not contain enough spatial information for precise boundary generation.</li><li name="54fd" id="54fd" class="graf graf--li graf-after--li">Generating segment-based proposals takes time and would greatly affect the final performance.</li></ul><p name="ea54" id="ea54" class="graf graf--p graf-after--li">Due to these bottlenecks, recent research has been proposed to address the problems, including <a href="https://arxiv.org/pdf/1407.1808.pdf" data-href="https://arxiv.org/pdf/1407.1808.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SDS</a>, <a href="https://arxiv.org/pdf/1411.5752.pdf" data-href="https://arxiv.org/pdf/1411.5752.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Hypercolumns</a>, <a href="https://arxiv.org/pdf/1703.06870.pdf" data-href="https://arxiv.org/pdf/1703.06870.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Mask R-CNN</a>.</p><h4 name="c548" id="c548" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">2 — Fully Convolutional Network-Based Semantic Segmentation</strong></h4><p name="18eb" id="18eb" class="graf graf--p graf-after--h4">The original <a href="https://arxiv.org/pdf/1411.4038.pdf" data-href="https://arxiv.org/pdf/1411.4038.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank"><strong class="markup--strong markup--p-strong">Fully Convolutional Network (FCN)</strong></a> learns a mapping from pixels to pixels, without extracting the region proposals. The FCN network pipeline is an extension of the classical CNN. The main idea is to make the classical CNN take as input arbitrary-sized images. The restriction of CNNs to accept and produce labels only for specific sized inputs comes from the fully-connected layers which are fixed. Contrary to them, FCNs only have convolutional and pooling layers which give them the ability to make predictions on arbitrary-sized inputs.</p><figure name="8b37" id="8b37" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 272px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 51.800000000000004%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*Aa2fKFN2PEKmMQoy8ps-lw.png" data-width="2024" data-height="1049" data-action="zoom" data-action-value="1*Aa2fKFN2PEKmMQoy8ps-lw.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*Aa2fKFN2PEKmMQoy8ps-lw.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*Aa2fKFN2PEKmMQoy8ps-lw.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*Aa2fKFN2PEKmMQoy8ps-lw.png"></noscript></div></div><figcaption class="imageCaption">FCN Architecture</figcaption></figure><p name="293e" id="293e" class="graf graf--p graf-after--figure">One issue in this specific FCN is that by propagating through several alternated convolutional and pooling layers, the resolution of the output feature maps is down sampled. Therefore, the direct predictions of FCN are typically in low resolution, resulting in relatively fuzzy object boundaries. A variety of more advanced FCN-based approaches have been proposed to address this issue, including <a href="https://arxiv.org/pdf/1511.00561.pdf" data-href="https://arxiv.org/pdf/1511.00561.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">SegNet</a>, <a href="https://arxiv.org/pdf/1412.7062.pdf" data-href="https://arxiv.org/pdf/1412.7062.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">DeepLab-CRF</a>, and <a href="https://arxiv.org/pdf/1511.07122.pdf" data-href="https://arxiv.org/pdf/1511.07122.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Dilated Convolutions</a>.</p><h4 name="a79a" id="a79a" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">3 — Weakly Supervised Semantic Segmentation</strong></h4><p name="322e" id="322e" class="graf graf--p graf-after--h4">Most of the relevant methods in semantic segmentation rely on a large number of images with pixel-wise segmentation masks. However, manually annotating these masks is quite time-consuming, frustrating and commercially expensive. Therefore, some weakly supervised methods have recently been proposed, which are dedicated to fulfilling the semantic segmentation by utilizing annotated bounding boxes.</p><figure name="9baa" id="9baa" class="graf graf--figure graf--layoutOutsetLeft graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 525px; max-height: 197px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 37.6%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*Zti4CyayplzrKnIJIQfb_Q.jpeg" data-width="2116" data-height="796" data-action="zoom" data-action-value="1*Zti4CyayplzrKnIJIQfb_Q.jpeg"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*Zti4CyayplzrKnIJIQfb_Q.jpeg?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1200/1*Zti4CyayplzrKnIJIQfb_Q.jpeg"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1200/1*Zti4CyayplzrKnIJIQfb_Q.jpeg"></noscript></div></div><figcaption class="imageCaption">Boxsup Training</figcaption></figure><p name="21f7" id="21f7" class="graf graf--p graf-after--figure">For example, <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" data-href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Boxsup</a> employed the bounding box annotations as a supervision to train the network and iteratively improve the estimated masks for semantic segmentation. <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" data-href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Simple Does It</a> treated the weak supervision limitation as an issue of input label noise and explored recursive training as a de-noising strategy. <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pinheiro_From_Image-Level_to_2015_CVPR_paper.pdf" data-href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pinheiro_From_Image-Level_to_2015_CVPR_paper.pdf" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Pixel-level Labeling</a> interpreted the segmentation task within the multiple-instance learning framework and added an extra layer to constrain the model to assign more weight to important pixels for image-level classification.</p><h3 name="70ee" id="70ee" class="graf graf--h3 graf-after--p"><strong class="markup--strong markup--h3-strong">Doing Semantic Segmentation with Fully-Convolutional Network</strong></h3><p name="0820" id="0820" class="graf graf--p graf-after--h3">In this section, let’s walk through a step-by-step implementation of the most popular architecture for semantic segmentation — the Fully-Convolutional Net (FCN). We’ll implement it using the TensorFlow library in Python 3, along with other dependencies such as Numpy and Scipy.</p><p name="2071" id="2071" class="graf graf--p graf-after--p">In this exercise we will label the pixels of a road in images using FCN. We’ll work with the <a href="http://www.cvlibs.net/datasets/kitti/eval_road.php" data-href="http://www.cvlibs.net/datasets/kitti/eval_road.php" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">Kitti Road Dataset</a> for road/lane detection. This is a simple exercise from the Udacity’s Self-Driving Car Nano-degree program, which you can learn more about the setup in <a href="https://github.com/udacity/CarND-Semantic-Segmentation/" data-href="https://github.com/udacity/CarND-Semantic-Segmentation/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">this GitHub repo</a>.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="ec43" id="ec43" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 302px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 30.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*OJJyC_CBCM8V1uwLOv2RMA.png" data-width="1242" data-height="375" data-action="zoom" data-action-value="1*OJJyC_CBCM8V1uwLOv2RMA.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*OJJyC_CBCM8V1uwLOv2RMA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*OJJyC_CBCM8V1uwLOv2RMA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*OJJyC_CBCM8V1uwLOv2RMA.png"></noscript></div></div><figcaption class="imageCaption">Kitti Road Dataset Training Sample (Source: <a href="http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=3748e213cf8e0100b7a26198114b3cdc7caa3aff" data-href="http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=3748e213cf8e0100b7a26198114b3cdc7caa3aff" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=3748e213cf8e0100b7a26198114b3cdc7caa3aff</a>)</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="731c" id="731c" class="graf graf--p graf-after--figure">Here are the key features of the FCN architecture:</p><ul class="postList"><li name="8fab" id="8fab" class="graf graf--li graf-after--p">FCN transfers knowledge from VGG16 to perform semantic segmentation.</li><li name="3f60" id="3f60" class="graf graf--li graf-after--li">The fully connected layers of VGG16 is converted to fully convolutional layers, using 1x1 convolution. This process produces a class presence heat map in low resolution.</li><li name="3c59" id="3c59" class="graf graf--li graf-after--li">The upsampling of these low resolution semantic feature maps is done using transposed convolutions (initialized with bilinear interpolation filters).</li><li name="b93f" id="b93f" class="graf graf--li graf-after--li">At each stage, the upsampling process is further refined by adding features from coarser but higher resolution feature maps from lower layers in VGG16.</li><li name="3856" id="3856" class="graf graf--li graf-after--li">Skip connection is introduced after each convolution block to enable the subsequent block to extract more abstract, class-salient features from the previously pooled features.</li></ul><p name="230f" id="230f" class="graf graf--p graf-after--li">There are 3 versions of FCN (FCN-32, FCN-16, FCN-8). We’ll implement <a href="https://github.com/MarvinTeichmann/tensorflow-fcn/blob/master/fcn8_vgg.py" data-href="https://github.com/MarvinTeichmann/tensorflow-fcn/blob/master/fcn8_vgg.py" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">FCN-8</a>, as detailed step-by-step below:</p><ul class="postList"><li name="be72" id="be72" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Encoder:</strong> A pre-trained <strong class="markup--strong markup--li-strong">VGG16</strong> is used as an encoder. The decoder starts from <strong class="markup--strong markup--li-strong">Layer 7</strong> of VGG16.</li><li name="2e04" id="2e04" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">FCN Layer-8:</strong> The last fully connected layer of VGG16 is replaced by a 1x1 convolution.</li><li name="9350" id="9350" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">FCN Layer-9:</strong> FCN Layer-8 is upsampled 2 times to match dimensions with <strong class="markup--strong markup--li-strong">Layer 4</strong> of VGG 16, using transposed convolution with parameters: <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">(kernel=(4,4), stride=(2,2), paddding=’same’)</em></strong>. After that, a skip connection was added between <strong class="markup--strong markup--li-strong">Layer 4</strong> of VGG16 and FCN Layer-9.</li><li name="956a" id="956a" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">FCN Layer-10:</strong> FCN Layer-9 is upsampled 2 times to match dimensions with <strong class="markup--strong markup--li-strong">Layer 3</strong> of VGG16, using transposed convolution with parameters: <strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em">(kernel=(4,4), stride=(2,2), paddding=’same’)</em></strong>. After that, a skip connection was added between <strong class="markup--strong markup--li-strong">Layer 3</strong> of VGG 16 and FCN Layer-10.</li><li name="05a2" id="05a2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">FCN Layer-11:</strong> FCN Layer-10 is upsampled 4 times to match dimensions with input image size so we get the actual image back and depth is equal to number of classes, using transposed convolution with parameters:<strong class="markup--strong markup--li-strong"><em class="markup--em markup--li-em"> (kernel=(16,16), stride=(8,8), paddding=’same’)</em></strong>.</li></ul><figure name="467c" id="467c" class="graf graf--figure graf-after--li"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 380px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 54.2%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*e08wr6of8F1J-6v4iiG2HQ.png" data-width="850" data-height="461" data-action="zoom" data-action-value="1*e08wr6of8F1J-6v4iiG2HQ.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*e08wr6of8F1J-6v4iiG2HQ.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/1600/1*e08wr6of8F1J-6v4iiG2HQ.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/1600/1*e08wr6of8F1J-6v4iiG2HQ.png"></noscript></div></div><figcaption class="imageCaption">FCN-8 Architecture (Source: <a href="https://www.researchgate.net/figure/Illustration-of-the-FCN-8s-network-architecture-as-proposed-in-20-In-our-method-the_fig1_305770331" data-href="https://www.researchgate.net/figure/Illustration-of-the-FCN-8s-network-architecture-as-proposed-in-20-In-our-method-the_fig1_305770331" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://www.researchgate.net/figure/Illustration-of-the-FCN-8s-network-architecture-as-proposed-in-20-In-our-method-the_fig1_305770331</a>)</figcaption></figure><h4 name="d949" id="d949" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Step 1</strong></h4><p name="b189" id="b189" class="graf graf--p graf-after--h4">We first load the pre-trained <strong class="markup--strong markup--p-strong">VGG-16</strong> model into TensorFlow. Taking in the TensorFlow session and the path to the VGG Folder (which is downloadable <a href="http://www.cs.toronto.edu/~frossard/post/vgg16/" data-href="http://www.cs.toronto.edu/~frossard/post/vgg16/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">here</a>), we return the tuple of tensors from VGG model, including the image input, keep_prob (to control dropout rate), layer 3, layer 4, and layer 7.</p><figure name="11bc" id="11bc" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia"><img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><div class="iframeContainer"><IFRAME width="700" height="250" data-src="/media/da5ff90136a268e6c420645f360599fe?postId=c673cc5862ef" data-media-id="da5ff90136a268e6c420645f360599fe" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen frameborder="0"></IFRAME></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/da5ff90136a268e6c420645f360599fe?postId=c673cc5862ef" data-media-id="da5ff90136a268e6c420645f360599fe" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div><figcaption class="imageCaption">VGG16 Function</figcaption></figure><h4 name="412d" id="412d" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Step 2</strong></h4><p name="f372" id="f372" class="graf graf--p graf-after--h4">Now we focus on creating the layers for a FCN, using the tensors from the VGG model. Given the tensors for VGG layer output and the number of classes to classify, we return the tensor for the last layer of that output. In particular, we apply a <strong class="markup--strong markup--p-strong">1x1 convolution</strong> to the encoder layers, and then add decoder layers to the network with <strong class="markup--strong markup--p-strong">skip connections</strong> and <strong class="markup--strong markup--p-strong">upsampling</strong>.</p><figure name="7b01" id="7b01" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia"><img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><div class="iframeContainer"><IFRAME width="700" height="250" data-src="/media/d2b9c40d8d98784991dfec073dc693b4?postId=c673cc5862ef" data-media-id="d2b9c40d8d98784991dfec073dc693b4" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen frameborder="0"></IFRAME></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/d2b9c40d8d98784991dfec073dc693b4?postId=c673cc5862ef" data-media-id="d2b9c40d8d98784991dfec073dc693b4" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div><figcaption class="imageCaption">Layers Function</figcaption></figure><h4 name="aaf0" id="aaf0" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Step 3</strong></h4><p name="e050" id="e050" class="graf graf--p graf-after--h4">The next step is to optimize our neural network, aka building TensorFlow loss functions and optimizer operations. Here we use <strong class="markup--strong markup--p-strong">cross entropy</strong> as our loss function and <strong class="markup--strong markup--p-strong">Adam</strong> as our optimization algorithm.</p><figure name="dd17" id="dd17" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia"><img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><div class="iframeContainer"><IFRAME width="700" height="250" data-src="/media/08ae46401e68b104ba167894fc37e112?postId=c673cc5862ef" data-media-id="08ae46401e68b104ba167894fc37e112" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen frameborder="0"></IFRAME></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/08ae46401e68b104ba167894fc37e112?postId=c673cc5862ef" data-media-id="08ae46401e68b104ba167894fc37e112" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div><figcaption class="imageCaption">Optimize Function</figcaption></figure><h4 name="6206" id="6206" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Step 4</strong></h4><p name="b8b0" id="b8b0" class="graf graf--p graf-after--h4">Here we define the <strong class="markup--strong markup--p-strong">train_nn</strong> function, which takes in important parameters including number of epochs, batch size, loss function, optimizer operation, and placeholders for input images, label images, learning rate. For the training process, we also set <strong class="markup--strong markup--p-strong">keep_probability</strong> to 0.5 and <strong class="markup--strong markup--p-strong">learning_rate</strong> to 0.001. To keep track of the progress, we also print out the loss during training.</p><figure name="72c1" id="72c1" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia"><img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><div class="iframeContainer"><IFRAME width="700" height="250" data-src="/media/58a5b62f0355bb6e3f4588abf6d73f47?postId=c673cc5862ef" data-media-id="58a5b62f0355bb6e3f4588abf6d73f47" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen frameborder="0"></IFRAME></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/58a5b62f0355bb6e3f4588abf6d73f47?postId=c673cc5862ef" data-media-id="58a5b62f0355bb6e3f4588abf6d73f47" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div></figure><h4 name="ed36" id="ed36" class="graf graf--h4 graf-after--figure"><strong class="markup--strong markup--h4-strong">Step 5</strong></h4><p name="7410" id="7410" class="graf graf--p graf-after--h4">Finally, it’s time to train our net! In this <strong class="markup--strong markup--p-strong">run</strong> function, we first build our net using the <em class="markup--em markup--p-em">load_vgg</em>, <em class="markup--em markup--p-em">layers</em>, and <em class="markup--em markup--p-em">optimize</em> function. Then we train the net using the <em class="markup--em markup--p-em">train_nn</em> function and save the inference data for records.</p><figure name="4be3" id="4be3" class="graf graf--figure graf--iframe graf-after--p"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 35.699999999999996%;"></div><div class="progressiveMedia js-progressiveMedia"><img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><div class="iframeContainer"><IFRAME width="700" height="250" data-src="/media/ca849a752e4e49ffc020eb36608f3aec?postId=c673cc5862ef" data-media-id="ca849a752e4e49ffc020eb36608f3aec" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" class="progressiveMedia-iframe js-progressiveMedia-iframe" allowfullscreen frameborder="0"></IFRAME></div><noscript class="js-progressiveMedia-inner"><div class="iframeContainer"><IFRAME width="700" height="250" src="/media/ca849a752e4e49ffc020eb36608f3aec?postId=c673cc5862ef" data-media-id="ca849a752e4e49ffc020eb36608f3aec" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0"></IFRAME></div></noscript></div></div><figcaption class="imageCaption">Run Function</figcaption></figure><p name="150b" id="150b" class="graf graf--p graf-after--figure">About our parameters, we choose epochs = 40, batch_size = 16, num_classes = 2, and image_shape = (160, 576). After doing 2 trial passes with dropout = 0.5 and dropout = 0.75, we found that the 2nd trial yields better results with better average losses.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="33f5" id="33f5" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 489px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.9%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*mjUI2EeA5DIKXwDBnIx-cg.png" data-width="2126" data-height="1040" data-action="zoom" data-action-value="1*mjUI2EeA5DIKXwDBnIx-cg.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*mjUI2EeA5DIKXwDBnIx-cg.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*mjUI2EeA5DIKXwDBnIx-cg.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*mjUI2EeA5DIKXwDBnIx-cg.png"></noscript></div></div><figcaption class="imageCaption">Training Sample Results</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="cc49" id="cc49" class="graf graf--p graf-after--figure graf--trailing">To see the full code, check out this link: <a href="https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88" data-href="https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88</a></p></div></div></section><section name="d7b7" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="6a6b" id="6a6b" class="graf graf--p graf--leading"><em class="markup--em markup--p-em">If you enjoyed this piece, I’d love it if you hit the clap button</em> 👏 <em class="markup--em markup--p-em">so others might stumble upon it.</em></p><h3 name="379b" id="379b" class="graf graf--h3 graf-after--p">About Nanonets</h3></div><div class="section-inner sectionLayout--outsetColumn"><figure name="7076" id="7076" class="graf graf--figure graf--layoutOutsetCenter graf-after--h3 graf--trailing"><div class="aspectRatioPlaceholder is-locked" style="max-width: 1000px; max-height: 390px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 39%;"></div><div class="progressiveMedia js-progressiveMedia graf-image" data-image-id="1*-MJ35jGG7CGGKCrziMTaWA.png" data-width="1308" data-height="510" data-action="zoom" data-action-value="1*-MJ35jGG7CGGKCrziMTaWA.png"><img src="https://cdn-images-1.medium.com/freeze/max/60/1*-MJ35jGG7CGGKCrziMTaWA.png?q=20" crossorigin="anonymous" class="progressiveMedia-thumbnail js-progressiveMedia-thumbnail"><canvas class="progressiveMedia-canvas js-progressiveMedia-canvas"></canvas><img class="progressiveMedia-image js-progressiveMedia-image" data-src="https://cdn-images-1.medium.com/max/2000/1*-MJ35jGG7CGGKCrziMTaWA.png"><noscript class="js-progressiveMedia-inner"><img class="progressiveMedia-noscript js-progressiveMedia-inner" src="https://cdn-images-1.medium.com/max/2000/1*-MJ35jGG7CGGKCrziMTaWA.png"></noscript></div></div><figcaption class="imageCaption">Visit <a href="https://nanonets.com/?utm_source=Medium.com&amp;utm_campaign=How%20to%20do%20Semantic%20Segmentation%20using%20Deep%C2%A0learning" data-href="https://nanonets.com/?utm_source=Medium.com&amp;utm_campaign=How%20to%20do%20Semantic%20Segmentation%20using%20Deep%C2%A0learning" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">Nanonets.com</a> to know more about us.</figcaption></figure></div></div></section></div><footer class="u-paddingTop10"><div class="container u-maxWidth740"><div class="row"><div class="col u-size12of12"><div class="postMetaInline postMetaInline--acknowledgments u-paddingTop5 u-paddingBottom20 js-postMetaAcknowledgments"><span data-tooltip="The following people helped the author by providing feedback before the story was published.">Thanks to</span> <span><a class="link u-baseColor--link"   href="https://medium.com/@prathameshajuvatkar?source=post_page" data-action="show-user-card" data-action-source="post_page" data-action-value="9bbcab6d816f" data-action-type="hover" data-user-id="9bbcab6d816f" dir="auto">Prathamesh A Juvatkar</a></span>. </div></div></div><div class="row"><div class="col u-size12of12 js-postTags"><div class="u-paddingBottom10"><ul class="tags tags--postTags tags--borderless"><li><a class="link u-baseColor--link"   href="https://medium.com/tag/machine-learning?source=post" data-action-source="post">Machine Learning</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/computer-vision?source=post" data-action-source="post">Computer Vision</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/deep-learning?source=post" data-action-source="post">Deep Learning</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/segmentation?source=post" data-action-source="post">Segmentation</a></li><li><a class="link u-baseColor--link"   href="https://medium.com/tag/neural-networks?source=post" data-action-source="post">Neural Networks</a></li></ul></div></div></div><section class="uiScale uiScale-ui--small uiScale-caption--regular u-borderTopLightest u-marginTop10 u-paddingTop20"><div class="ui-h3 u-textColorDarker u-fontSize22">Like what you read? Give James Le a round of applause.</div><p class="ui-body u-marginBottom20 u-textColorDark u-fontSize16">From a quick cheer to a standing ovation, clap to show how much you enjoyed this story.</p></section><div class="postActions js-postActionsFooter"><div class="u-flexCenter"><div class="u-flex1"><div class="multirecommend js-actionMultirecommend u-flexCenter u-width60" data-post-id="c673cc5862ef" data-is-icon-29px="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_actions_footer-----c673cc5862ef---------------------clap_footer"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal clap-onboardingcollection"  data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/c673cc5862ef" data-action-source="post_actions_footer-----c673cc5862ef---------------------clap_footer" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33" ><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33" ><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"/><path d="M13.285.48l-1.916.881 2.37 2.837z"/><path d="M21.719 1.361L19.79.501l-.44 3.697z"/><path d="M16.502 3.298L15.481 0h2.043z"/></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight"  data-action="multivote-undo" data-action-value="c673cc5862ef"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"/></svg></span></button></div></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-marginLeft10"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton"  data-action="show-recommends" data-action-value="c673cc5862ef">2.7K</button></span></div></div><div class="buttonSet u-flex0"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  data-action="scroll-to-responses" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--response svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.27 20.058c1.89-1.826 2.754-4.17 2.754-6.674C24.024 8.21 19.67 4 14.1 4 8.53 4 4 8.21 4 13.384c0 5.175 4.53 9.385 10.1 9.385 1.007 0 2-.14 2.95-.41.285.25.592.49.918.7 1.306.87 2.716 1.31 4.19 1.31.276-.01.494-.14.6-.36a.625.625 0 0 0-.052-.65c-.61-.84-1.042-1.71-1.282-2.58a5.417 5.417 0 0 1-.154-.75zm-3.85 1.324l-.083-.28-.388.12a9.72 9.72 0 0 1-2.85.424c-4.96 0-8.99-3.706-8.99-8.262 0-4.556 4.03-8.263 8.99-8.263 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32c0 .07 0 .19.02.37.03.29.1.6.19.92.19.7.49 1.4.89 2.08-.93-.14-1.83-.49-2.67-1.06-.34-.22-.88-.48-1.16-.74z"/></svg></span></button><button class="button button--chromeless u-baseColor--buttonNormal"  data-action="scroll-to-responses">8</button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide"  title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"/></svg></span></button><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-hide"  title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"/></svg></span></button><button class="button button--large button--dark button--chromeless u-baseColor--buttonDark button--withIcon button--withSvgIcon u-xs-show"  title="Share this story on Twitter or Facebook" aria-label="Share this story on Twitter or Facebook" data-action="show-share-popover" data-action-source="post_actions_footer"><span class="svgIcon svgIcon--share svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M20.385 8H19a.5.5 0 1 0 .011 1h1.39c.43 0 .84.168 1.14.473.31.305.48.71.48 1.142v10.77c0 .43-.17.837-.47 1.142-.3.305-.71.473-1.14.473H8.62c-.43 0-.84-.168-1.144-.473a1.603 1.603 0 0 1-.473-1.142v-10.77c0-.43.17-.837.48-1.142A1.599 1.599 0 0 1 8.62 9H10a.502.502 0 0 0 0-1H8.615c-.67 0-1.338.255-1.85.766-.51.51-.765 1.18-.765 1.85v10.77c0 .668.255 1.337.766 1.848.51.51 1.18.766 1.85.766h11.77c.668 0 1.337-.255 1.848-.766.51-.51.766-1.18.766-1.85v-10.77c0-.668-.255-1.337-.766-1.848A2.61 2.61 0 0 0 20.384 8zm-8.67-2.508L14 3.207v8.362c0 .27.224.5.5.5s.5-.23.5-.5V3.2l2.285 2.285a.49.49 0 0 0 .704-.001.511.511 0 0 0 0-.708l-3.14-3.14a.504.504 0 0 0-.71 0L11 4.776a.501.501 0 0 0 .71.706" fill-rule="evenodd"/></svg></span></button></div></div></div></div><div class="u-maxWidth740 u-paddingTop20 u-marginTop20 u-borderTopLightest container u-paddingBottom20 u-xs-paddingBottom10 js-postAttributionFooterContainer"><div class="row js-postFooterInfo"><div class="col u-size6of12 u-xs-size12of12"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardUser"><div class="u-marginLeft20 u-floatRight"><span class="followState js-followState" data-user-id="52aa38cb8e25"><button class="button button--small u-noUserSelect button--withChrome u-baseColor--buttonNormal button--withHover button--unblock js-unblockButton"  data-action="sign-up-prompt" data-sign-in-action="toggle-block-user" data-requires-token="true" data-redirect="https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef" data-action-source="footer_card"><span class="button-label  button-defaultState">Blocked</span><span class="button-label button-hoverState">Unblock</span></button><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal button--follow js-followButton"  data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-user" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/user/52aa38cb8e25" data-action-source="footer_card-52aa38cb8e25-------------------------follow_footer"><span class="button-label  button-defaultState js-buttonLabel">Follow</span><span class="button-label button-activeState">Following</span></button></span></div><div class="u-tableCell"><a class="link u-baseColor--link avatar"   href="https://medium.com/@james_aka_yale?source=footer_card" title="Go to the profile of James Le" aria-label="Go to the profile of James Le" data-action-source="footer_card" data-user-id="52aa38cb8e25" dir="auto"><img  src="https://cdn-images-1.medium.com/fit/c/120/120/1*kbXSc2-EEtk9ekKq36woIQ.jpeg" class="avatar-image avatar-image--small" alt="Go to the profile of James Le"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"   href="https://medium.com/@james_aka_yale" property="cc:attributionName" title="Go to the profile of James Le" aria-label="Go to the profile of James Le" rel="author cc:attributionUrl" data-user-id="52aa38cb8e25" dir="auto">James Le</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">Blue Ocean Thinker (<a rel="nofollow" href="https://jameskle.com/">https://jameskle.com/</a>)</p></div></li></div><div class="col u-size6of12 u-xs-size12of12 u-xs-marginTop30"><li class="uiScale uiScale-ui--small uiScale-caption--regular u-block u-paddingBottom18 js-cardCollection"><div class="u-marginLeft20 u-floatRight"><button class="button button--primary button--small u-noUserSelect button--withChrome u-accentColor--buttonNormal js-relationshipButton"  data-action="sign-up-prompt" data-sign-in-action="toggle-follow-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/nanonets" data-action-source="----81ecf7ff5ef4----------------------follow_footer" data-collection-id="81ecf7ff5ef4"><span class="button-label  js-buttonLabel">Follow</span></button></div><div class="u-tableCell "><a class="link u-baseColor--link avatar avatar--roundedRectangle"   href="https://medium.com/nanonets?source=footer_card" title="Go to NanoNets" aria-label="Go to NanoNets" data-action-source="footer_card"><img src="https://cdn-images-1.medium.com/fit/c/120/120/1*-a4bbXgtmdNW0Nj2RMzRGw.png" class="avatar-image u-size60x60" alt="NanoNets"></a></div><div class="u-tableCell u-verticalAlignMiddle u-breakWord u-paddingLeft15"><h3 class="ui-h3 u-fontSize18 u-lineHeightTighter u-marginBottom4"><a class="link link--primary u-accentColor--hoverTextNormal"   href="https://medium.com/nanonets?source=footer_card" rel="collection" data-action-source="footer_card">NanoNets</a></h3><p class="ui-body u-fontSize14 u-lineHeightBaseSans u-textColorDark u-marginBottom4">NanoNets: Machine Learning API</p><div class="buttonSet"></div></div></li></div></div></div><div class="js-postFooterPlacements"></div><div class="u-padding0 u-clearfix u-backgroundGrayLightest u-print-hide supplementalPostContent js-responsesWrapper"></div><div class="supplementalPostContent js-heroPromo"></div></footer></article></main><aside class="u-marginAuto u-maxWidth1000 js-postLeftSidebar"><div class="u-foreground u-top0 u-transition--fadeOut300 u-fixed u-sm-hide u-marginLeftNegative12 js-postShareWidget"><ul><li class="u-textAlignCenter u-marginVertical10"><div class="multirecommend js-actionMultirecommend u-flexColumn u-marginBottom10 u-width60" data-post-id="c673cc5862ef" data-is-icon-29px="true" data-is-vertical="true" data-is-circle="true" data-has-recommend-list="true" data-source="post_share_widget-----c673cc5862ef---------------------clap_sidebar"><div class="u-relative u-foreground"><button class="button button--large button--circle button--withChrome u-baseColor--buttonNormal button--withIcon button--withSvgIcon clapButton js-actionMultirecommendButton clapButton--largePill u-relative u-foreground u-xs-paddingLeft13 u-width60 u-height60 u-accentColor--textNormal u-accentColor--buttonNormal"  data-action="sign-up-prompt" data-sign-in-action="multivote" data-requires-token="true" data-redirect="https://medium.com/_/vote/p/c673cc5862ef" data-action-source="post_share_widget-----c673cc5862ef---------------------clap_sidebar" aria-label="Clap"><span class="button-defaultState"><span class="svgIcon svgIcon--clap svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33" ><path d="M28.86 17.342l-3.64-6.402c-.292-.433-.712-.729-1.163-.8a1.124 1.124 0 0 0-.889.213c-.63.488-.742 1.181-.33 2.061l1.222 2.587 1.4 2.46c2.234 4.085 1.511 8.007-2.145 11.663-.26.26-.526.49-.797.707 1.42-.084 2.881-.683 4.292-2.094 3.822-3.823 3.565-7.876 2.05-10.395zm-6.252 11.075c3.352-3.35 3.998-6.775 1.978-10.469l-3.378-5.945c-.292-.432-.712-.728-1.163-.8a1.122 1.122 0 0 0-.89.213c-.63.49-.742 1.182-.33 2.061l1.72 3.638a.502.502 0 0 1-.806.568l-8.91-8.91a1.335 1.335 0 0 0-1.887 1.886l5.292 5.292a.5.5 0 0 1-.707.707l-5.292-5.292-1.492-1.492c-.503-.503-1.382-.505-1.887 0a1.337 1.337 0 0 0 0 1.886l1.493 1.492 5.292 5.292a.499.499 0 0 1-.353.854.5.5 0 0 1-.354-.147L5.642 13.96a1.338 1.338 0 0 0-1.887 0 1.338 1.338 0 0 0 0 1.887l2.23 2.228 3.322 3.324a.499.499 0 0 1-.353.853.502.502 0 0 1-.354-.146l-3.323-3.324a1.333 1.333 0 0 0-1.886 0 1.325 1.325 0 0 0-.39.943c0 .356.138.691.39.943l6.396 6.397c3.528 3.53 8.86 5.313 12.821 1.353zM12.73 9.26l5.68 5.68-.49-1.037c-.518-1.107-.426-2.13.224-2.89l-3.303-3.304a1.337 1.337 0 0 0-1.886 0 1.326 1.326 0 0 0-.39.944c0 .217.067.42.165.607zm14.787 19.184c-1.599 1.6-3.417 2.392-5.353 2.392-.349 0-.7-.03-1.058-.082a7.922 7.922 0 0 1-3.667.887c-3.049 0-6.115-1.626-8.359-3.87l-6.396-6.397A2.315 2.315 0 0 1 2 19.724a2.327 2.327 0 0 1 1.923-2.296l-.875-.875a2.339 2.339 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.647l-.139-.139c-.91-.91-.91-2.39 0-3.3.884-.884 2.421-.882 3.301 0l.138.14a2.335 2.335 0 0 1 3.948-1.24l.093.092c.091-.423.291-.828.62-1.157a2.336 2.336 0 0 1 3.3 0l3.384 3.386a2.167 2.167 0 0 1 1.271-.173c.534.086 1.03.354 1.441.765.11-.549.415-1.034.911-1.418a2.12 2.12 0 0 1 1.661-.41c.727.117 1.385.565 1.853 1.262l3.652 6.423c1.704 2.832 2.025 7.377-2.205 11.607zM13.217.484l-1.917.882 2.37 2.837-.454-3.719zm8.487.877l-1.928-.86-.44 3.697 2.368-2.837zM16.5 3.293L15.478-.005h2.044L16.5 3.293z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--clapFilled svgIcon--33px u-relative u-topNegative2 u-xs-top0"><svg class="svgIcon-use" width="33" height="33" viewBox="0 0 33 33" ><g fill-rule="evenodd"><path d="M29.58 17.1l-3.854-6.78c-.365-.543-.876-.899-1.431-.989a1.491 1.491 0 0 0-1.16.281c-.42.327-.65.736-.7 1.207v.001l3.623 6.367c2.46 4.498 1.67 8.802-2.333 12.807-.265.265-.536.505-.81.728 1.973-.222 3.474-1.286 4.45-2.263 4.166-4.165 3.875-8.6 2.215-11.36zm-4.831.82l-3.581-6.3c-.296-.439-.725-.742-1.183-.815a1.105 1.105 0 0 0-.89.213c-.647.502-.755 1.188-.33 2.098l1.825 3.858a.601.601 0 0 1-.197.747.596.596 0 0 1-.77-.067L10.178 8.21c-.508-.506-1.393-.506-1.901 0a1.335 1.335 0 0 0-.393.95c0 .36.139.698.393.95v.001l5.61 5.61a.599.599 0 1 1-.848.847l-5.606-5.606c-.001 0-.002 0-.003-.002L5.848 9.375a1.349 1.349 0 0 0-1.902 0 1.348 1.348 0 0 0 0 1.901l1.582 1.582 5.61 5.61a.6.6 0 0 1-.848.848l-5.61-5.61c-.51-.508-1.393-.508-1.9 0a1.332 1.332 0 0 0-.394.95c0 .36.139.697.393.952l2.363 2.362c.002.001.002.002.002.003l3.52 3.52a.6.6 0 0 1-.848.847l-3.522-3.523h-.001a1.336 1.336 0 0 0-.95-.393 1.345 1.345 0 0 0-.949 2.295l6.779 6.78c3.715 3.713 9.327 5.598 13.49 1.434 3.527-3.528 4.21-7.13 2.086-11.015zM11.817 7.727c.06-.328.213-.64.466-.893.64-.64 1.755-.64 2.396 0l3.232 3.232c-.82.783-1.09 1.833-.764 2.992l-5.33-5.33z"/><path d="M13.285.48l-1.916.881 2.37 2.837z"/><path d="M21.719 1.361L19.79.501l-.44 3.697z"/><path d="M16.502 3.298L15.481 0h2.043z"/></g></svg></span></span></button><div class="clapUndo u-width60 u-round u-height32 u-absolute u-borderBox u-paddingRight5 u-transition--transform200Spring u-background--brandSageLighter js-clapUndo" style="top: 14px; padding: 2px;"><button class="button button--chromeless u-baseColor--buttonNormal button--withIcon button--withSvgIcon u-floatRight"  data-action="multivote-undo" data-action-value="c673cc5862ef"><span class="svgIcon svgIcon--removeThin svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M20.13 8.11l-5.61 5.61-5.609-5.61-.801.801 5.61 5.61-5.61 5.61.801.8 5.61-5.609 5.61 5.61.8-.801-5.609-5.61 5.61-5.61" fill-rule="evenodd"/></svg></span></button></div></div><span class="u-textAlignCenter u-relative u-background js-actionMultirecommendCount u-flexOrderNegative1 u-height20 u-marginBottom7"><button class="button button--chromeless u-baseColor--buttonNormal js-multirecommendCountButton u-block u-marginAuto"  data-action="show-recommends" data-action-value="c673cc5862ef">2.7K</button></span></div></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Twitter" aria-label="Share on Twitter" data-action="share-on-twitter" data-action-source="post_share_widget"><span class="svgIcon svgIcon--twitter svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M21.967 11.8c.018 5.93-4.607 11.18-11.177 11.18-2.172 0-4.25-.62-6.047-1.76l-.268.422-.038.5.186.013.168.012c.3.02.44.032.6.046 2.06-.026 3.95-.686 5.49-1.86l1.12-.85-1.4-.048c-1.57-.055-2.92-1.08-3.36-2.51l-.48.146-.05.5c.22.03.48.05.75.08.48-.02.87-.07 1.25-.15l2.33-.49-2.32-.49c-1.68-.35-2.91-1.83-2.91-3.55 0-.05 0-.01-.01.03l-.49-.1-.25.44c.63.36 1.35.57 2.07.58l1.7.04L7.4 13c-.978-.662-1.59-1.79-1.618-3.047a4.08 4.08 0 0 1 .524-1.8l-.825.07a12.188 12.188 0 0 0 8.81 4.515l.59.033-.06-.59v-.02c-.05-.43-.06-.63-.06-.87a3.617 3.617 0 0 1 6.27-2.45l.2.21.28-.06c1.01-.22 1.94-.59 2.73-1.09l-.75-.56c-.1.36-.04.89.12 1.36.23.68.58 1.13 1.17.85l-.21-.45-.42-.27c-.52.8-1.17 1.48-1.92 2L22 11l.016.28c.013.2.014.35 0 .52v.04zm.998.038c.018-.22.017-.417 0-.66l-.498.034.284.41a8.183 8.183 0 0 0 2.2-2.267l.97-1.48-1.6.755c.17-.08.3-.02.34.03a.914.914 0 0 1-.13-.292c-.1-.297-.13-.64-.1-.766l.36-1.254-1.1.695c-.69.438-1.51.764-2.41.963l.48.15a4.574 4.574 0 0 0-3.38-1.484 4.616 4.616 0 0 0-4.61 4.613c0 .29.02.51.08.984l.01.02.5-.06.03-.5c-3.17-.18-6.1-1.7-8.08-4.15l-.48-.56-.36.64c-.39.69-.62 1.48-.65 2.28.04 1.61.81 3.04 2.06 3.88l.3-.92c-.55-.02-1.11-.17-1.6-.45l-.59-.34-.14.67c-.02.08-.02.16 0 .24-.01 2.12 1.55 4.01 3.69 4.46l.1-.49-.1-.49c-.33.07-.67.12-1.03.14-.18-.02-.43-.05-.64-.07l-.76-.09.23.73c.57 1.84 2.29 3.14 4.28 3.21l-.28-.89a8.252 8.252 0 0 1-4.85 1.66c-.12-.01-.26-.02-.56-.05l-.17-.01-.18-.01L2.53 21l1.694 1.07a12.233 12.233 0 0 0 6.58 1.917c7.156 0 12.2-5.73 12.18-12.18l-.002.04z"/></svg></span></button></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconBlackPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon"  title="Share on Facebook" aria-label="Share on Facebook" data-action="share-on-facebook" data-action-source="post_share_widget"><span class="svgIcon svgIcon--facebook svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M16.39 23.61v-5.808h1.846a.55.55 0 0 0 .546-.48l.36-2.797a.551.551 0 0 0-.547-.62H16.39V12.67c0-.67.12-.813.828-.813h1.474a.55.55 0 0 0 .55-.55V8.803a.55.55 0 0 0-.477-.545c-.436-.06-1.36-.116-2.22-.116-2.5 0-4.13 1.62-4.13 4.248v1.513H10.56a.551.551 0 0 0-.55.55v2.797c0 .304.248.55.55.55h1.855v5.76c-4.172-.96-7.215-4.7-7.215-9.1 0-5.17 4.17-9.36 9.31-9.36 5.14 0 9.31 4.19 9.31 9.36 0 4.48-3.155 8.27-7.43 9.15M14.51 4C8.76 4 4.1 8.684 4.1 14.46c0 5.162 3.75 9.523 8.778 10.32a.55.55 0 0 0 .637-.543v-6.985a.551.551 0 0 0-.55-.55H11.11v-1.697h1.855a.55.55 0 0 0 .55-.55v-2.063c0-2.02 1.136-3.148 3.03-3.148.567 0 1.156.027 1.597.06v1.453h-.924c-1.363 0-1.93.675-1.93 1.912v1.78c0 .3.247.55.55.55h2.132l-.218 1.69H15.84c-.305 0-.55.24-.55.55v7.02c0 .33.293.59.623.54 5.135-.7 9.007-5.11 9.007-10.36C24.92 8.68 20.26 4 14.51 4"/></svg></span></button></li><li class="u-textAlignCenter u-marginVertical10"><button class="button button--large button--dark button--chromeless is-touchIconFadeInPulse u-baseColor--buttonDark button--withIcon button--withSvgIcon button--bookmark js-bookmarkButton"  title="Bookmark this story to read later" aria-label="Bookmark this story to read later" data-action="sign-up-prompt" data-sign-in-action="add-to-bookmarks" data-requires-token="true" data-redirect="https://medium.com/_/bookmark/p/c673cc5862ef" data-action-source="post_share_widget-----c673cc5862ef---------------------bookmark_sidebar"><span class="button-defaultState"><span class="svgIcon svgIcon--bookmark svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4zM21 23l-5.91-3.955-.148-.107a.751.751 0 0 0-.884 0l-.147.107L8 23V6.615C8 5.725 8.725 5 9.615 5h9.77C20.275 5 21 5.725 21 6.615V23z" fill-rule="evenodd"/></svg></span></span><span class="button-activeState"><span class="svgIcon svgIcon--bookmarkFilled svgIcon--29px"><svg class="svgIcon-use" width="29" height="29" viewBox="0 0 29 29" ><path d="M19.385 4h-9.77A2.623 2.623 0 0 0 7 6.615V23.01a1.022 1.022 0 0 0 1.595.847l5.905-4.004 5.905 4.004A1.022 1.022 0 0 0 22 23.011V6.62A2.625 2.625 0 0 0 19.385 4z" fill-rule="evenodd"/></svg></span></span></button></li></ul></div></aside><div class="u-fixed u-bottom0 u-sizeFullWidth u-backgroundWhite u-boxShadowTop u-borderBox u-paddingTop10 u-paddingBottom10 u-zIndexMetabar u-xs-paddingLeft10 u-xs-paddingRight10 js-stickyFooter"><div class="u-maxWidth700 u-marginAuto u-flexCenter"><div class="u-fontSize16 u-flex1 u-flexCenter"><div class="u-flex0 u-inlineBlock u-paddingRight20 u-xs-paddingRight10"><a class="link u-baseColor--link avatar avatar--roundedRectangle"   href="https://medium.com/nanonets" title="Go to NanoNets" aria-label="Go to NanoNets"><img src="https://cdn-images-1.medium.com/fit/c/80/80/1*-a4bbXgtmdNW0Nj2RMzRGw.png" class="avatar-image avatar-image--smaller" alt="NanoNets"></a></div><div class="u-flex1 u-inlineBlock"><div class="u-xs-hide">Never miss a story from<strong> NanoNets</strong>, when you sign up for Medium. <a class="link u-baseColor--link link--accent u-accentColor--textNormal u-accentColor--textDarken"   href="https://medium.com/@Medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg" data-action-source="sticky_footer">Learn more</a></div><div class="u-xs-show">Never miss a story from<strong> NanoNets</strong></div></div></div><div class="u-marginLeft50 u-xs-marginAuto"><button class="button button--primary button--dark is-active u-noUserSelect button--withChrome u-accentColor--buttonDark u-uiTextSemibold u-textUppercase u-fontSize12 button--followCollection js-followCollectionButton"  data-action="sign-up-prompt" data-sign-in-action="toggle-subscribe-collection" data-requires-token="true" data-redirect="https://medium.com/_/subscribe/collection/nanonets" data-action-source="sticky_footer----81ecf7ff5ef4----------------------follow_metabar"><span class="button-label  button-defaultState js-buttonLabel">Get updates</span><span class="button-label button-activeState">Get updates</span></button></div></div></div><style class="js-collectionStyle">
.u-accentColor--borderLight {border-color: #5E7BFF !important;}
.u-accentColor--borderNormal {border-color: #5E7BFF !important;}
.u-accentColor--borderDark {border-color: #536BDB !important;}
.u-accentColor--iconLight .svgIcon,.u-accentColor--iconLight.svgIcon {fill: #5E7BFF !important;}
.u-accentColor--iconNormal .svgIcon,.u-accentColor--iconNormal.svgIcon {fill: #5E7BFF !important;}
.u-accentColor--iconDark .svgIcon,.u-accentColor--iconDark.svgIcon {fill: #536BDB !important;}
.u-accentColor--textNormal {color: #536BDB !important;}
.u-accentColor--hoverTextNormal:hover {color: #536BDB !important;}
.u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #4D63C4 !important;}
.u-accentColor--textDark {color: #4D63C4 !important;}
.u-accentColor--backgroundLight {background-color: #5E7BFF !important;}
.u-accentColor--backgroundNormal {background-color: #5E7BFF !important;}
.u-accentColor--backgroundDark {background-color: #536BDB !important;}
.u-accentColor--buttonDark {border-color: #536BDB !important; color: #4D63C4 !important;}
.u-accentColor--buttonDark:hover {border-color: #4D63C4 !important;}
.u-accentColor--buttonDark .icon:before,.u-accentColor--buttonDark .svgIcon{color: #536BDB !important; fill: #536BDB !important;}
.u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #5E7BFF !important; color: #536BDB !important;}
.u-accentColor--buttonNormal:hover {border-color: #536BDB !important;}
.u-accentColor--buttonNormal .icon:before,.u-accentColor--buttonNormal .svgIcon{color: #5E7BFF !important; fill: #5E7BFF !important;}
.u-accentColor--buttonNormal.button--filled .icon:before,.u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonDark.button--filled,.u-accentColor--buttonDark.button--withChrome.is-active,.u-accentColor--fillWhenActive.is-active {background-color: #536BDB !important; border-color: #536BDB !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #5E7BFF !important; border-color: #5E7BFF !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--user,.postArticle.is-withAccentColors .markup--query {color: #536BDB !important;}.u-tintBgColor {background-color: rgba(84, 111, 255, 1) !important;}.u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(84, 111, 255, 1) 0%, rgba(84, 111, 255, 0) 100%) !important;}.u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(84, 111, 255, 0) 0%, rgba(84, 111, 255, 1) 100%) !important;}
.u-tintSpectrum .u-baseColor--borderLight {border-color: #A8BEFF !important;}
.u-tintSpectrum .u-baseColor--borderNormal {border-color: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--borderDark {border-color: #E9F2FF !important;}
.u-tintSpectrum .u-baseColor--iconLight .svgIcon,.u-tintSpectrum .u-baseColor--iconLight.svgIcon {fill: #A8BEFF !important;}
.u-tintSpectrum .u-baseColor--iconNormal .svgIcon,.u-tintSpectrum .u-baseColor--iconNormal.svgIcon {fill: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--iconDark .svgIcon,.u-tintSpectrum .u-baseColor--iconDark.svgIcon {fill: #E9F2FF !important;}
.u-tintSpectrum .u-baseColor--textNormal {color: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--textDark {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--textDarker {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--backgroundLight {background-color: #A8BEFF !important;}
.u-tintSpectrum .u-baseColor--backgroundNormal {background-color: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--backgroundDark {background-color: #E9F2FF !important;}
.u-tintSpectrum .u-baseColor--buttonLight {border-color: #A8BEFF !important; color: #A8BEFF !important;}
.u-tintSpectrum .u-baseColor--buttonLight:hover {border-color: #A8BEFF !important;}
.u-tintSpectrum .u-baseColor--buttonLight .icon:before,.u-tintSpectrum .u-baseColor--buttonLight .svgIcon {color: #A8BEFF !important; fill: #A8BEFF !important;}
.u-tintSpectrum .u-baseColor--buttonDark {border-color: #E9F2FF !important; color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--buttonDark:hover {border-color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--buttonDark .icon:before,.u-tintSpectrum .u-baseColor--buttonDark .svgIcon {color: #E9F2FF !important; fill: #E9F2FF !important;}
.u-tintSpectrum .u-baseColor--buttonNormal {border-color: #C9D9FF !important; color: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--buttonNormal:hover {border-color: #E9F2FF !important;}
.u-tintSpectrum .u-baseColor--buttonNormal .icon:before,.u-tintSpectrum .u-baseColor--buttonNormal .svgIcon {color: #C9D9FF !important; fill: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--buttonDark.button--filled,.u-tintSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: #E9F2FF !important; border-color: #E9F2FF !important; color: rgba(84, 111, 255, 1) !important; fill: rgba(84, 111, 255, 1) !important;}
.u-tintSpectrum .u-baseColor--buttonNormal.button--filled,.u-tintSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: #C9D9FF !important; border-color: #C9D9FF !important; color: rgba(84, 111, 255, 1) !important; fill: rgba(84, 111, 255, 1) !important;}
.u-tintSpectrum .u-baseColor--link {color: #C9D9FF !important;}
.u-tintSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--darken:active {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--dark {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-tintSpectrum .u-baseColor--link.link--dark.link--darken:active {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--link.link--darker {color: #F8FFFF !important;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: #A8BEFF;}
.u-tintSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: #A8BEFF;}
.u-tintSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: #A8BEFF;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: #7692FF !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: #87A2FF !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: #A8BEFF !important;}
.u-tintSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: #C9D9FF !important;}
.u-tintSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: #F8FFFF !important;}
.u-tintSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: #F8FFFF !important;}
.u-tintSpectrum  .ui-h1,.u-tintSpectrum  .ui-h2,.u-tintSpectrum  .ui-h3,.u-tintSpectrum  .ui-h4,.u-tintSpectrum  .ui-brand1,.u-tintSpectrum  .ui-brand2,.u-tintSpectrum  .ui-captionStrong {color: #F8FFFF !important; fill: #F8FFFF !important;}
.u-tintSpectrum  .ui-body,.u-tintSpectrum  .ui-caps {color: #F8FFFF !important; fill: #F8FFFF !important;}
.u-tintSpectrum  .ui-summary,.u-tintSpectrum  .ui-caption {color: #A8BEFF !important; fill: #A8BEFF !important;}
.u-tintSpectrum .u-accentColor--borderLight {border-color: #A8BEFF !important;}
.u-tintSpectrum .u-accentColor--borderNormal {border-color: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--borderDark {border-color: #E9F2FF !important;}
.u-tintSpectrum .u-accentColor--iconLight .svgIcon,.u-tintSpectrum .u-accentColor--iconLight.svgIcon {fill: #A8BEFF !important;}
.u-tintSpectrum .u-accentColor--iconNormal .svgIcon,.u-tintSpectrum .u-accentColor--iconNormal.svgIcon {fill: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--iconDark .svgIcon,.u-tintSpectrum .u-accentColor--iconDark.svgIcon {fill: #E9F2FF !important;}
.u-tintSpectrum .u-accentColor--textNormal {color: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--hoverTextNormal:hover {color: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: #F8FFFF !important;}
.u-tintSpectrum .u-accentColor--textDark {color: #F8FFFF !important;}
.u-tintSpectrum .u-accentColor--backgroundLight {background-color: #A8BEFF !important;}
.u-tintSpectrum .u-accentColor--backgroundNormal {background-color: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--backgroundDark {background-color: #E9F2FF !important;}
.u-tintSpectrum .u-accentColor--buttonDark {border-color: #E9F2FF !important; color: #F8FFFF !important;}
.u-tintSpectrum .u-accentColor--buttonDark:hover {border-color: #F8FFFF !important;}
.u-tintSpectrum .u-accentColor--buttonDark .icon:before,.u-tintSpectrum .u-accentColor--buttonDark .svgIcon{color: #E9F2FF !important; fill: #E9F2FF !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: #C9D9FF !important; color: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--buttonNormal:hover {border-color: #E9F2FF !important;}
.u-tintSpectrum .u-accentColor--buttonNormal .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal .svgIcon{color: #C9D9FF !important; fill: #C9D9FF !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-tintSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(84, 111, 255, 1) !important; fill: rgba(84, 111, 255, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonDark.button--filled,.u-tintSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-tintSpectrum .u-accentColor--fillWhenActive.is-active {background-color: #E9F2FF !important; border-color: #E9F2FF !important; color: rgba(84, 111, 255, 1) !important; fill: rgba(84, 111, 255, 1) !important;}
.u-tintSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-tintSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: #C9D9FF !important; border-color: #C9D9FF !important; color: rgba(84, 111, 255, 1) !important; fill: rgba(84, 111, 255, 1) !important;}
.u-tintSpectrum .postArticle.is-withAccentColors .markup--user,.u-tintSpectrum .postArticle.is-withAccentColors .markup--query {color: #C9D9FF !important;}
.u-accentColor--highlightFaint {background-color: rgba(229, 240, 255, 1) !important;}
.u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(191, 221, 255, 1) !important;}
.postArticle.is-withAccentColors .markup--quote.is-other {background-color: rgba(229, 240, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(229, 240, 255, 1), rgba(229, 240, 255, 1));}
.postArticle.is-withAccentColors .markup--quote.is-me {background-color: rgba(208, 230, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(208, 230, 255, 1), rgba(208, 230, 255, 1));}
.postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: rgba(191, 221, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(191, 221, 255, 1), rgba(191, 221, 255, 1));}
.postArticle.is-withAccentColors .markup--quote.is-selected {background-color: rgba(191, 221, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(191, 221, 255, 1), rgba(191, 221, 255, 1));}
.postArticle.is-withAccentColors .markup--highlight {background-color: rgba(191, 221, 255, 1) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(191, 221, 255, 1), rgba(191, 221, 255, 1));}.u-baseColor--iconNormal.avatar-halo {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}</style><style class="js-collectionStyleConstant">.u-imageBgColor {background-color: rgba(0, 0, 0, 0.24705882352941178);}
.u-imageSpectrum .u-baseColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconLight .svgIcon,.u-imageSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--iconNormal .svgIcon,.u-imageSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--iconDark .svgIcon,.u-imageSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--textDarker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-baseColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important; color: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-baseColor--buttonLight .icon:before,.u-imageSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-baseColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonDark .icon:before,.u-imageSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal .icon:before,.u-imageSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--buttonDark.button--filled,.u-imageSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--buttonNormal.button--filled,.u-imageSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-baseColor--link {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-imageSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--link.link--darker {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(255, 255, 255, 0.8);}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(255, 255, 255, 0.4) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(255, 255, 255, 0.4980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-h1,.u-imageSpectrum  .ui-h2,.u-imageSpectrum  .ui-h3,.u-imageSpectrum  .ui-h4,.u-imageSpectrum  .ui-brand1,.u-imageSpectrum  .ui-brand2,.u-imageSpectrum  .ui-captionStrong {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-body,.u-imageSpectrum  .ui-caps {color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum  .ui-summary,.u-imageSpectrum  .ui-caption {color: rgba(255, 255, 255, 0.8) !important; fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--borderLight {border-color: rgba(255, 255, 255, 0.6980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderNormal {border-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--borderDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconLight .svgIcon,.u-imageSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(255, 255, 255, 0.8) !important;}
.u-imageSpectrum .u-accentColor--iconNormal .svgIcon,.u-imageSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--iconDark .svgIcon,.u-imageSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textNormal {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--textDark {color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--backgroundLight {background-color: rgba(255, 255, 255, 0.8980392156862745) !important;}
.u-imageSpectrum .u-accentColor--backgroundNormal {background-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--backgroundDark {background-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark {border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonDark .icon:before,.u-imageSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(255, 255, 255, 0.8980392156862745) !important; color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(255, 255, 255, 0.9490196078431372) !important; fill: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-imageSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonDark.button--filled,.u-imageSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-imageSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(255, 255, 255, 1) !important; border-color: rgba(255, 255, 255, 1) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-imageSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(255, 255, 255, 0.9490196078431372) !important; border-color: rgba(255, 255, 255, 0.9490196078431372) !important; color: rgba(0, 0, 0, 0.24705882352941178) !important; fill: rgba(0, 0, 0, 0.24705882352941178) !important;}
.u-imageSpectrum .postArticle.is-withAccentColors .markup--user,.u-imageSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(255, 255, 255, 0.9490196078431372) !important;}
.u-imageSpectrum .u-accentColor--highlightFaint {background-color: rgba(255, 255, 255, 0.2) !important;}
.u-imageSpectrum .u-accentColor--highlightStrong.is-active .svgIcon {fill: rgba(255, 255, 255, 0.6) !important;}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: rgba(255, 255, 255, 0.2) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-other {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 0.2));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: rgba(255, 255, 255, 0.4) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-me {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.4), rgba(255, 255, 255, 0.4));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-targeted {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--quote.is-selected {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}
.postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: rgba(255, 255, 255, 0.6) !important;}
body.is-withMagicUnderlines .postArticle.is-withAccentColors .u-imageSpectrum .markup--highlight {background-color: transparent !important; background-image: linear-gradient(to bottom, rgba(255, 255, 255, 0.6), rgba(255, 255, 255, 0.6));}.u-resetSpectrum .u-tintBgColor {background-color: rgba(255, 255, 255, 1) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeLeft:before {background-image: linear-gradient(to right, rgba(255, 255, 255, 1) 0%, rgba(255, 255, 255, 0) 100%) !important;}.u-resetSpectrum .u-tintBgColor .u-fadeRight:after {background-image: linear-gradient(to right, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 1) 100%) !important;}
.u-resetSpectrum .u-baseColor--borderLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--borderDark {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--iconLight .svgIcon,.u-resetSpectrum .u-baseColor--iconLight.svgIcon {fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconNormal .svgIcon,.u-resetSpectrum .u-baseColor--iconNormal.svgIcon {fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--iconDark .svgIcon,.u-resetSpectrum .u-baseColor--iconDark.svgIcon {fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textNormal {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--textNormal.u-baseColor--textDarken:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--textDarker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--backgroundLight {background-color: rgba(0, 0, 0, 0.09803921568627451) !important;}
.u-resetSpectrum .u-baseColor--backgroundNormal {background-color: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .u-baseColor--backgroundDark {background-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight {border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight:hover {border-color: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonLight .icon:before,.u-resetSpectrum .u-baseColor--buttonLight .svgIcon {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark {border-color: rgba(0, 0, 0, 0.6) !important; color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonDark:hover {border-color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--buttonDark .icon:before,.u-resetSpectrum .u-baseColor--buttonDark .svgIcon {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal {border-color: rgba(0, 0, 0, 0.4980392156862745) !important; color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal:hover {border-color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal .icon:before,.u-resetSpectrum .u-baseColor--buttonNormal .svgIcon {color: rgba(0, 0, 0, 0.4980392156862745) !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--buttonDark.button--filled,.u-resetSpectrum .u-baseColor--buttonDark.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2980392156862745) !important; border-color: rgba(0, 0, 0, 0.2980392156862745) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--buttonNormal.button--filled,.u-resetSpectrum .u-baseColor--buttonNormal.button--withChrome.is-active {background-color: rgba(0, 0, 0, 0.2) !important; border-color: rgba(0, 0, 0, 0.2) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-baseColor--link {color: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .u-baseColor--link.link--darkenOnHover:hover {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--darken:active {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark {color: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:hover,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:focus,.u-resetSpectrum .u-baseColor--link.link--dark.link--darken:active {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--link.link--darker {color: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-webkit-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal ::-moz-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .u-baseColor--placeholderNormal :-ms-input-placeholder {color: rgba(0, 0, 0, 0.2980392156862745);}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(1) {stroke: none !important; fill: rgba(0, 0, 0, 0.2) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(2) {stroke: none !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(3) {stroke: none !important; fill: rgba(0, 0, 0, 0.4) !important;}
.u-resetSpectrum .svgIcon--logoNew path:nth-child(4) {stroke: none !important; fill: rgba(0, 0, 0, 0.4980392156862745) !important;}
.u-resetSpectrum .svgIcon--logoWordmark {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum .svgIcon--logoMonogram {stroke: none !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-h1,.u-resetSpectrum  .ui-h2,.u-resetSpectrum  .ui-h3,.u-resetSpectrum  .ui-h4,.u-resetSpectrum  .ui-brand1,.u-resetSpectrum  .ui-brand2,.u-resetSpectrum  .ui-captionStrong {color: rgba(0, 0, 0, 0.8) !important; fill: rgba(0, 0, 0, 0.8) !important;}
.u-resetSpectrum  .ui-body,.u-resetSpectrum  .ui-caps {color: rgba(0, 0, 0, 0.6) !important; fill: rgba(0, 0, 0, 0.6) !important;}
.u-resetSpectrum  .ui-summary,.u-resetSpectrum  .ui-caption {color: rgba(0, 0, 0, 0.2980392156862745) !important; fill: rgba(0, 0, 0, 0.2980392156862745) !important;}
.u-resetSpectrum .u-accentColor--borderLight {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderNormal {border-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--borderDark {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconLight .svgIcon,.u-resetSpectrum .u-accentColor--iconLight.svgIcon {fill: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--iconNormal .svgIcon,.u-resetSpectrum .u-accentColor--iconNormal.svgIcon {fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--iconDark .svgIcon,.u-resetSpectrum .u-accentColor--iconDark.svgIcon {fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--hoverTextNormal:hover {color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--textNormal.u-accentColor--textDarken:hover {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--textDark {color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundLight {background-color: rgba(2, 184, 117, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundNormal {background-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--backgroundDark {background-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark {border-color: rgba(0, 171, 107, 1) !important; color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark:hover {border-color: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark .icon:before,.u-resetSpectrum .u-accentColor--buttonDark .svgIcon{color: rgba(28, 153, 99, 1) !important; fill: rgba(28, 153, 99, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:not(.clapButton--largePill) {border-color: rgba(2, 184, 117, 1) !important; color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal:hover {border-color: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal .svgIcon{color: rgba(0, 171, 107, 1) !important; fill: rgba(0, 171, 107, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .icon:before,.u-resetSpectrum .u-accentColor--buttonNormal.button--filled .svgIcon{color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonDark.button--filled,.u-resetSpectrum .u-accentColor--buttonDark.button--withChrome.is-active,.u-resetSpectrum .u-accentColor--fillWhenActive.is-active {background-color: rgba(28, 153, 99, 1) !important; border-color: rgba(28, 153, 99, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .u-accentColor--buttonNormal.button--filled:not(.clapButton--largePill),.u-resetSpectrum .u-accentColor--buttonNormal.button--withChrome.is-active:not(.clapButton--largePill) {background-color: rgba(0, 171, 107, 1) !important; border-color: rgba(0, 171, 107, 1) !important; color: rgba(255, 255, 255, 1) !important; fill: rgba(255, 255, 255, 1) !important;}
.u-resetSpectrum .postArticle.is-withAccentColors .markup--user,.u-resetSpectrum .postArticle.is-withAccentColors .markup--query {color: rgba(0, 171, 107, 1) !important;}</style></div></div></div><div class="loadingBar"></div><script>// <![CDATA[
window["obvInit"] = function (opt_embedded) {window["obvInit"]["embedded"] = opt_embedded; window["obvInit"]["ready"] = true;}
// ]]></script><script>// <![CDATA[
var GLOBALS = {"audioUrl":"https://d1fcbxp97j4nb2.cloudfront.net","baseUrl":"https://medium.com","buildLabel":"34742-2bac98a","currentUser":{"userId":"lo_rejQ0qt6DyRP","isVerified":false,"subscriberEmail":"","hasPastMemberships":false,"isEnrolledInHightower":false,"isEligibleForHightower":false,"hightowerLastLockedAt":0,"isWriterProgramEnrolled":false,"isWriterProgramInvited":false,"isWriterProgramOptedOut":false},"currentUserHasUnverifiedEmail":false,"isAuthenticated":false,"isCurrentUserVerified":false,"language":"en-gb","miroUrl":"https://cdn-images-1.medium.com","moduleUrls":{"base":"https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.t75scpQGoB32yGtcW06FAQ.js","common-async":"https://cdn-static-1.medium.com/_/fp/gen-js/main-common-async.bundle.nJD9-yjP0__r2XhkwIzYwQ.js","hightower":"https://cdn-static-1.medium.com/_/fp/gen-js/main-hightower.bundle.lyMtEgvr6vx3RmPx-TcNPg.js","home-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-home-screens.bundle.rETwCz2rT4MOOaxsV6NeMg.js","misc-screens":"https://cdn-static-1.medium.com/_/fp/gen-js/main-misc-screens.bundle.NgZ2kCyRYN1wSA9SFxSx7w.js","notes":"https://cdn-static-1.medium.com/_/fp/gen-js/main-notes.bundle.YL0ZDiM8Uv0f_wuvzZFRDQ.js","payments":"https://cdn-static-1.medium.com/_/fp/gen-js/main-payments.bundle.U9gFJA1f4kskkNz5EnQnug.js","posters":"https://cdn-static-1.medium.com/_/fp/gen-js/main-posters.bundle.hf6p7AoMRwwyCNFV-W_MIw.js","power-readers":"https://cdn-static-1.medium.com/_/fp/gen-js/main-power-readers.bundle.DWOaMAlaWz-NjqCTmZ6Kgw.js","pubs":"https://cdn-static-1.medium.com/_/fp/gen-js/main-pubs.bundle.4IFFvlnVIaZlVtY-TWyK5g.js","stats":"https://cdn-static-1.medium.com/_/fp/gen-js/main-stats.bundle.buLWUhAsdID8Fc4h9KFHCA.js"},"previewConfig":{"weightThreshold":1,"weightImageParagraph":0.51,"weightIframeParagraph":0.8,"weightTextParagraph":0.08,"weightEmptyParagraph":0,"weightP":0.003,"weightH":0.005,"weightBq":0.003,"minPTextLength":60,"truncateBoundaryChars":20,"detectTitle":true,"detectTitleLevThreshold":0.15},"productName":"Medium","supportsEdit":true,"termsUrl":"//medium.com/policy/9db0094a1e0f","textshotHost":"textshot.medium.com","transactionId":"1535503836103:1cf3b8c3246d","useragent":{"browser":"firefox","family":"firefox","os":"","version":61,"supportsDesktopEdit":true,"supportsInteract":true,"supportsView":true,"isMobile":false,"isTablet":false,"isNative":false,"supportsFileAPI":true,"isTier1":true,"clientVersion":"","unknownParagraphsBad":false,"clientChannel":"","supportsRealScrollEvents":true,"supportsVhUnits":true,"ruinsViewportSections":false,"supportsHtml5Video":true,"supportsMagicUnderlines":true,"isWebView":false,"isFacebookWebView":false,"supportsProgressiveMedia":true,"supportsPromotedPosts":true,"isBot":false,"isNativeIphone":false,"supportsCssVariables":true,"supportsVideoSections":true,"emojiSupportLevel":1,"isSearchBot":false,"isSyndicationBot":false,"supportsScrollableMetabar":true},"variants":{"allow_access":true,"allow_signup":true,"allow_test_auth":"disallow","signin_services":"twitter,facebook,google,email,google-fastidv","signup_services":"twitter,facebook,google,email,google-fastidv","android_rating_prompt_recommend_threshold":5,"google_sign_in_android":true,"reengagement_notification_duration":3,"browsable_stream_config_bucket":"curated-topics","enable_dedicated_series_tab_api_ios":true,"enable_post_import":true,"available_monthly_plan":"60e220181034","available_annual_plan":"2c754bcc2995","marketing_promo_experiment_b":"bucket_one","disable_ios_resume_reading_toast":true,"is_not_medium_subscriber":true,"glyph_font_set":"m2","enable_branding":true,"enable_branding_fonts":true,"enable_multirecommends":true,"enable_post_monger_v2":true,"enable_post_monger_v3":true,"enable_fastrak_beta":true,"enable_fastrak_beta_opt_out_toggle":true,"enable_user_post_metering":true,"max_premium_content_per_user_under_metering":3,"enable_metered_out_email":true,"enable_automated_mission_control_triggers":true,"enable_user_profile_featured_moc":true,"enable_strong_graph_chp_reorder":true,"enable_top_stories_for_you":true,"enable_ios_member_post_labeling":true,"enable_lite_profile":true,"enable_li_search_collection":true,"enable_homepage_remodel":true,"enable_signin_wall_custom_domain":true,"app_download_email_template":"control","enable_topic_lifecycle_email":true,"enable_marketing_emails":true,"enable_curation_post_locking":true,"ios_hide_avatars_on_home":true,"android_disable_author_avatars":true,"enable_truncated_rss_for_tags_and_topics":true,"enable_ios_related_reads_api_change":true,"enable_ios_related_reads_ui_large":true,"enable_ios_responses_collapsed":true,"enable_parsely":true,"enable_branch_io":true,"enable_synchronous_parsely":true,"enable_curation_master_feed":true,"enable_distributable_post_post_similarity_table":true,"enable_post_stats_page":true,"enable_ios_post_stats":true,"enable_prepublish_flow_2":true,"enable_prepublish_flow_2_featured_image_selector":true,"enable_prepublish_flow_2_milestone_2":true,"enable_prepublish_flow_2_republish":true,"enable_lite_topics":true,"enable_lite_billing_history":true,"accelerate_membership_headline_test":"feeds-headline","enable_ios_payment_page_v2":true,"enable_new_meter_email_design":true,"new_meter_email_experiment":true,"enable_profile_page_updates":true,"enable_meter_banner_copy_test":true,"annual_renewal_reminder_email_variant":"control","enable_annual_renewal_reminder_email":true,"enable_sift_integration":true,"enable_quality_assurance_queue":true,"enable_boost_moc_highlights":true,"enable_featured_week_lookback":true,"enable_writer_controls":true},"xsrfToken":"","iosAppId":"828256236","supportEmail":"yourfriends@medium.com","fp":{"/icons/monogram-mask.svg":"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg","/icons/favicon-dev-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-dev-editor.YKKRxBO8EMvIqhyCwIiJeQ.ico","/icons/favicon-hatch-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-hatch-editor.BuEyHIqlyh2s_XEk4Rl32Q.ico","/icons/favicon-medium-editor.ico":"https://cdn-static-1.medium.com/_/fp/icons/favicon-medium-editor.PiakrZWB7Yb80quUVQWM6g.ico"},"authBaseUrl":"https://medium.com","imageUploadSizeMb":25,"isAuthDomainRequest":true,"algoliaApiEndpoint":"https://MQ57UUUQZ2-dsn.algolia.net","algoliaAppId":"MQ57UUUQZ2","algoliaSearchOnlyApiKey":"394474ced050e3911ae2249ecc774921","iosAppStoreUrl":"https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8","iosAppLinkBaseUrl":"medium:","algoliaIndexPrefix":"medium_","androidPlayStoreUrl":"https://play.google.com/store/apps/details?id=com.medium.reader","googleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","androidPackage":"com.medium.reader","androidPlayStoreMarketScheme":"market://details?id=com.medium.reader","googleAuthUri":"https://accounts.google.com/o/oauth2/auth","androidScheme":"medium","layoutData":{"useDynamicScripts":false,"googleAnalyticsTrackingCode":"UA-24232453-2","jsShivUrl":"https://cdn-static-1.medium.com/_/fp/js/shiv.RI2ePTZ5gFmMgLzG5bEVAA.js","useDynamicCss":false,"faviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico","faviconImageId":"1*8I-HPL0bfoIzGied-dzOvA.png","fontSets":[{"id":8,"url":"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css"},{"id":11,"url":"https://glyph.medium.com/css/m2.css"},{"id":9,"url":"https://glyph.medium.com/css/mkt.css"},{"id":10,"url":"https://glyph.medium.com/css/elv8.css"}],"editorFaviconUrl":"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium-editor.3Y6xpZ-0FSdWDnPM3hSBIA.ico","glyphUrl":"https://glyph.medium.com"},"authBaseUrlRev":"moc.muidem//:sptth","isDnt":false,"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","archiveUploadSizeMb":100,"paymentData":{"currencies":{"1":{"label":"US Dollar","external":"usd"}},"countries":{"1":{"label":"United States of America","external":"US"}},"accountTypes":{"1":{"label":"Individual","external":"individual"},"2":{"label":"Company","external":"company"}}},"previewConfig2":{"weightThreshold":1,"weightImageParagraph":0.05,"raiseImage":true,"enforceHeaderHierarchy":true,"isImageInsetRight":true},"isAmp":false,"iosScheme":"medium","isSwBoot":false,"lightstep":{"accessToken":"ce5be895bef60919541332990ac9fef2","carrier":"{\"ot-tracer-spanid\":\"290bc8ba0f08894c\",\"ot-tracer-traceid\":\"191e6d9c4aa3e156\",\"ot-tracer-sampled\":\"true\"}","host":"collector-medium.lightstep.com"},"facebook":{"key":"542599432471018","namespace":"medium-com","scope":{"default":["public_profile","email"],"connect":["public_profile","email"],"login":["public_profile","email"],"share":["public_profile","email"]}},"editorsPicksTopicId":"3985d2a191c5","popularOnMediumTopicId":"9d34e48ecf94","memberContentTopicId":"13d7efd82fb2","audioContentTopicId":"3792abbd134","brandedSequenceId":"7d337ddf1941","isDoNotAuth":false,"goldfinchUrl":"https://goldfinch.medium.com","buggle":{"url":"https://buggle.medium.com","videoUrl":"https://cdn-videos-1.medium.com","audioUrl":"https://cdn-audio-1.medium.com"},"referrerType":2,"isMeteredOut":false,"meterConfig":{"maxUnlockCount":3,"windowLength":"MONTHLY"},"partnerProgramEmail":"partnerprogram@medium.com","userResearchPrompts":[{"promptId":"lo_post_page_4","type":0,"url":"www.calendly.com"},{"promptId":"lo_home_page","type":1,"url":"www.calendly.com"},{"promptId":"lo_profile_page","type":2,"url":"www.calendly.com"}],"recaptchaKey":"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ","paypalClientMode":"production","signinWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"countryCode":"US","bypassMeter":false,"branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm"}
// ]]></script><script charset="UTF-8" src="https://cdn-static-1.medium.com/_/fp/gen-js/main-base.bundle.t75scpQGoB32yGtcW06FAQ.js" async></script><script>// <![CDATA[
window["obvInit"]({"value":{"id":"c673cc5862ef","versionId":"335ee591f06a","creatorId":"52aa38cb8e25","creator":{"userId":"52aa38cb8e25","name":"James Le","username":"james_aka_yale","createdAt":1418957998761,"lastPostCreatedAt":1535228046119,"imageId":"1*kbXSc2-EEtk9ekKq36woIQ.jpeg","backgroundImageId":"1*ttOzXYqh5n4xr9k4E57khQ.jpeg","bio":"Blue Ocean Thinker (https://jameskle.com/)","twitterScreenName":"james_aka_yale","socialStats":{"userId":"52aa38cb8e25","usersFollowedCount":1137,"usersFollowedByCount":8811,"type":"SocialStats"},"social":{"userId":"lo_rejQ0qt6DyRP","targetUserId":"52aa38cb8e25","type":"Social"},"facebookAccountId":"1052926731387809","allowNotes":1,"isNsfw":false,"isWriterProgramInvited":false,"isPartnerProgramEnrolled":true,"isWriterProgramEnrolled":false,"isWriterProgramOptedOut":false,"type":"User"},"homeCollection":{"id":"81ecf7ff5ef4","name":"NanoNets","slug":"nanonets","tags":["STARTUP","MACHINE LEARNING","AI","TECHNOLOGY"],"creatorId":"3ba3eae5c891","description":"NanoNets: Machine Learning API","shortDescription":"NanoNets: Machine Learning API","image":{"imageId":"1*-a4bbXgtmdNW0Nj2RMzRGw.png","filter":"","backgroundSize":"","originalWidth":800,"originalHeight":800,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":1279,"activeAt":1533284594998},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":"","isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"1*-a4bbXgtmdNW0Nj2RMzRGw.png","filter":"","backgroundSize":"","originalWidth":800,"originalHeight":800,"strategy":"resample","height":0,"width":0},"twitterUsername":"nano_nets","facebookPageName":"nanonets","publicEmail":"info@nanonets.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"NanoNets","description":"Machine Learning APIs","backgroundImage":{},"logoImage":{"id":"1*-a4bbXgtmdNW0Nj2RMzRGw@2x.png","originalWidth":800,"originalHeight":800,"alt":"NanoNets"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":7,"postIds":[]}}],"tintColor":"#FF546FFF","lightText":false,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF5E7BFF","point":0},{"color":"#FF5873F3","point":0.1},{"color":"#FF536BDB","point":0.2},{"color":"#FF4D63C4","point":0.3},{"color":"#FF465AAE","point":0.4},{"color":"#FF3F5198","point":0.5},{"color":"#FF384782","point":0.6},{"color":"#FF313D6C","point":0.7},{"color":"#FF283257","point":0.8},{"color":"#FF1F2642","point":0.9},{"color":"#FF14192D","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF546FFF","point":0},{"color":"#FF6581FF","point":0.1},{"color":"#FF7692FF","point":0.2},{"color":"#FF87A2FF","point":0.3},{"color":"#FF98B0FF","point":0.4},{"color":"#FFA8BEFF","point":0.5},{"color":"#FFB9CCFF","point":0.6},{"color":"#FFC9D9FF","point":0.7},{"color":"#FFD9E6FF","point":0.8},{"color":"#FFE9F2FF","point":0.9},{"color":"#FFF8FFFF","point":1}],"backgroundColor":"#FF546FFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFEAF2FF","point":0},{"color":"#FFE5F0FF","point":0.1},{"color":"#FFE1EEFF","point":0.2},{"color":"#FFDDEDFF","point":0.3},{"color":"#FFD9EBFF","point":0.4},{"color":"#FFD5E8FF","point":0.5},{"color":"#FFD0E6FF","point":0.6},{"color":"#FFCCE4FF","point":0.7},{"color":"#FFC8E2FF","point":0.8},{"color":"#FFC3E0FF","point":0.9},{"color":"#FFBFDDFF","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"NanoNets","description":"Machine Learning APIs","backgroundImage":{},"logoImage":{"id":"1*-a4bbXgtmdNW0Nj2RMzRGw@2x.png","originalWidth":800,"originalHeight":800,"alt":"NanoNets"},"alignment":1,"layout":6},"type":"Collection"},"homeCollectionId":"81ecf7ff5ef4","title":"How to do Semantic Segmentation using Deep learning","detectedLanguage":"en","latestVersion":"335ee591f06a","latestPublishedVersion":"335ee591f06a","hasUnpublishedEdits":false,"latestRev":681,"createdAt":1523071445232,"updatedAt":1531131204186,"acceptedAt":0,"firstPublishedAt":1525334636621,"latestPublishedAt":1531131204186,"vote":false,"experimentalCss":"","displayAuthor":"","content":{"subtitle":"This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model.","bodyModel":{"paragraphs":[{"name":"f0af","type":3,"text":"How to do Semantic Segmentation using Deep learning","markups":[]},{"name":"a3c4","type":1,"text":"This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model.","markups":[{"type":1,"start":18,"end":41},{"type":1,"start":53,"end":71},{"type":1,"start":101,"end":126},{"type":2,"start":0,"end":126}]},{"name":"00d0","type":3,"text":"#update: We just launched a new product: Nanonets Object Detection APIs","markups":[{"type":3,"start":41,"end":71,"href":"https://nanonets.com/object-detection-api/?utm_source=medium.com_Semantic_Segmentation&utm_medium=blog&utm_campaign=semantic_segmentation","title":"","rel":"","anchorType":0}]},{"name":"71e4","type":4,"text":"Deeplab Image Semantic Segmentation Network (Source: https://sthalles.github.io/deep_segmentation_network/)","markups":[{"type":3,"start":53,"end":106,"href":"https://sthalles.github.io/deep_segmentation_network/","title":"","rel":"nofollow","anchorType":0}],"layout":5,"metadata":{"id":"1*rZ1vDrOBWqISFiNL5OMEbg.jpeg","originalWidth":2592,"originalHeight":1068}},{"name":"e700","type":1,"text":"Nowadays, semantic segmentation is one of the key problems in the field of computer vision. Looking at the big picture, semantic segmentation is one of the high-level task that paves the way towards complete scene understanding. The importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery. Some of those applications include self-driving vehicles, human-computer interaction, virtual reality etc. With the popularity of deep learning in recent years, many semantic segmentation problems are being tackled using deep architectures, most often Convolutional Neural Nets, which surpass other approaches by a large margin in terms of accuracy and efficiency.","markups":[{"type":1,"start":10,"end":31}]},{"name":"f9b0","type":3,"text":"What is Semantic Segmentation?","markups":[{"type":1,"start":0,"end":30}]},{"name":"41fc","type":1,"text":"Semantic segmentation is a natural step in the progression from coarse to fine inference:","markups":[]},{"name":"bd03","type":9,"text":"The origin could be located at classification, which consists of making a prediction for a whole input.","markups":[{"type":1,"start":31,"end":45}]},{"name":"88f3","type":9,"text":"The next step is localization / detection, which provide not only the classes but also additional information regarding the spatial location of those classes.","markups":[{"type":1,"start":17,"end":41}]},{"name":"7277","type":9,"text":"Finally, semantic segmentation achieves fine-grained inference by making dense predictions inferring labels for every pixel, so that each pixel is labeled with the class of its enclosing object ore region.","markups":[{"type":1,"start":9,"end":30}]},{"name":"efba","type":4,"text":"An example of semantic segmentation (Source: https://blog.goodaudience.com/using-convolutional-neural-networks-for-image-segmentation-a-quick-intro-75bd68779225)","markups":[{"type":3,"start":45,"end":160,"href":"https://blog.goodaudience.com/using-convolutional-neural-networks-for-image-segmentation-a-quick-intro-75bd68779225","title":"","rel":"nofollow","anchorType":0}],"layout":3,"metadata":{"id":"1*MQCvfEbbA44fiZk5GoDvhA.png","originalWidth":1600,"originalHeight":693}},{"name":"023f","type":1,"text":"It is also worthy to review some standard deep networks that have made significant contributions to the field of computer vision, as they are often used as the basis of semantic segmentation systems:","markups":[]},{"name":"0096","type":9,"text":"AlexNet: Toronto’s pioneering deep CNN that won the 2012 ImageNet competition with a test accuracy of 84.6%. It consists of 5 convolutional layers, max-pooling ones, ReLUs as non-linearities, 3 fully-convolutional layers, and dropout.","markups":[{"type":3,"start":0,"end":7,"href":"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":7}]},{"name":"1756","type":9,"text":"VGG-16: This Oxford’s model won the 2013 ImageNet competition with 92.7% accuracy. It uses a stack of convolution layers with small receptive fields in the first layers instead of few layers with big receptive fields.","markups":[{"type":3,"start":0,"end":6,"href":"https://arxiv.org/pdf/1409.1556.pdf","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":6}]},{"name":"27b0","type":9,"text":"GoogLeNet: This Google’s network won the 2014 ImageNet competition with accuracy of 93.3%. It is composed by 22 layers and a newly introduced building block called inception module. The module consists of a Network-in-Network layer, a pooling operation, a large-sized convolution layer, and small-sized convolution layer.","markups":[{"type":3,"start":0,"end":9,"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":9},{"type":2,"start":164,"end":173}]},{"name":"c388","type":9,"text":"ResNet: This Microsoft’s model won the 2016 ImageNet competition with 96.4 % accuracy. It is well-known due to its depth (152 layers) and the introduction of residual blocks. The residual blocks address the problem of training a really deep architecture by introducing identity skip connections so that layers can copy their inputs to the next layer.","markups":[{"type":3,"start":0,"end":6,"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":6}]},{"name":"b330","type":4,"text":"CNN Architectures (Source: https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/28ee688947cf9d31fc48f07a0497cd75200a9485)","markups":[{"type":3,"start":27,"end":167,"href":"https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/28ee688947cf9d31fc48f07a0497cd75200a9485","title":"","rel":"nofollow","anchorType":0}],"layout":1,"metadata":{"id":"1*7UXzOt97gQAmCCOL58hgAw.png","originalWidth":920,"originalHeight":380}},{"name":"8dd7","type":3,"text":"What are the existing Semantic Segmentation approaches?","markups":[{"type":1,"start":0,"end":55}]},{"name":"1dbf","type":1,"text":"A general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network:","markups":[{"type":1,"start":77,"end":84},{"type":1,"start":107,"end":114}]},{"name":"2a10","type":9,"text":"The encoder is usually is a pre-trained classification network like VGG/ResNet followed by a decoder network.","markups":[{"type":1,"start":4,"end":11}]},{"name":"b8d7","type":9,"text":"The task of the decoder is to semantically project the discriminative features (lower resolution) learnt by the encoder onto the pixel space (higher resolution) to get a dense classification.","markups":[{"type":1,"start":16,"end":23}]},{"name":"9609","type":1,"text":"Unlike classification where the end result of the very deep network is the only important thing, semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space. Different approaches employ different mechanisms as a part of the decoding mechanism. Let’s explore the 3 main approaches:","markups":[]},{"name":"3d31","type":13,"text":"1 — Region-Based Semantic Segmentation","markups":[{"type":1,"start":0,"end":38}]},{"name":"c022","type":1,"text":"The region-based methods generally follow the “segmentation using recognition” pipeline, which first extracts free-form regions from an image and describes them, followed by region-based classification. At test time, the region-based predictions are transformed to pixel predictions, usually by labeling a pixel according to the highest scoring region that contains it.","markups":[]},{"name":"a124","type":4,"text":"R-CNN Architecture","markups":[],"layout":4,"metadata":{"id":"1*ccdPdFdcSqkxRg-7902Uuw.jpeg","originalWidth":1280,"originalHeight":720}},{"name":"98be","type":1,"text":"R-CNN (Regions with CNN feature) is one representative work for the region-based methods. It performs the semantic segmentation based on the object detection results. To be specific, R-CNN first utilizes selective search to extract a large quantity of object proposals and then computes CNN features for each of them. Finally, it classifies each region using the class-specific linear SVMs. Compared with traditional CNN structures which are mainly intended for image classification, R-CNN can address more complicated tasks, such as object detection and image segmentation, and it even becomes one important basis for both fields. Moreover, R-CNN can be built on top of any CNN benchmark structures, such as AlexNet, VGG, GoogLeNet, and ResNet.","markups":[{"type":3,"start":0,"end":5,"href":"https://arxiv.org/pdf/1311.2524.pdf","title":"","rel":"","anchorType":0},{"type":1,"start":0,"end":5},{"type":1,"start":6,"end":32}]},{"name":"1d74","type":1,"text":"For the image segmentation task, R-CNN extracted 2 types of features for each region: full region feature and foreground feature, and found that it could lead to better performance when concatenating them together as the region feature. R-CNN achieved significant performance improvements due to using the highly discriminative CNN features. However, it also suffers from a couple of drawbacks for the segmentation task:","markups":[]},{"name":"e79b","type":9,"text":"The feature is not compatible with the segmentation task.","markups":[]},{"name":"1723","type":9,"text":"The feature does not contain enough spatial information for precise boundary generation.","markups":[]},{"name":"54fd","type":9,"text":"Generating segment-based proposals takes time and would greatly affect the final performance.","markups":[]},{"name":"ea54","type":1,"text":"Due to these bottlenecks, recent research has been proposed to address the problems, including SDS, Hypercolumns, Mask R-CNN.","markups":[{"type":3,"start":95,"end":98,"href":"https://arxiv.org/pdf/1407.1808.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":100,"end":112,"href":"https://arxiv.org/pdf/1411.5752.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":114,"end":124,"href":"https://arxiv.org/pdf/1703.06870.pdf","title":"","rel":"","anchorType":0}]},{"name":"c548","type":13,"text":"2 — Fully Convolutional Network-Based Semantic Segmentation","markups":[{"type":1,"start":0,"end":59}]},{"name":"18eb","type":1,"text":"The original Fully Convolutional Network (FCN) learns a mapping from pixels to pixels, without extracting the region proposals. The FCN network pipeline is an extension of the classical CNN. The main idea is to make the classical CNN take as input arbitrary-sized images. The restriction of CNNs to accept and produce labels only for specific sized inputs comes from the fully-connected layers which are fixed. Contrary to them, FCNs only have convolutional and pooling layers which give them the ability to make predictions on arbitrary-sized inputs.","markups":[{"type":3,"start":13,"end":46,"href":"https://arxiv.org/pdf/1411.4038.pdf","title":"","rel":"","anchorType":0},{"type":1,"start":13,"end":46}]},{"name":"8b37","type":4,"text":"FCN Architecture","markups":[],"layout":4,"metadata":{"id":"1*Aa2fKFN2PEKmMQoy8ps-lw.png","originalWidth":2024,"originalHeight":1049}},{"name":"293e","type":1,"text":"One issue in this specific FCN is that by propagating through several alternated convolutional and pooling layers, the resolution of the output feature maps is down sampled. Therefore, the direct predictions of FCN are typically in low resolution, resulting in relatively fuzzy object boundaries. A variety of more advanced FCN-based approaches have been proposed to address this issue, including SegNet, DeepLab-CRF, and Dilated Convolutions.","markups":[{"type":3,"start":397,"end":403,"href":"https://arxiv.org/pdf/1511.00561.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":405,"end":416,"href":"https://arxiv.org/pdf/1412.7062.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":422,"end":442,"href":"https://arxiv.org/pdf/1511.07122.pdf","title":"","rel":"","anchorType":0}]},{"name":"a79a","type":13,"text":"3 — Weakly Supervised Semantic Segmentation","markups":[{"type":1,"start":0,"end":43}]},{"name":"322e","type":1,"text":"Most of the relevant methods in semantic segmentation rely on a large number of images with pixel-wise segmentation masks. However, manually annotating these masks is quite time-consuming, frustrating and commercially expensive. Therefore, some weakly supervised methods have recently been proposed, which are dedicated to fulfilling the semantic segmentation by utilizing annotated bounding boxes.","markups":[]},{"name":"9baa","type":4,"text":"Boxsup Training","markups":[],"layout":4,"metadata":{"id":"1*Zti4CyayplzrKnIJIQfb_Q.jpeg","originalWidth":2116,"originalHeight":796}},{"name":"21f7","type":1,"text":"For example, Boxsup employed the bounding box annotations as a supervision to train the network and iteratively improve the estimated masks for semantic segmentation. Simple Does It treated the weak supervision limitation as an issue of input label noise and explored recursive training as a de-noising strategy. Pixel-level Labeling interpreted the segmentation task within the multiple-instance learning framework and added an extra layer to constrain the model to assign more weight to important pixels for image-level classification.","markups":[{"type":3,"start":13,"end":19,"href":"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":167,"end":181,"href":"http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf","title":"","rel":"","anchorType":0},{"type":3,"start":313,"end":333,"href":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pinheiro_From_Image-Level_to_2015_CVPR_paper.pdf","title":"","rel":"","anchorType":0}]},{"name":"70ee","type":3,"text":"Doing Semantic Segmentation with Fully-Convolutional Network","markups":[{"type":1,"start":0,"end":60}]},{"name":"0820","type":1,"text":"In this section, let’s walk through a step-by-step implementation of the most popular architecture for semantic segmentation — the Fully-Convolutional Net (FCN). We’ll implement it using the TensorFlow library in Python 3, along with other dependencies such as Numpy and Scipy.","markups":[]},{"name":"2071","type":1,"text":"In this exercise we will label the pixels of a road in images using FCN. We’ll work with the Kitti Road Dataset for road/lane detection. This is a simple exercise from the Udacity’s Self-Driving Car Nano-degree program, which you can learn more about the setup in this GitHub repo.","markups":[{"type":3,"start":93,"end":111,"href":"http://www.cvlibs.net/datasets/kitti/eval_road.php","title":"","rel":"","anchorType":0},{"type":3,"start":264,"end":280,"href":"https://github.com/udacity/CarND-Semantic-Segmentation/","title":"","rel":"","anchorType":0}]},{"name":"ec43","type":4,"text":"Kitti Road Dataset Training Sample (Source: http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=3748e213cf8e0100b7a26198114b3cdc7caa3aff)","markups":[{"type":3,"start":44,"end":149,"href":"http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=3748e213cf8e0100b7a26198114b3cdc7caa3aff","title":"","rel":"nofollow","anchorType":0}],"layout":3,"metadata":{"id":"1*OJJyC_CBCM8V1uwLOv2RMA.png","originalWidth":1242,"originalHeight":375}},{"name":"731c","type":1,"text":"Here are the key features of the FCN architecture:","markups":[]},{"name":"8fab","type":9,"text":"FCN transfers knowledge from VGG16 to perform semantic segmentation.","markups":[]},{"name":"3f60","type":9,"text":"The fully connected layers of VGG16 is converted to fully convolutional layers, using 1x1 convolution. This process produces a class presence heat map in low resolution.","markups":[]},{"name":"3c59","type":9,"text":"The upsampling of these low resolution semantic feature maps is done using transposed convolutions (initialized with bilinear interpolation filters).","markups":[]},{"name":"b93f","type":9,"text":"At each stage, the upsampling process is further refined by adding features from coarser but higher resolution feature maps from lower layers in VGG16.","markups":[]},{"name":"3856","type":9,"text":"Skip connection is introduced after each convolution block to enable the subsequent block to extract more abstract, class-salient features from the previously pooled features.","markups":[]},{"name":"230f","type":1,"text":"There are 3 versions of FCN (FCN-32, FCN-16, FCN-8). We’ll implement FCN-8, as detailed step-by-step below:","markups":[{"type":3,"start":69,"end":74,"href":"https://github.com/MarvinTeichmann/tensorflow-fcn/blob/master/fcn8_vgg.py","title":"","rel":"","anchorType":0}]},{"name":"be72","type":9,"text":"Encoder: A pre-trained VGG16 is used as an encoder. The decoder starts from Layer 7 of VGG16.","markups":[{"type":1,"start":0,"end":8},{"type":1,"start":23,"end":28},{"type":1,"start":76,"end":83}]},{"name":"2e04","type":9,"text":"FCN Layer-8: The last fully connected layer of VGG16 is replaced by a 1x1 convolution.","markups":[{"type":1,"start":0,"end":12}]},{"name":"9350","type":9,"text":"FCN Layer-9: FCN Layer-8 is upsampled 2 times to match dimensions with Layer 4 of VGG 16, using transposed convolution with parameters: (kernel=(4,4), stride=(2,2), paddding=’same’). After that, a skip connection was added between Layer 4 of VGG16 and FCN Layer-9.","markups":[{"type":1,"start":0,"end":12},{"type":1,"start":71,"end":78},{"type":1,"start":136,"end":181},{"type":1,"start":231,"end":238},{"type":2,"start":136,"end":181}]},{"name":"956a","type":9,"text":"FCN Layer-10: FCN Layer-9 is upsampled 2 times to match dimensions with Layer 3 of VGG16, using transposed convolution with parameters: (kernel=(4,4), stride=(2,2), paddding=’same’). After that, a skip connection was added between Layer 3 of VGG 16 and FCN Layer-10.","markups":[{"type":1,"start":0,"end":13},{"type":1,"start":72,"end":79},{"type":1,"start":136,"end":181},{"type":1,"start":231,"end":238},{"type":2,"start":136,"end":181}]},{"name":"05a2","type":9,"text":"FCN Layer-11: FCN Layer-10 is upsampled 4 times to match dimensions with input image size so we get the actual image back and depth is equal to number of classes, using transposed convolution with parameters: (kernel=(16,16), stride=(8,8), paddding=’same’).","markups":[{"type":1,"start":0,"end":13},{"type":1,"start":208,"end":256},{"type":2,"start":208,"end":256}]},{"name":"467c","type":4,"text":"FCN-8 Architecture (Source: https://www.researchgate.net/figure/Illustration-of-the-FCN-8s-network-architecture-as-proposed-in-20-In-our-method-the_fig1_305770331)","markups":[{"type":3,"start":28,"end":162,"href":"https://www.researchgate.net/figure/Illustration-of-the-FCN-8s-network-architecture-as-proposed-in-20-In-our-method-the_fig1_305770331","title":"","rel":"nofollow","anchorType":0}],"layout":1,"metadata":{"id":"1*e08wr6of8F1J-6v4iiG2HQ.png","originalWidth":850,"originalHeight":461}},{"name":"d949","type":13,"text":"Step 1","markups":[{"type":1,"start":0,"end":6}]},{"name":"b189","type":1,"text":"We first load the pre-trained VGG-16 model into TensorFlow. Taking in the TensorFlow session and the path to the VGG Folder (which is downloadable here), we return the tuple of tensors from VGG model, including the image input, keep_prob (to control dropout rate), layer 3, layer 4, and layer 7.","markups":[{"type":3,"start":147,"end":151,"href":"http://www.cs.toronto.edu/~frossard/post/vgg16/","title":"","rel":"","anchorType":0},{"type":1,"start":30,"end":36}]},{"name":"11bc","type":11,"text":"VGG16 Function","markups":[],"layout":1,"iframe":{"mediaResourceId":"da5ff90136a268e6c420645f360599fe","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"412d","type":13,"text":"Step 2","markups":[{"type":1,"start":0,"end":6}]},{"name":"f372","type":1,"text":"Now we focus on creating the layers for a FCN, using the tensors from the VGG model. Given the tensors for VGG layer output and the number of classes to classify, we return the tensor for the last layer of that output. In particular, we apply a 1x1 convolution to the encoder layers, and then add decoder layers to the network with skip connections and upsampling.","markups":[{"type":1,"start":245,"end":260},{"type":1,"start":332,"end":348},{"type":1,"start":353,"end":363}]},{"name":"7b01","type":11,"text":"Layers Function","markups":[],"layout":1,"iframe":{"mediaResourceId":"d2b9c40d8d98784991dfec073dc693b4","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"aaf0","type":13,"text":"Step 3","markups":[{"type":1,"start":0,"end":6}]},{"name":"e050","type":1,"text":"The next step is to optimize our neural network, aka building TensorFlow loss functions and optimizer operations. Here we use cross entropy as our loss function and Adam as our optimization algorithm.","markups":[{"type":1,"start":126,"end":139},{"type":1,"start":165,"end":169}]},{"name":"dd17","type":11,"text":"Optimize Function","markups":[],"layout":1,"iframe":{"mediaResourceId":"08ae46401e68b104ba167894fc37e112","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"6206","type":13,"text":"Step 4","markups":[{"type":1,"start":0,"end":6}]},{"name":"b8b0","type":1,"text":"Here we define the train_nn function, which takes in important parameters including number of epochs, batch size, loss function, optimizer operation, and placeholders for input images, label images, learning rate. For the training process, we also set keep_probability to 0.5 and learning_rate to 0.001. To keep track of the progress, we also print out the loss during training.","markups":[{"type":1,"start":19,"end":27},{"type":1,"start":252,"end":268},{"type":1,"start":280,"end":293}]},{"name":"72c1","type":11,"text":"","markups":[],"layout":1,"iframe":{"mediaResourceId":"58a5b62f0355bb6e3f4588abf6d73f47","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"ed36","type":13,"text":"Step 5","markups":[{"type":1,"start":0,"end":6}]},{"name":"7410","type":1,"text":"Finally, it’s time to train our net! In this run function, we first build our net using the load_vgg, layers, and optimize function. Then we train the net using the train_nn function and save the inference data for records.","markups":[{"type":1,"start":45,"end":48},{"type":2,"start":92,"end":100},{"type":2,"start":102,"end":108},{"type":2,"start":114,"end":122},{"type":2,"start":165,"end":173}]},{"name":"4be3","type":11,"text":"Run Function","markups":[],"layout":1,"iframe":{"mediaResourceId":"ca849a752e4e49ffc020eb36608f3aec","thumbnailUrl":"https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10627238%3Fs%3D400%26v%3D4&key=a19fcc184b9711e1b4764040d3dc5c07"}},{"name":"150b","type":1,"text":"About our parameters, we choose epochs = 40, batch_size = 16, num_classes = 2, and image_shape = (160, 576). After doing 2 trial passes with dropout = 0.5 and dropout = 0.75, we found that the 2nd trial yields better results with better average losses.","markups":[]},{"name":"33f5","type":4,"text":"Training Sample Results","markups":[],"layout":3,"metadata":{"id":"1*mjUI2EeA5DIKXwDBnIx-cg.png","originalWidth":2126,"originalHeight":1040}},{"name":"cc49","type":1,"text":"To see the full code, check out this link: https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88","markups":[{"type":3,"start":43,"end":114,"href":"https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88","title":"","rel":"","anchorType":0}]},{"name":"6a6b","type":1,"text":"If you enjoyed this piece, I’d love it if you hit the clap button 👏 so others might stumble upon it.","markups":[{"type":2,"start":0,"end":65},{"type":2,"start":69,"end":101}]},{"name":"379b","type":3,"text":"About Nanonets","markups":[]},{"name":"7076","type":4,"text":"Visit Nanonets.com to know more about us.","markups":[{"type":3,"start":6,"end":18,"href":"https://nanonets.com/?utm_source=Medium.com&utm_campaign=How%20to%20do%20Semantic%20Segmentation%20using%20Deep%C2%A0learning","title":"","rel":"","anchorType":0}],"layout":3,"metadata":{"id":"1*-MJ35jGG7CGGKCrziMTaWA.png","originalWidth":1308,"originalHeight":510}}],"sections":[{"name":"16f2","startIndex":0},{"name":"d7b7","startIndex":74}]},"postDisplay":{"coverless":true}},"virtuals":{"statusForCollection":"APPROVED","allowNotes":true,"previewImage":{"imageId":"1*rZ1vDrOBWqISFiNL5OMEbg.jpeg","filter":"","backgroundSize":"","originalWidth":2592,"originalHeight":1068,"strategy":"resample","height":0,"width":0},"wordCount":1926,"imageCount":10,"readingTime":8.517924528301887,"subtitle":"This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model.","publishedInCount":1,"usersBySocialRecommends":[],"noIndex":false,"recommends":375,"socialRecommends":[],"isBookmarked":false,"tags":[{"slug":"machine-learning","name":"Machine Learning","postCount":44802,"virtuals":{"isFollowing":false},"metadata":{"followerCount":23701,"postCount":44802,"coverImage":{"id":"1*gccuMDV8fXjcvz1RSk4kgQ.png","originalWidth":2000,"originalHeight":3000}},"type":"Tag"},{"slug":"computer-vision","name":"Computer Vision","postCount":2077,"virtuals":{"isFollowing":false},"metadata":{"followerCount":675,"postCount":2077,"coverImage":{"id":"1*-WkySYuR7koWY3g_Ikec2A.gif","originalWidth":480,"originalHeight":270,"isFeatured":true}},"type":"Tag"},{"slug":"deep-learning","name":"Deep Learning","postCount":10647,"virtuals":{"isFollowing":false},"metadata":{"followerCount":9980,"postCount":10647,"coverImage":{"id":"1*gccuMDV8fXjcvz1RSk4kgQ.png","originalWidth":2000,"originalHeight":3000}},"type":"Tag"},{"slug":"segmentation","name":"Segmentation","postCount":291,"virtuals":{"isFollowing":false},"metadata":{"followerCount":14,"postCount":291,"coverImage":{"id":"1*rZ1vDrOBWqISFiNL5OMEbg.jpeg","originalWidth":2592,"originalHeight":1068}},"type":"Tag"},{"slug":"neural-networks","name":"Neural Networks","postCount":3364,"virtuals":{"isFollowing":false},"metadata":{"followerCount":3747,"postCount":3364,"coverImage":{"id":"1*gccuMDV8fXjcvz1RSk4kgQ.png","originalWidth":2000,"originalHeight":3000}},"type":"Tag"}],"socialRecommendsCount":0,"responsesCreatedCount":8,"links":{"entries":[{"url":"https://sthalles.github.io/deep_segmentation_network/","alts":[],"httpStatus":200},{"url":"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf","alts":[],"httpStatus":200},{"url":"https://nanonets.com/object-detection-api/?utm_source=medium.com_Semantic_Segmentation&utm_medium=blog&utm_campaign=semantic_segmentation","alts":[],"httpStatus":200},{"url":"https://nanonets.com/?utm_source=Medium.com&utm_campaign=How%20to%20do%20Semantic%20Segmentation%20using%20Deep%C2%A0learning","alts":[],"httpStatus":200},{"url":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf","alts":[],"httpStatus":200},{"url":"http://openaccess.thecvf.com/content_cvpr_2017/papers/Khoreva_Simple_Does_It_CVPR_2017_paper.pdf","alts":[],"httpStatus":200},{"url":"https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Pinheiro_From_Image-Level_to_2015_CVPR_paper.pdf","alts":[],"httpStatus":200},{"url":"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Dai_BoxSup_Exploiting_Bounding_ICCV_2015_paper.pdf","alts":[],"httpStatus":200},{"url":"https://www.researchgate.net/figure/Illustration-of-the-FCN-8s-network-architecture-as-proposed-in-20-In-our-method-the_fig1_305770331","alts":[],"httpStatus":429},{"url":"https://github.com/udacity/CarND-Semantic-Segmentation/","alts":[],"httpStatus":200},{"url":"http://www.cs.toronto.edu/~frossard/post/vgg16/","alts":[],"httpStatus":200},{"url":"https://github.com/MarvinTeichmann/tensorflow-fcn/blob/master/fcn8_vgg.py","alts":[],"httpStatus":200},{"url":"http://www.cvlibs.net/datasets/kitti/eval_road_detail.php?result=3748e213cf8e0100b7a26198114b3cdc7caa3aff","alts":[],"httpStatus":200},{"url":"https://gist.github.com/khanhnamle1994/e2ff59ddca93c0205ac4e566d40b5e88","alts":[],"httpStatus":200},{"url":"https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/28ee688947cf9d31fc48f07a0497cd75200a9485","alts":[],"httpStatus":200},{"url":"https://blog.goodaudience.com/using-convolutional-neural-networks-for-image-segmentation-a-quick-intro-75bd68779225","alts":[{"type":2,"url":"medium://p/75bd68779225"},{"type":3,"url":"medium://p/75bd68779225"}],"httpStatus":200},{"url":"http://www.cvlibs.net/datasets/kitti/eval_road.php","alts":[],"httpStatus":200},{"url":"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1511.00561.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1411.4038.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1407.1808.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1409.1556.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1511.07122.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1412.7062.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1411.5752.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1311.2524.pdf","alts":[],"httpStatus":200},{"url":"https://arxiv.org/pdf/1703.06870.pdf","alts":[],"httpStatus":200}],"version":"0.3","generatedAt":1531131209011},"isLockedPreviewOnly":false,"takeoverId":"","metaDescription":"","totalClapCount":2733,"sectionCount":2,"readingList":0,"topics":[{"topicId":"ae5d4995e225","slug":"data-science","createdAt":1493923906289,"deletedAt":0,"image":{"id":"1*NHWOEki_ncCX-xzbKtkEWw@2x.jpeg","originalWidth":5760,"originalHeight":3840},"name":"Data science","description":"Query this.","briefCatalogId":"63c534ad8b15","relatedTopics":[],"visibility":1,"relatedTags":[],"type":"Topic"}]},"coverless":true,"slug":"how-to-do-image-segmentation-using-deep-learning","translationSourcePostId":"","translationSourceCreatorId":"","isApprovedTranslation":false,"inResponseToPostId":"","inResponseToRemovedAt":0,"isTitleSynthesized":true,"allowResponses":true,"importedUrl":"","importedPublishedAt":0,"visibility":0,"uniqueSlug":"how-to-do-image-segmentation-using-deep-learning-c673cc5862ef","previewContent":{"bodyModel":{"paragraphs":[{"name":"f0af","type":3,"text":"How to do Semantic Segmentation using Deep learning","markups":[],"alignment":1},{"name":"a3c4","type":1,"text":"This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model.","markups":[{"type":1,"start":18,"end":41},{"type":1,"start":53,"end":71},{"type":1,"start":101,"end":126},{"type":2,"start":0,"end":126}],"alignment":1},{"name":"00d0","type":3,"text":"#update: We just launched a new product: Nanonets Object…","markups":[{"type":3,"start":41,"end":56,"href":"https://nanonets.com/object-detection-api/?utm_source=medium.com_Semantic_Segmentation&utm_medium=blog&utm_campaign=semantic_segmentation","title":"","rel":"","anchorType":0}],"alignment":1}],"sections":[{"startIndex":0}]},"isFullContent":false,"subtitle":"This article is a comprehensive overview including a step-by-step guide to implement a deep learning image segmentation model."},"license":0,"inResponseToMediaResourceId":"","canonicalUrl":"https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef","approvedHomeCollectionId":"81ecf7ff5ef4","approvedHomeCollection":{"id":"81ecf7ff5ef4","name":"NanoNets","slug":"nanonets","tags":["STARTUP","MACHINE LEARNING","AI","TECHNOLOGY"],"creatorId":"3ba3eae5c891","description":"NanoNets: Machine Learning API","shortDescription":"NanoNets: Machine Learning API","image":{"imageId":"1*-a4bbXgtmdNW0Nj2RMzRGw.png","filter":"","backgroundSize":"","originalWidth":800,"originalHeight":800,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":1279,"activeAt":1533284594998},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":"","isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"1*-a4bbXgtmdNW0Nj2RMzRGw.png","filter":"","backgroundSize":"","originalWidth":800,"originalHeight":800,"strategy":"resample","height":0,"width":0},"twitterUsername":"nano_nets","facebookPageName":"nanonets","publicEmail":"info@nanonets.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"NanoNets","description":"Machine Learning APIs","backgroundImage":{},"logoImage":{"id":"1*-a4bbXgtmdNW0Nj2RMzRGw@2x.png","originalWidth":800,"originalHeight":800,"alt":"NanoNets"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":7,"postIds":[]}}],"tintColor":"#FF546FFF","lightText":false,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF5E7BFF","point":0},{"color":"#FF5873F3","point":0.1},{"color":"#FF536BDB","point":0.2},{"color":"#FF4D63C4","point":0.3},{"color":"#FF465AAE","point":0.4},{"color":"#FF3F5198","point":0.5},{"color":"#FF384782","point":0.6},{"color":"#FF313D6C","point":0.7},{"color":"#FF283257","point":0.8},{"color":"#FF1F2642","point":0.9},{"color":"#FF14192D","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF546FFF","point":0},{"color":"#FF6581FF","point":0.1},{"color":"#FF7692FF","point":0.2},{"color":"#FF87A2FF","point":0.3},{"color":"#FF98B0FF","point":0.4},{"color":"#FFA8BEFF","point":0.5},{"color":"#FFB9CCFF","point":0.6},{"color":"#FFC9D9FF","point":0.7},{"color":"#FFD9E6FF","point":0.8},{"color":"#FFE9F2FF","point":0.9},{"color":"#FFF8FFFF","point":1}],"backgroundColor":"#FF546FFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFEAF2FF","point":0},{"color":"#FFE5F0FF","point":0.1},{"color":"#FFE1EEFF","point":0.2},{"color":"#FFDDEDFF","point":0.3},{"color":"#FFD9EBFF","point":0.4},{"color":"#FFD5E8FF","point":0.5},{"color":"#FFD0E6FF","point":0.6},{"color":"#FFCCE4FF","point":0.7},{"color":"#FFC8E2FF","point":0.8},{"color":"#FFC3E0FF","point":0.9},{"color":"#FFBFDDFF","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"NanoNets","description":"Machine Learning APIs","backgroundImage":{},"logoImage":{"id":"1*-a4bbXgtmdNW0Nj2RMzRGw@2x.png","originalWidth":800,"originalHeight":800,"alt":"NanoNets"},"alignment":1,"layout":6},"type":"Collection"},"newsletterId":"","webCanonicalUrl":"https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef","mediumUrl":"https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef","migrationId":"","notifyFollowers":true,"notifyTwitter":false,"isSponsored":false,"isRequestToPubDisabled":false,"notifyFacebook":false,"responseHiddenOnParentPostAt":0,"isSeries":false,"isSubscriptionLocked":false,"seriesLastAppendedAt":0,"audioVersionDurationSec":0,"sequenceId":"","isNsfw":false,"isEligibleForRevenue":false,"isBlockedFromHightower":false,"deletedAt":0,"lockedPostSource":0,"hightowerMinimumGuaranteeStartsAt":0,"hightowerMinimumGuaranteeEndsAt":0,"featureLockRequestAcceptedAt":0,"featureLockRequestMinimumGuaranteeAmount":0,"isElevate":false,"mongerRequestType":1,"layerCake":0,"socialTitle":"","socialDek":"","editorialPreviewTitle":"","editorialPreviewDek":"","type":"Post"},"mentionedUsers":[],"collaborators":[{"user":{"userId":"9bbcab6d816f","name":"Prathamesh A Juvatkar","username":"prathameshajuvatkar","createdAt":1414072638193,"lastPostCreatedAt":1489801635412,"imageId":"1*tfd0imH_s-faysp_lDFb2A.jpeg","backgroundImageId":"","bio":"https://nanonets.com","twitterScreenName":"PJuvatkar","facebookAccountId":"10204048603273079","allowNotes":1,"mediumMemberAt":0,"isPartnerProgramEnrolled":false,"type":"User"},"state":"visible"}],"collectionUserRelations":[],"mode":null,"references":{"User":{"52aa38cb8e25":{"userId":"52aa38cb8e25","name":"James Le","username":"james_aka_yale","createdAt":1418957998761,"lastPostCreatedAt":1535228046119,"imageId":"1*kbXSc2-EEtk9ekKq36woIQ.jpeg","backgroundImageId":"1*ttOzXYqh5n4xr9k4E57khQ.jpeg","bio":"Blue Ocean Thinker (https://jameskle.com/)","twitterScreenName":"james_aka_yale","socialStats":{"userId":"52aa38cb8e25","usersFollowedCount":1137,"usersFollowedByCount":8811,"type":"SocialStats"},"social":{"userId":"lo_rejQ0qt6DyRP","targetUserId":"52aa38cb8e25","type":"Social"},"facebookAccountId":"1052926731387809","allowNotes":1,"isNsfw":false,"isWriterProgramInvited":false,"isPartnerProgramEnrolled":true,"isWriterProgramEnrolled":false,"isWriterProgramOptedOut":false,"type":"User"}},"Collection":{"81ecf7ff5ef4":{"id":"81ecf7ff5ef4","name":"NanoNets","slug":"nanonets","tags":["STARTUP","MACHINE LEARNING","AI","TECHNOLOGY"],"creatorId":"3ba3eae5c891","description":"NanoNets: Machine Learning API","shortDescription":"NanoNets: Machine Learning API","image":{"imageId":"1*-a4bbXgtmdNW0Nj2RMzRGw.png","filter":"","backgroundSize":"","originalWidth":800,"originalHeight":800,"strategy":"resample","height":0,"width":0},"metadata":{"followerCount":1279,"activeAt":1533284594998},"virtuals":{"permissions":{"canPublish":false,"canPublishAll":false,"canRepublish":false,"canRemove":false,"canManageAll":false,"canSubmit":false,"canEditPosts":false,"canAddWriters":false,"canViewStats":false,"canSendNewsletter":false,"canViewLockedPosts":false,"canViewCloaked":false,"canEditOwnPosts":false,"canBeAssignedAuthor":false,"canEnrollInHightower":false,"canLockPostsForMediumMembers":false,"canLockOwnPostsForMediumMembers":false},"isSubscribed":false,"isNewsletterSubscribed":false,"memberOfMembershipPlanId":"","isEnrolledInHightower":false,"isEligibleForHightower":false},"logo":{"imageId":"1*-a4bbXgtmdNW0Nj2RMzRGw.png","filter":"","backgroundSize":"","originalWidth":800,"originalHeight":800,"strategy":"resample","height":0,"width":0},"twitterUsername":"nano_nets","facebookPageName":"nanonets","publicEmail":"info@nanonets.com","sections":[{"type":2,"collectionHeaderMetadata":{"title":"NanoNets","description":"Machine Learning APIs","backgroundImage":{},"logoImage":{"id":"1*-a4bbXgtmdNW0Nj2RMzRGw@2x.png","originalWidth":800,"originalHeight":800,"alt":"NanoNets"},"alignment":1,"layout":6}},{"type":1,"postListMetadata":{"source":1,"layout":4,"number":7,"postIds":[]}}],"tintColor":"#FF546FFF","lightText":false,"favicon":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"colorPalette":{"defaultBackgroundSpectrum":{"colorPoints":[{"color":"#FF5E7BFF","point":0},{"color":"#FF5873F3","point":0.1},{"color":"#FF536BDB","point":0.2},{"color":"#FF4D63C4","point":0.3},{"color":"#FF465AAE","point":0.4},{"color":"#FF3F5198","point":0.5},{"color":"#FF384782","point":0.6},{"color":"#FF313D6C","point":0.7},{"color":"#FF283257","point":0.8},{"color":"#FF1F2642","point":0.9},{"color":"#FF14192D","point":1}],"backgroundColor":"#FFFFFFFF"},"tintBackgroundSpectrum":{"colorPoints":[{"color":"#FF546FFF","point":0},{"color":"#FF6581FF","point":0.1},{"color":"#FF7692FF","point":0.2},{"color":"#FF87A2FF","point":0.3},{"color":"#FF98B0FF","point":0.4},{"color":"#FFA8BEFF","point":0.5},{"color":"#FFB9CCFF","point":0.6},{"color":"#FFC9D9FF","point":0.7},{"color":"#FFD9E6FF","point":0.8},{"color":"#FFE9F2FF","point":0.9},{"color":"#FFF8FFFF","point":1}],"backgroundColor":"#FF546FFF"},"highlightSpectrum":{"colorPoints":[{"color":"#FFEAF2FF","point":0},{"color":"#FFE5F0FF","point":0.1},{"color":"#FFE1EEFF","point":0.2},{"color":"#FFDDEDFF","point":0.3},{"color":"#FFD9EBFF","point":0.4},{"color":"#FFD5E8FF","point":0.5},{"color":"#FFD0E6FF","point":0.6},{"color":"#FFCCE4FF","point":0.7},{"color":"#FFC8E2FF","point":0.8},{"color":"#FFC3E0FF","point":0.9},{"color":"#FFBFDDFF","point":1}],"backgroundColor":"#FFFFFFFF"}},"navItems":[],"colorBehavior":1,"instantArticlesState":0,"acceleratedMobilePagesState":0,"ampLogo":{"imageId":"","filter":"","backgroundSize":"","originalWidth":0,"originalHeight":0,"strategy":"resample","height":0,"width":0},"header":{"title":"NanoNets","description":"Machine Learning APIs","backgroundImage":{},"logoImage":{"id":"1*-a4bbXgtmdNW0Nj2RMzRGw@2x.png","originalWidth":800,"originalHeight":800,"alt":"NanoNets"},"alignment":1,"layout":6},"type":"Collection"}},"Social":{"52aa38cb8e25":{"userId":"lo_rejQ0qt6DyRP","targetUserId":"52aa38cb8e25","type":"Social"}},"SocialStats":{"52aa38cb8e25":{"userId":"52aa38cb8e25","usersFollowedCount":1137,"usersFollowedByCount":8811,"type":"SocialStats"}}}})
// ]]></script><script id="parsely-cfg" src="//d1z2jf7jlzjs58.cloudfront.net/keys/medium.com/p.js"></script><script type="text/javascript">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0); branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true}, function(err, data) {});</script></body></html>